{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machineAssignment6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP1hmBTSST_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as tdata\n",
        "import torchvision.transforms as tTrans\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optm\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "# Python Imaging Library\n",
        "import PIL\n",
        "import numpy as np\n",
        "import sys as sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSunkkNLSbVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global Parameters\n",
        "# Automatically detect if there is a GPU or just use CPU.\n",
        "device  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# ========================================================================================================================\n",
        "# Functions and Network Template\n",
        "# ========================================================================================================================\n",
        "def load_data(bSize = 32):\n",
        "    # bundle common args to the Dataloader module as a kewword list.\n",
        "    # pin_memory reserves memory to act as a buffer for cuda memcopy \n",
        "    # operations\n",
        "    comArgs = {'shuffle': True,'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        " \n",
        "    # Data Loading -----------------------\n",
        "    # ******************\n",
        "    # At this point the data come in Python tuples, a 28x28 image and a label.\n",
        "    # while the label is a tensor, the image is not; it needs to be converted.  \n",
        "    # So we need to transform PIL image to tensor and then normalize it.\n",
        "    # Normalization is quite a good practise to avoid numerical and convergence\n",
        "    # problems. For that we need the dataset's mean and std which fortunately\n",
        "    # can be computed!\n",
        "    # ******************\n",
        "    mean = 0.1307\n",
        "    std  = 0.3081\n",
        "    # Bundle our transforms sequentially, one after another. This is important.\n",
        "    # Convert images to tensors + normalize\n",
        "    transform = tTrans.Compose([tTrans.ToTensor(), tTrans.Normalize( (mean,), (std,) )])\n",
        "    # Load data set\n",
        "    mnistTrainset = tdata.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    mnistTestset = tdata.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        " \n",
        "    # Once we have a dataset, torch.utils has a very nice lirary for iterating on that\n",
        "    # dataset, with shuffle AND batch logic. Very usefull in larger datasets.\n",
        "    trainLoader = torch.utils.data.DataLoader(mnistTrainset, batch_size = bSize, **comArgs )\n",
        "    testLoader = torch.utils.data.DataLoader(mnistTestset, batch_size = bSize, **comArgs)\n",
        "    # End of DataLoading -------------------\n",
        " \n",
        " \n",
        "    # Sanity Prints---\n",
        "    # print(len(mnistTrainset))\n",
        "    # print(type(mnist_trainset[0]))\n",
        " \n",
        "    return trainLoader, testLoader\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IaxKoh3SmsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Definition\n",
        "class Net(nn.Module):\n",
        " \n",
        "    # Class variables for measures.\n",
        "    accuracy = 0\n",
        "    trainLoss= 0\n",
        "    testLoss = 0\n",
        "    # Mod init + boiler plate code\n",
        "    # Skeleton of this network; the blocks to be used.\n",
        "    # Similar to Fischer prize building blocks!\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Declare the layers along with their dimension here!\n",
        "        # NOTE: Trying to run the code with no layer declared and no architecture defined will \n",
        "        # Lead to an error \"ValueError: optimizer got an empty parameter list\"\n",
        "        # self.conv1 = conv2d\n",
        "        self.simple1 = nn.Linear(784, 392)\n",
        "        self.simple2 = nn.Linear(392, 10)\n",
        "        self.double1 = nn.Linear(784, 392)\n",
        "        self.double2 = nn.Linear(392,64)\n",
        "        self.double3 = nn.Linear(64, 10)\n",
        "        self.triple1 = nn.Linear(784, 256)\n",
        "        self.triple2 = nn.Linear(256, 128)\n",
        "        self.triple3 = nn.Linear(128, 64)\n",
        "        self.triple4 = nn.Linear(64, 10)\n",
        "        self.simpleBlock = nn.Sequential(\n",
        "            nn.Conv2d(1,8,kernel_size = 5, stride = 2, padding = 1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "        self.simpleBlockFC = nn.Linear(6*6*8,10)\n",
        "        self.doubleBlock1 = nn.Sequential(\n",
        "            nn.Conv2d(1,16,kernel_size = 5, stride = 1, padding = 2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "        self.doubleBlock2 = nn.Sequential(\n",
        "            nn.Conv2d(16,64,kernel_size = 5, stride = 1, padding = 2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "        self.doubleBlockFC = nn.Linear(7*7*32,10)\n",
        "        self.fullFc1 = nn.Linear(7*7*64, 256)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.fullFc2 = nn.Linear(256, 10)\n",
        "        # ---|\n",
        "    # ------------------\n",
        " \n",
        "    # Set the aove defined building blocks as an\n",
        "    # organized, meaningful architecture here.\n",
        "    def forwardSimple(self, x):\n",
        "        # Define the network architecture here.\n",
        "        # Each layer would be given as input to the next. \n",
        "        # Output should be of size [batchSize, classes]\n",
        "        # NOTE: After each layer, especially after convolutional layers the shape\n",
        "        # of the size tensor changes. print(x.shape) might be your Samwise Gamtzee\n",
        "        # in those difficult moments!\n",
        "        x = x.view(-1,784)\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.simple1(x))\n",
        "        x = F.relu(self.simple2(x))\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def forwardDouble(self, x):\n",
        "        x = x.view(-1,784)\n",
        "        x = F.relu(self.double1(x))\n",
        "        x = F.relu(self.double2(x))\n",
        "        x = F.relu(self.double3(x))\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "    def forwardTriple(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.relu(self.triple1(x))\n",
        "        x = F.relu(self.triple2(x))\n",
        "        x = F.relu(self.triple3(x))\n",
        "        x = F.relu(self.triple4(x))\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "    def forwardSimpleBlock(self, x):\n",
        "        x = self.simpleBlock(x)\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = self.simpleBlockFC(x)\n",
        "        x = F.log_softmax(x, dim =1)\n",
        "        return x\n",
        "    def forwardDoubleBlock(self, x):\n",
        "        x = self.doubleBlock1(x)\n",
        "        x = self.doubleBlock2(x)\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = self.doubleBlockFC(x)\n",
        "        x = F.log_softmax(x, dim =1)\n",
        "        return x\n",
        "    def fullNetwork(self, x):\n",
        "        x = self.doubleBlock1(x)\n",
        "        x = self.doubleBlock2(x)\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = self.fullFc1(x)\n",
        "        x = self.fullFc2(x)\n",
        "        x = F.log_softmax(x, dim =1)\n",
        "        return x\n",
        "    def robustNetwork(self, x):\n",
        "        x = self.doubleBlock1(x)\n",
        "        x = self.doubleBlock2(x)\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = self.fullFc1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fullFc2(x)\n",
        "        x = F.log_softmax(x, dim =1)\n",
        "        return x\n",
        "\n",
        "    def train(self, args, device, indata, optim, verbose = True):\n",
        "        for idx, (img, label) in enumerate(indata):\n",
        "            data, label = img.to(device), label.to(device)\n",
        "            # forward pass calculate output of model\n",
        "            # output      = self.forward_no_drop(data)\n",
        "            output = self.robustNetwork(data)\n",
        "            # print(\"output shape\",output.shape)\n",
        "            # print(\"label\",label.shape)\n",
        "            # compute loss\n",
        "            loss        = F.nll_loss(output, label)\n",
        "            # loss = nn.CrossEntropyLoss(output, F.softmax(label, dim =1))\n",
        "\n",
        "            # Backpropagation part\n",
        "            # 1. Zero out Grads\n",
        "            optim.zero_grad()\n",
        "            # 2. Perform the backpropagation based on loss\n",
        "            loss.backward()            \n",
        "            # 3. Update weights \n",
        "            optim.step()\n",
        " \n",
        "           # Training Progress report for sanity purposes! \n",
        "            if verbose:\n",
        "                if idx % 20 == 0: \n",
        "                    print(\"Epoch: {}->Batch: {} / {}. Loss = {}\".format(args, idx, len(indata), loss.item() ))\n",
        "        # Log the current train loss\n",
        "        # self.trainLoss = loss.item()*data.size(0)\n",
        "        self.trainLoss = loss   \n",
        "    # -----------------------\n",
        " \n",
        "    # Testing and error reports are done here\n",
        "    def test(self, device, testLoader):\n",
        "        print(\"In Testing Function!\")        \n",
        "        loss = 0\n",
        "        true = 0\n",
        "        acc  = 0\n",
        "        # Inform Pytorch that keeping track of gradients is not required in\n",
        "        # testing phase.\n",
        "        with torch.no_grad():\n",
        "            for data, label in testLoader:\n",
        "                data, label = data.to(device), label.to(device)\n",
        "                output = self.robustNetwork(data)\n",
        "                # output = self.forward_no_drop(data)\n",
        "                # Sum all loss terms and tern then into a numpy number for late use.\n",
        "                # loss        = nn.CrossEntropyLoss(output, label).item()*data.size(0)\n",
        "                # ll = nn.CrossEntropyLoss(output, F.softmax(label, dim =1))\n",
        "                # loss += ll.item()\n",
        "                loss  += F.nll_loss(output, label, reduction = 'sum').item()\n",
        "                # loss = nn.CrossEntropyLoss(output, label)\n",
        "                \n",
        "                # Find the max along a row but maitain the original dimenions.\n",
        "                # in this case  a 10 -dimensional array.\n",
        "                pred   = output.max(dim = 1, keepdim = True)\n",
        "                # Select the indexes of the prediction maxes.\n",
        "                # Reshape the output vector in the same form of the label one, so they \n",
        "                # can be compared directly; from batchsize x 10 to batchsize. Compare\n",
        "                # predictions with label;  1 indicates equality. Sum the correct ones\n",
        "                # and turn them to numpy number. In this case the idx of the maximum \n",
        "                # prediciton coincides with the label as we are predicting numbers 0-9.\n",
        "                # So the indx of the max output of the network is essentially the predicted\n",
        "                # label (number).\n",
        "                true  += label.eq(pred[1].view_as(label)).sum().item()\n",
        "        acc = true/len(testLoader.dataset)\n",
        "        self.accuracy = acc\n",
        "        self.testLoss = loss \n",
        "        # Print accuracy report!\n",
        "        print(\"Accuracy: {} ({} / {})\".format(acc, true,\n",
        "                                                len(testLoader.dataset)))\n",
        "\n",
        "    def report(self):\n",
        "        print(\"Current stats of MNIST_NET:\")\n",
        "        print(\"Accuracy:      {}\" .format(self.accuracy))\n",
        "        print(\"Training Loss: {}\" .format(self.trainLoss))\n",
        "        print(\"Test Loss:     {}\" .format(self.testLoss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wUivEYYTFfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_args():\n",
        "    \"\"\" Description: This function will create an argument parser. This will accept inputs from the console.\n",
        "                     But if no inputs are given, the default values listed will be used!\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(prog='Fashion MNIST Network building!')\n",
        "    # Tell parser to accept the following arguments, along with default vals.\n",
        "    parser.add_argument('--lr',    type = float,metavar = 'lr',   default='0.001',help=\"Learning rate for the oprimizer.\")\n",
        "    parser.add_argument('--m',     type = float,metavar = 'float',default= 0,     help=\"Momentum for the optimizer, if any.\")\n",
        "    parser.add_argument('--bSize', type = int,  metavar = 'bSize',default=32,     help=\"Batch size of data loader, in terms of samples. a size of 32 means 32 images for an optimization step.\")\n",
        "    parser.add_argument('--epochs',type = int,  metavar = 'e',    default=12   ,  help=\"Number of training epochs. One epoch is to perform an optimization step over every sample, once.\")\n",
        "    # Parse the input from the console. To access a specific arg-> dim = args.dim\n",
        "    args = parser.parse_args()\n",
        "    lr, m, bSize, epochs = args.lr, args.m, args.bSize, args.epochs\n",
        "    # Sanitize input\n",
        "    m = m if (m>0 and m <1) else 0\n",
        "    lr = lr if lr < 1 else 0.1\n",
        "    # It is standard in larger project to return a dictionary instead of a myriad of args like:\n",
        "    # return {'lr':lr,'m':m,'bSize':bbSize,'epochs':epochs}\n",
        "    return lr, m , bSize, epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meutlk4qTmrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be8f5f6b-3a2c-4979-d8dd-6a4fa2b578ba"
      },
      "source": [
        "def main():\n",
        "    # Get keyboard arguments, if any! (Try the dictionary approach in the code aboe for some practice!)\n",
        "    # lr, m , bSize, epochs = parse_args()\n",
        "    # Load data, initialize model and optimizer!\n",
        "    # transform = tTrans.Compose([tTrans.ToTensor(),tTrans.Normalize((mean,)(std,))])\n",
        "    bSize = 32\n",
        "    epochs = 24\n",
        "    lr = 0.001\n",
        "    m = 0\n",
        "    trainLoader, testLoader = load_data(bSize=bSize)\n",
        "    model = Net().to(device) # send model to appropriate computing device (CPU or CUDA)\n",
        "    print(model)\n",
        "    optim = optm.SGD(model.parameters(), lr=lr, momentum=m) # Instantiate optimizer with the model's parameters.\n",
        "    print(\"######### Initiating Fashion MNIST network training #########\\n\")\n",
        "    print(\"Parameters: lr:{}, momentum:{}, batch Size:{}, epochs:{}\".format(lr,m,bSize,epochs))\n",
        "    for e in range(epochs):\n",
        "        print(\"Epoch: {} start ------------\\n\".format(e))\n",
        "        # print(\"Dev {}\".format(device))\n",
        "        args = e\n",
        "        model.train(args, device, trainLoader, optim)\n",
        "        model.test(device, testLoader)\n",
        " \n",
        "    # Final report\n",
        "    model.report()\n",
        " \n",
        "# Define behavior if this module is the main executable. Standard code.\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (simple1): Linear(in_features=784, out_features=392, bias=True)\n",
            "  (simple2): Linear(in_features=392, out_features=10, bias=True)\n",
            "  (double1): Linear(in_features=784, out_features=392, bias=True)\n",
            "  (double2): Linear(in_features=392, out_features=64, bias=True)\n",
            "  (double3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (triple1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (triple2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (triple3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (triple4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (simpleBlock): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (simpleBlockFC): Linear(in_features=288, out_features=10, bias=True)\n",
            "  (doubleBlock1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (doubleBlock2): Sequential(\n",
            "    (0): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (doubleBlockFC): Linear(in_features=1568, out_features=10, bias=True)\n",
            "  (fullFc1): Linear(in_features=3136, out_features=256, bias=True)\n",
            "  (drop): Dropout(p=0.3, inplace=False)\n",
            "  (fullFc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "######### Initiating Fashion MNIST network training #########\n",
            "\n",
            "Parameters: lr:0.001, momentum:0, batch Size:32, epochs:24\n",
            "Epoch: 0 start ------------\n",
            "\n",
            "Epoch: 0->Batch: 0 / 1875. Loss = 2.332733631134033\n",
            "Epoch: 0->Batch: 20 / 1875. Loss = 1.9528676271438599\n",
            "Epoch: 0->Batch: 40 / 1875. Loss = 1.8489031791687012\n",
            "Epoch: 0->Batch: 60 / 1875. Loss = 1.6961442232131958\n",
            "Epoch: 0->Batch: 80 / 1875. Loss = 1.5571935176849365\n",
            "Epoch: 0->Batch: 100 / 1875. Loss = 1.4787344932556152\n",
            "Epoch: 0->Batch: 120 / 1875. Loss = 1.3382070064544678\n",
            "Epoch: 0->Batch: 140 / 1875. Loss = 1.203568935394287\n",
            "Epoch: 0->Batch: 160 / 1875. Loss = 1.2115724086761475\n",
            "Epoch: 0->Batch: 180 / 1875. Loss = 1.1896103620529175\n",
            "Epoch: 0->Batch: 200 / 1875. Loss = 1.0569896697998047\n",
            "Epoch: 0->Batch: 220 / 1875. Loss = 0.9658207297325134\n",
            "Epoch: 0->Batch: 240 / 1875. Loss = 0.8632856011390686\n",
            "Epoch: 0->Batch: 260 / 1875. Loss = 1.00384521484375\n",
            "Epoch: 0->Batch: 280 / 1875. Loss = 0.9173598289489746\n",
            "Epoch: 0->Batch: 300 / 1875. Loss = 0.876076340675354\n",
            "Epoch: 0->Batch: 320 / 1875. Loss = 0.7707237601280212\n",
            "Epoch: 0->Batch: 340 / 1875. Loss = 0.8353874683380127\n",
            "Epoch: 0->Batch: 360 / 1875. Loss = 0.7648678421974182\n",
            "Epoch: 0->Batch: 380 / 1875. Loss = 0.8535988926887512\n",
            "Epoch: 0->Batch: 400 / 1875. Loss = 0.8558638095855713\n",
            "Epoch: 0->Batch: 420 / 1875. Loss = 0.8126741051673889\n",
            "Epoch: 0->Batch: 440 / 1875. Loss = 0.7380539774894714\n",
            "Epoch: 0->Batch: 460 / 1875. Loss = 0.8220134973526001\n",
            "Epoch: 0->Batch: 480 / 1875. Loss = 0.543863832950592\n",
            "Epoch: 0->Batch: 500 / 1875. Loss = 0.6502907872200012\n",
            "Epoch: 0->Batch: 520 / 1875. Loss = 0.7034834027290344\n",
            "Epoch: 0->Batch: 540 / 1875. Loss = 0.8549729585647583\n",
            "Epoch: 0->Batch: 560 / 1875. Loss = 0.4396035671234131\n",
            "Epoch: 0->Batch: 580 / 1875. Loss = 0.5417130589485168\n",
            "Epoch: 0->Batch: 600 / 1875. Loss = 0.9431005716323853\n",
            "Epoch: 0->Batch: 620 / 1875. Loss = 0.5593127608299255\n",
            "Epoch: 0->Batch: 640 / 1875. Loss = 0.6393699645996094\n",
            "Epoch: 0->Batch: 660 / 1875. Loss = 0.6764838695526123\n",
            "Epoch: 0->Batch: 680 / 1875. Loss = 0.7202800512313843\n",
            "Epoch: 0->Batch: 700 / 1875. Loss = 0.8050439953804016\n",
            "Epoch: 0->Batch: 720 / 1875. Loss = 0.6047594547271729\n",
            "Epoch: 0->Batch: 740 / 1875. Loss = 0.5191042423248291\n",
            "Epoch: 0->Batch: 760 / 1875. Loss = 0.501462459564209\n",
            "Epoch: 0->Batch: 780 / 1875. Loss = 0.6162713170051575\n",
            "Epoch: 0->Batch: 800 / 1875. Loss = 0.5351216793060303\n",
            "Epoch: 0->Batch: 820 / 1875. Loss = 0.7365342974662781\n",
            "Epoch: 0->Batch: 840 / 1875. Loss = 0.7184156179428101\n",
            "Epoch: 0->Batch: 860 / 1875. Loss = 0.7381799221038818\n",
            "Epoch: 0->Batch: 880 / 1875. Loss = 0.4387623071670532\n",
            "Epoch: 0->Batch: 900 / 1875. Loss = 0.6898211240768433\n",
            "Epoch: 0->Batch: 920 / 1875. Loss = 0.618187665939331\n",
            "Epoch: 0->Batch: 940 / 1875. Loss = 0.8846167922019958\n",
            "Epoch: 0->Batch: 960 / 1875. Loss = 0.7724244594573975\n",
            "Epoch: 0->Batch: 980 / 1875. Loss = 0.7452936172485352\n",
            "Epoch: 0->Batch: 1000 / 1875. Loss = 0.557483434677124\n",
            "Epoch: 0->Batch: 1020 / 1875. Loss = 0.5584720373153687\n",
            "Epoch: 0->Batch: 1040 / 1875. Loss = 0.5908015370368958\n",
            "Epoch: 0->Batch: 1060 / 1875. Loss = 0.44677096605300903\n",
            "Epoch: 0->Batch: 1080 / 1875. Loss = 0.726657509803772\n",
            "Epoch: 0->Batch: 1100 / 1875. Loss = 0.5503883957862854\n",
            "Epoch: 0->Batch: 1120 / 1875. Loss = 0.23931832611560822\n",
            "Epoch: 0->Batch: 1140 / 1875. Loss = 0.580845296382904\n",
            "Epoch: 0->Batch: 1160 / 1875. Loss = 0.6033657193183899\n",
            "Epoch: 0->Batch: 1180 / 1875. Loss = 0.7416913509368896\n",
            "Epoch: 0->Batch: 1200 / 1875. Loss = 0.6110122799873352\n",
            "Epoch: 0->Batch: 1220 / 1875. Loss = 0.6425905823707581\n",
            "Epoch: 0->Batch: 1240 / 1875. Loss = 0.4403482675552368\n",
            "Epoch: 0->Batch: 1260 / 1875. Loss = 0.5494951009750366\n",
            "Epoch: 0->Batch: 1280 / 1875. Loss = 0.5994739532470703\n",
            "Epoch: 0->Batch: 1300 / 1875. Loss = 0.3289183974266052\n",
            "Epoch: 0->Batch: 1320 / 1875. Loss = 0.40195855498313904\n",
            "Epoch: 0->Batch: 1340 / 1875. Loss = 0.6728782653808594\n",
            "Epoch: 0->Batch: 1360 / 1875. Loss = 0.7172667980194092\n",
            "Epoch: 0->Batch: 1380 / 1875. Loss = 0.4936952590942383\n",
            "Epoch: 0->Batch: 1400 / 1875. Loss = 0.5220867395401001\n",
            "Epoch: 0->Batch: 1420 / 1875. Loss = 0.501406729221344\n",
            "Epoch: 0->Batch: 1440 / 1875. Loss = 0.5485159158706665\n",
            "Epoch: 0->Batch: 1460 / 1875. Loss = 0.46180254220962524\n",
            "Epoch: 0->Batch: 1480 / 1875. Loss = 0.8164697885513306\n",
            "Epoch: 0->Batch: 1500 / 1875. Loss = 0.5552965402603149\n",
            "Epoch: 0->Batch: 1520 / 1875. Loss = 0.5787206888198853\n",
            "Epoch: 0->Batch: 1540 / 1875. Loss = 0.7128875851631165\n",
            "Epoch: 0->Batch: 1560 / 1875. Loss = 0.7136414051055908\n",
            "Epoch: 0->Batch: 1580 / 1875. Loss = 0.4769299626350403\n",
            "Epoch: 0->Batch: 1600 / 1875. Loss = 0.45034119486808777\n",
            "Epoch: 0->Batch: 1620 / 1875. Loss = 0.5632907152175903\n",
            "Epoch: 0->Batch: 1640 / 1875. Loss = 0.5823872089385986\n",
            "Epoch: 0->Batch: 1660 / 1875. Loss = 0.5506786704063416\n",
            "Epoch: 0->Batch: 1680 / 1875. Loss = 0.5812053680419922\n",
            "Epoch: 0->Batch: 1700 / 1875. Loss = 0.6422044038772583\n",
            "Epoch: 0->Batch: 1720 / 1875. Loss = 0.4672566056251526\n",
            "Epoch: 0->Batch: 1740 / 1875. Loss = 0.4441114068031311\n",
            "Epoch: 0->Batch: 1760 / 1875. Loss = 0.33748185634613037\n",
            "Epoch: 0->Batch: 1780 / 1875. Loss = 0.5247417688369751\n",
            "Epoch: 0->Batch: 1800 / 1875. Loss = 0.653545081615448\n",
            "Epoch: 0->Batch: 1820 / 1875. Loss = 0.5647182464599609\n",
            "Epoch: 0->Batch: 1840 / 1875. Loss = 0.5594061613082886\n",
            "Epoch: 0->Batch: 1860 / 1875. Loss = 0.3555517792701721\n",
            "In Testing Function!\n",
            "Accuracy: 0.8139 (8139 / 10000)\n",
            "Epoch: 1 start ------------\n",
            "\n",
            "Epoch: 1->Batch: 0 / 1875. Loss = 0.6512110233306885\n",
            "Epoch: 1->Batch: 20 / 1875. Loss = 0.5291304588317871\n",
            "Epoch: 1->Batch: 40 / 1875. Loss = 0.48848575353622437\n",
            "Epoch: 1->Batch: 60 / 1875. Loss = 0.6819264888763428\n",
            "Epoch: 1->Batch: 80 / 1875. Loss = 0.6437683701515198\n",
            "Epoch: 1->Batch: 100 / 1875. Loss = 0.4111481308937073\n",
            "Epoch: 1->Batch: 120 / 1875. Loss = 0.6037134528160095\n",
            "Epoch: 1->Batch: 140 / 1875. Loss = 0.2565978765487671\n",
            "Epoch: 1->Batch: 160 / 1875. Loss = 0.41229069232940674\n",
            "Epoch: 1->Batch: 180 / 1875. Loss = 0.5582224130630493\n",
            "Epoch: 1->Batch: 200 / 1875. Loss = 0.44467368721961975\n",
            "Epoch: 1->Batch: 220 / 1875. Loss = 0.5089489817619324\n",
            "Epoch: 1->Batch: 240 / 1875. Loss = 0.6439961194992065\n",
            "Epoch: 1->Batch: 260 / 1875. Loss = 0.4783932566642761\n",
            "Epoch: 1->Batch: 280 / 1875. Loss = 0.5700234174728394\n",
            "Epoch: 1->Batch: 300 / 1875. Loss = 0.409729540348053\n",
            "Epoch: 1->Batch: 320 / 1875. Loss = 0.4643166661262512\n",
            "Epoch: 1->Batch: 340 / 1875. Loss = 1.0852909088134766\n",
            "Epoch: 1->Batch: 360 / 1875. Loss = 0.415892630815506\n",
            "Epoch: 1->Batch: 380 / 1875. Loss = 0.582035481929779\n",
            "Epoch: 1->Batch: 400 / 1875. Loss = 0.5223356485366821\n",
            "Epoch: 1->Batch: 420 / 1875. Loss = 0.4467812776565552\n",
            "Epoch: 1->Batch: 440 / 1875. Loss = 0.43802541494369507\n",
            "Epoch: 1->Batch: 460 / 1875. Loss = 0.36817657947540283\n",
            "Epoch: 1->Batch: 480 / 1875. Loss = 0.5228041410446167\n",
            "Epoch: 1->Batch: 500 / 1875. Loss = 0.4228239059448242\n",
            "Epoch: 1->Batch: 520 / 1875. Loss = 0.5673171877861023\n",
            "Epoch: 1->Batch: 540 / 1875. Loss = 0.49266862869262695\n",
            "Epoch: 1->Batch: 560 / 1875. Loss = 0.4454330801963806\n",
            "Epoch: 1->Batch: 580 / 1875. Loss = 0.5469664931297302\n",
            "Epoch: 1->Batch: 600 / 1875. Loss = 0.6634609699249268\n",
            "Epoch: 1->Batch: 620 / 1875. Loss = 0.5343309044837952\n",
            "Epoch: 1->Batch: 640 / 1875. Loss = 0.577376663684845\n",
            "Epoch: 1->Batch: 660 / 1875. Loss = 0.2832629680633545\n",
            "Epoch: 1->Batch: 680 / 1875. Loss = 0.48739200830459595\n",
            "Epoch: 1->Batch: 700 / 1875. Loss = 0.46818894147872925\n",
            "Epoch: 1->Batch: 720 / 1875. Loss = 0.29068657755851746\n",
            "Epoch: 1->Batch: 740 / 1875. Loss = 0.4271795153617859\n",
            "Epoch: 1->Batch: 760 / 1875. Loss = 0.4005715250968933\n",
            "Epoch: 1->Batch: 780 / 1875. Loss = 0.47062596678733826\n",
            "Epoch: 1->Batch: 800 / 1875. Loss = 0.649699866771698\n",
            "Epoch: 1->Batch: 820 / 1875. Loss = 0.4382665753364563\n",
            "Epoch: 1->Batch: 840 / 1875. Loss = 0.6855087280273438\n",
            "Epoch: 1->Batch: 860 / 1875. Loss = 0.36429640650749207\n",
            "Epoch: 1->Batch: 880 / 1875. Loss = 0.6169215440750122\n",
            "Epoch: 1->Batch: 900 / 1875. Loss = 0.7273141741752625\n",
            "Epoch: 1->Batch: 920 / 1875. Loss = 0.4137595593929291\n",
            "Epoch: 1->Batch: 940 / 1875. Loss = 0.3788171410560608\n",
            "Epoch: 1->Batch: 960 / 1875. Loss = 0.6788121461868286\n",
            "Epoch: 1->Batch: 980 / 1875. Loss = 0.26909440755844116\n",
            "Epoch: 1->Batch: 1000 / 1875. Loss = 0.6153085231781006\n",
            "Epoch: 1->Batch: 1020 / 1875. Loss = 0.27321964502334595\n",
            "Epoch: 1->Batch: 1040 / 1875. Loss = 0.5107223391532898\n",
            "Epoch: 1->Batch: 1060 / 1875. Loss = 0.4804365038871765\n",
            "Epoch: 1->Batch: 1080 / 1875. Loss = 0.5441473126411438\n",
            "Epoch: 1->Batch: 1100 / 1875. Loss = 0.39812251925468445\n",
            "Epoch: 1->Batch: 1120 / 1875. Loss = 0.1467370241880417\n",
            "Epoch: 1->Batch: 1140 / 1875. Loss = 0.36637574434280396\n",
            "Epoch: 1->Batch: 1160 / 1875. Loss = 0.5857692956924438\n",
            "Epoch: 1->Batch: 1180 / 1875. Loss = 0.3377959728240967\n",
            "Epoch: 1->Batch: 1200 / 1875. Loss = 0.4175656735897064\n",
            "Epoch: 1->Batch: 1220 / 1875. Loss = 0.5212428569793701\n",
            "Epoch: 1->Batch: 1240 / 1875. Loss = 0.2596604526042938\n",
            "Epoch: 1->Batch: 1260 / 1875. Loss = 0.37791329622268677\n",
            "Epoch: 1->Batch: 1280 / 1875. Loss = 0.4028598964214325\n",
            "Epoch: 1->Batch: 1300 / 1875. Loss = 0.5621756315231323\n",
            "Epoch: 1->Batch: 1320 / 1875. Loss = 0.7804325819015503\n",
            "Epoch: 1->Batch: 1340 / 1875. Loss = 0.6580550670623779\n",
            "Epoch: 1->Batch: 1360 / 1875. Loss = 0.484098345041275\n",
            "Epoch: 1->Batch: 1380 / 1875. Loss = 0.3981931209564209\n",
            "Epoch: 1->Batch: 1400 / 1875. Loss = 0.5242597460746765\n",
            "Epoch: 1->Batch: 1420 / 1875. Loss = 0.3923870623111725\n",
            "Epoch: 1->Batch: 1440 / 1875. Loss = 0.564128577709198\n",
            "Epoch: 1->Batch: 1460 / 1875. Loss = 0.25428664684295654\n",
            "Epoch: 1->Batch: 1480 / 1875. Loss = 0.5902676582336426\n",
            "Epoch: 1->Batch: 1500 / 1875. Loss = 0.5340807437896729\n",
            "Epoch: 1->Batch: 1520 / 1875. Loss = 0.25056275725364685\n",
            "Epoch: 1->Batch: 1540 / 1875. Loss = 0.41570472717285156\n",
            "Epoch: 1->Batch: 1560 / 1875. Loss = 0.4348689615726471\n",
            "Epoch: 1->Batch: 1580 / 1875. Loss = 0.3746036887168884\n",
            "Epoch: 1->Batch: 1600 / 1875. Loss = 0.29566508531570435\n",
            "Epoch: 1->Batch: 1620 / 1875. Loss = 0.4969553053379059\n",
            "Epoch: 1->Batch: 1640 / 1875. Loss = 0.6372110843658447\n",
            "Epoch: 1->Batch: 1660 / 1875. Loss = 0.4479806423187256\n",
            "Epoch: 1->Batch: 1680 / 1875. Loss = 0.44307252764701843\n",
            "Epoch: 1->Batch: 1700 / 1875. Loss = 0.458326518535614\n",
            "Epoch: 1->Batch: 1720 / 1875. Loss = 0.4217982888221741\n",
            "Epoch: 1->Batch: 1740 / 1875. Loss = 0.3979756236076355\n",
            "Epoch: 1->Batch: 1760 / 1875. Loss = 0.4897462725639343\n",
            "Epoch: 1->Batch: 1780 / 1875. Loss = 0.26160508394241333\n",
            "Epoch: 1->Batch: 1800 / 1875. Loss = 0.5223137140274048\n",
            "Epoch: 1->Batch: 1820 / 1875. Loss = 0.30429935455322266\n",
            "Epoch: 1->Batch: 1840 / 1875. Loss = 0.40669548511505127\n",
            "Epoch: 1->Batch: 1860 / 1875. Loss = 0.22609275579452515\n",
            "In Testing Function!\n",
            "Accuracy: 0.8451 (8451 / 10000)\n",
            "Epoch: 2 start ------------\n",
            "\n",
            "Epoch: 2->Batch: 0 / 1875. Loss = 0.4206047058105469\n",
            "Epoch: 2->Batch: 20 / 1875. Loss = 0.6042729616165161\n",
            "Epoch: 2->Batch: 40 / 1875. Loss = 0.5595507025718689\n",
            "Epoch: 2->Batch: 60 / 1875. Loss = 0.37651997804641724\n",
            "Epoch: 2->Batch: 80 / 1875. Loss = 0.48757117986679077\n",
            "Epoch: 2->Batch: 100 / 1875. Loss = 0.4512275457382202\n",
            "Epoch: 2->Batch: 120 / 1875. Loss = 0.5051316618919373\n",
            "Epoch: 2->Batch: 140 / 1875. Loss = 0.6668534278869629\n",
            "Epoch: 2->Batch: 160 / 1875. Loss = 0.5108954906463623\n",
            "Epoch: 2->Batch: 180 / 1875. Loss = 0.3736194968223572\n",
            "Epoch: 2->Batch: 200 / 1875. Loss = 0.44836685061454773\n",
            "Epoch: 2->Batch: 220 / 1875. Loss = 0.555094838142395\n",
            "Epoch: 2->Batch: 240 / 1875. Loss = 0.371593713760376\n",
            "Epoch: 2->Batch: 260 / 1875. Loss = 0.40032675862312317\n",
            "Epoch: 2->Batch: 280 / 1875. Loss = 0.26159563660621643\n",
            "Epoch: 2->Batch: 300 / 1875. Loss = 0.6380202770233154\n",
            "Epoch: 2->Batch: 320 / 1875. Loss = 0.22613191604614258\n",
            "Epoch: 2->Batch: 340 / 1875. Loss = 0.3786025941371918\n",
            "Epoch: 2->Batch: 360 / 1875. Loss = 0.3362230956554413\n",
            "Epoch: 2->Batch: 380 / 1875. Loss = 0.36931243538856506\n",
            "Epoch: 2->Batch: 400 / 1875. Loss = 0.4014335870742798\n",
            "Epoch: 2->Batch: 420 / 1875. Loss = 0.22795352339744568\n",
            "Epoch: 2->Batch: 440 / 1875. Loss = 0.43016523122787476\n",
            "Epoch: 2->Batch: 460 / 1875. Loss = 0.43282026052474976\n",
            "Epoch: 2->Batch: 480 / 1875. Loss = 0.5046621561050415\n",
            "Epoch: 2->Batch: 500 / 1875. Loss = 0.42983734607696533\n",
            "Epoch: 2->Batch: 520 / 1875. Loss = 0.33073344826698303\n",
            "Epoch: 2->Batch: 540 / 1875. Loss = 0.3379095792770386\n",
            "Epoch: 2->Batch: 560 / 1875. Loss = 0.4670411944389343\n",
            "Epoch: 2->Batch: 580 / 1875. Loss = 0.37594154477119446\n",
            "Epoch: 2->Batch: 600 / 1875. Loss = 0.21245437860488892\n",
            "Epoch: 2->Batch: 620 / 1875. Loss = 0.2969198226928711\n",
            "Epoch: 2->Batch: 640 / 1875. Loss = 0.5375308990478516\n",
            "Epoch: 2->Batch: 660 / 1875. Loss = 0.5203425884246826\n",
            "Epoch: 2->Batch: 680 / 1875. Loss = 0.46173638105392456\n",
            "Epoch: 2->Batch: 700 / 1875. Loss = 0.4610935151576996\n",
            "Epoch: 2->Batch: 720 / 1875. Loss = 0.5878482460975647\n",
            "Epoch: 2->Batch: 740 / 1875. Loss = 0.4795289635658264\n",
            "Epoch: 2->Batch: 760 / 1875. Loss = 0.3332722783088684\n",
            "Epoch: 2->Batch: 780 / 1875. Loss = 0.23217102885246277\n",
            "Epoch: 2->Batch: 800 / 1875. Loss = 0.36726656556129456\n",
            "Epoch: 2->Batch: 820 / 1875. Loss = 0.5419430732727051\n",
            "Epoch: 2->Batch: 840 / 1875. Loss = 0.293830543756485\n",
            "Epoch: 2->Batch: 860 / 1875. Loss = 0.5240230560302734\n",
            "Epoch: 2->Batch: 880 / 1875. Loss = 0.31501415371894836\n",
            "Epoch: 2->Batch: 900 / 1875. Loss = 0.3506891131401062\n",
            "Epoch: 2->Batch: 920 / 1875. Loss = 0.4471278190612793\n",
            "Epoch: 2->Batch: 940 / 1875. Loss = 0.35951361060142517\n",
            "Epoch: 2->Batch: 960 / 1875. Loss = 0.2561973035335541\n",
            "Epoch: 2->Batch: 980 / 1875. Loss = 0.35562074184417725\n",
            "Epoch: 2->Batch: 1000 / 1875. Loss = 0.43616247177124023\n",
            "Epoch: 2->Batch: 1020 / 1875. Loss = 0.5339953899383545\n",
            "Epoch: 2->Batch: 1040 / 1875. Loss = 0.34108269214630127\n",
            "Epoch: 2->Batch: 1060 / 1875. Loss = 0.3826562762260437\n",
            "Epoch: 2->Batch: 1080 / 1875. Loss = 0.46983638405799866\n",
            "Epoch: 2->Batch: 1100 / 1875. Loss = 0.4962264895439148\n",
            "Epoch: 2->Batch: 1120 / 1875. Loss = 0.32981863617897034\n",
            "Epoch: 2->Batch: 1140 / 1875. Loss = 0.4378123879432678\n",
            "Epoch: 2->Batch: 1160 / 1875. Loss = 0.41204172372817993\n",
            "Epoch: 2->Batch: 1180 / 1875. Loss = 0.4135899841785431\n",
            "Epoch: 2->Batch: 1200 / 1875. Loss = 0.15717713534832\n",
            "Epoch: 2->Batch: 1220 / 1875. Loss = 0.3477330505847931\n",
            "Epoch: 2->Batch: 1240 / 1875. Loss = 0.2490294724702835\n",
            "Epoch: 2->Batch: 1260 / 1875. Loss = 0.31721779704093933\n",
            "Epoch: 2->Batch: 1280 / 1875. Loss = 0.6288102269172668\n",
            "Epoch: 2->Batch: 1300 / 1875. Loss = 0.1929074227809906\n",
            "Epoch: 2->Batch: 1320 / 1875. Loss = 0.7535439729690552\n",
            "Epoch: 2->Batch: 1340 / 1875. Loss = 0.523841142654419\n",
            "Epoch: 2->Batch: 1360 / 1875. Loss = 0.3657798767089844\n",
            "Epoch: 2->Batch: 1380 / 1875. Loss = 0.7439826130867004\n",
            "Epoch: 2->Batch: 1400 / 1875. Loss = 0.24177542328834534\n",
            "Epoch: 2->Batch: 1420 / 1875. Loss = 0.4670372009277344\n",
            "Epoch: 2->Batch: 1440 / 1875. Loss = 0.538931131362915\n",
            "Epoch: 2->Batch: 1460 / 1875. Loss = 0.5069941282272339\n",
            "Epoch: 2->Batch: 1480 / 1875. Loss = 0.382055401802063\n",
            "Epoch: 2->Batch: 1500 / 1875. Loss = 0.5143023729324341\n",
            "Epoch: 2->Batch: 1520 / 1875. Loss = 0.4151374399662018\n",
            "Epoch: 2->Batch: 1540 / 1875. Loss = 0.3137841820716858\n",
            "Epoch: 2->Batch: 1560 / 1875. Loss = 0.24082347750663757\n",
            "Epoch: 2->Batch: 1580 / 1875. Loss = 0.5142052173614502\n",
            "Epoch: 2->Batch: 1600 / 1875. Loss = 0.44456642866134644\n",
            "Epoch: 2->Batch: 1620 / 1875. Loss = 0.4115391969680786\n",
            "Epoch: 2->Batch: 1640 / 1875. Loss = 0.5224663019180298\n",
            "Epoch: 2->Batch: 1660 / 1875. Loss = 0.3230500817298889\n",
            "Epoch: 2->Batch: 1680 / 1875. Loss = 0.510493278503418\n",
            "Epoch: 2->Batch: 1700 / 1875. Loss = 0.4350976049900055\n",
            "Epoch: 2->Batch: 1720 / 1875. Loss = 0.3866283595561981\n",
            "Epoch: 2->Batch: 1740 / 1875. Loss = 0.47877177596092224\n",
            "Epoch: 2->Batch: 1760 / 1875. Loss = 0.5748833417892456\n",
            "Epoch: 2->Batch: 1780 / 1875. Loss = 0.12911008298397064\n",
            "Epoch: 2->Batch: 1800 / 1875. Loss = 0.34563493728637695\n",
            "Epoch: 2->Batch: 1820 / 1875. Loss = 0.22207669913768768\n",
            "Epoch: 2->Batch: 1840 / 1875. Loss = 0.38243377208709717\n",
            "Epoch: 2->Batch: 1860 / 1875. Loss = 0.25956010818481445\n",
            "In Testing Function!\n",
            "Accuracy: 0.86 (8600 / 10000)\n",
            "Epoch: 3 start ------------\n",
            "\n",
            "Epoch: 3->Batch: 0 / 1875. Loss = 0.4536234140396118\n",
            "Epoch: 3->Batch: 20 / 1875. Loss = 0.2782834768295288\n",
            "Epoch: 3->Batch: 40 / 1875. Loss = 0.2934316396713257\n",
            "Epoch: 3->Batch: 60 / 1875. Loss = 0.3709467053413391\n",
            "Epoch: 3->Batch: 80 / 1875. Loss = 0.35437363386154175\n",
            "Epoch: 3->Batch: 100 / 1875. Loss = 0.25079774856567383\n",
            "Epoch: 3->Batch: 120 / 1875. Loss = 0.26627665758132935\n",
            "Epoch: 3->Batch: 140 / 1875. Loss = 0.2848937511444092\n",
            "Epoch: 3->Batch: 160 / 1875. Loss = 0.3242861330509186\n",
            "Epoch: 3->Batch: 180 / 1875. Loss = 0.40270304679870605\n",
            "Epoch: 3->Batch: 200 / 1875. Loss = 0.4145803153514862\n",
            "Epoch: 3->Batch: 220 / 1875. Loss = 0.33790040016174316\n",
            "Epoch: 3->Batch: 240 / 1875. Loss = 0.4073946177959442\n",
            "Epoch: 3->Batch: 260 / 1875. Loss = 0.17388391494750977\n",
            "Epoch: 3->Batch: 280 / 1875. Loss = 0.23998573422431946\n",
            "Epoch: 3->Batch: 300 / 1875. Loss = 0.21773822605609894\n",
            "Epoch: 3->Batch: 320 / 1875. Loss = 0.4265269637107849\n",
            "Epoch: 3->Batch: 340 / 1875. Loss = 0.37834522128105164\n",
            "Epoch: 3->Batch: 360 / 1875. Loss = 0.5310875177383423\n",
            "Epoch: 3->Batch: 380 / 1875. Loss = 0.41760116815567017\n",
            "Epoch: 3->Batch: 400 / 1875. Loss = 0.3954417109489441\n",
            "Epoch: 3->Batch: 420 / 1875. Loss = 0.30131271481513977\n",
            "Epoch: 3->Batch: 440 / 1875. Loss = 0.5415109395980835\n",
            "Epoch: 3->Batch: 460 / 1875. Loss = 0.419434517621994\n",
            "Epoch: 3->Batch: 480 / 1875. Loss = 0.42694777250289917\n",
            "Epoch: 3->Batch: 500 / 1875. Loss = 0.4769541621208191\n",
            "Epoch: 3->Batch: 520 / 1875. Loss = 0.32558995485305786\n",
            "Epoch: 3->Batch: 540 / 1875. Loss = 0.33965611457824707\n",
            "Epoch: 3->Batch: 560 / 1875. Loss = 0.3779265284538269\n",
            "Epoch: 3->Batch: 580 / 1875. Loss = 0.22217029333114624\n",
            "Epoch: 3->Batch: 600 / 1875. Loss = 0.2750554382801056\n",
            "Epoch: 3->Batch: 620 / 1875. Loss = 0.5912764668464661\n",
            "Epoch: 3->Batch: 640 / 1875. Loss = 0.3333601653575897\n",
            "Epoch: 3->Batch: 660 / 1875. Loss = 0.31527286767959595\n",
            "Epoch: 3->Batch: 680 / 1875. Loss = 0.2806621789932251\n",
            "Epoch: 3->Batch: 700 / 1875. Loss = 0.1723974347114563\n",
            "Epoch: 3->Batch: 720 / 1875. Loss = 0.2483004331588745\n",
            "Epoch: 3->Batch: 740 / 1875. Loss = 0.4288704991340637\n",
            "Epoch: 3->Batch: 760 / 1875. Loss = 0.32555705308914185\n",
            "Epoch: 3->Batch: 780 / 1875. Loss = 0.23629476130008698\n",
            "Epoch: 3->Batch: 800 / 1875. Loss = 0.24754011631011963\n",
            "Epoch: 3->Batch: 820 / 1875. Loss = 0.29102230072021484\n",
            "Epoch: 3->Batch: 840 / 1875. Loss = 0.616400957107544\n",
            "Epoch: 3->Batch: 860 / 1875. Loss = 0.32015150785446167\n",
            "Epoch: 3->Batch: 880 / 1875. Loss = 0.26525411009788513\n",
            "Epoch: 3->Batch: 900 / 1875. Loss = 0.5039907693862915\n",
            "Epoch: 3->Batch: 920 / 1875. Loss = 0.29076290130615234\n",
            "Epoch: 3->Batch: 940 / 1875. Loss = 0.4669761657714844\n",
            "Epoch: 3->Batch: 960 / 1875. Loss = 0.397305965423584\n",
            "Epoch: 3->Batch: 980 / 1875. Loss = 0.38203758001327515\n",
            "Epoch: 3->Batch: 1000 / 1875. Loss = 0.25450047850608826\n",
            "Epoch: 3->Batch: 1020 / 1875. Loss = 0.5514593720436096\n",
            "Epoch: 3->Batch: 1040 / 1875. Loss = 0.25850561261177063\n",
            "Epoch: 3->Batch: 1060 / 1875. Loss = 0.2364734262228012\n",
            "Epoch: 3->Batch: 1080 / 1875. Loss = 0.1651260405778885\n",
            "Epoch: 3->Batch: 1100 / 1875. Loss = 0.3800828456878662\n",
            "Epoch: 3->Batch: 1120 / 1875. Loss = 0.17172573506832123\n",
            "Epoch: 3->Batch: 1140 / 1875. Loss = 0.29617321491241455\n",
            "Epoch: 3->Batch: 1160 / 1875. Loss = 0.3134022355079651\n",
            "Epoch: 3->Batch: 1180 / 1875. Loss = 0.19911305606365204\n",
            "Epoch: 3->Batch: 1200 / 1875. Loss = 0.36303773522377014\n",
            "Epoch: 3->Batch: 1220 / 1875. Loss = 0.3123566508293152\n",
            "Epoch: 3->Batch: 1240 / 1875. Loss = 0.3636055588722229\n",
            "Epoch: 3->Batch: 1260 / 1875. Loss = 0.2155054360628128\n",
            "Epoch: 3->Batch: 1280 / 1875. Loss = 0.4437286853790283\n",
            "Epoch: 3->Batch: 1300 / 1875. Loss = 0.3631274998188019\n",
            "Epoch: 3->Batch: 1320 / 1875. Loss = 0.6116002798080444\n",
            "Epoch: 3->Batch: 1340 / 1875. Loss = 0.556720495223999\n",
            "Epoch: 3->Batch: 1360 / 1875. Loss = 0.22086605429649353\n",
            "Epoch: 3->Batch: 1380 / 1875. Loss = 0.32849442958831787\n",
            "Epoch: 3->Batch: 1400 / 1875. Loss = 0.2552340030670166\n",
            "Epoch: 3->Batch: 1420 / 1875. Loss = 0.5464214086532593\n",
            "Epoch: 3->Batch: 1440 / 1875. Loss = 0.35278603434562683\n",
            "Epoch: 3->Batch: 1460 / 1875. Loss = 0.22663037478923798\n",
            "Epoch: 3->Batch: 1480 / 1875. Loss = 0.36798983812332153\n",
            "Epoch: 3->Batch: 1500 / 1875. Loss = 0.32337766885757446\n",
            "Epoch: 3->Batch: 1520 / 1875. Loss = 0.28875473141670227\n",
            "Epoch: 3->Batch: 1540 / 1875. Loss = 0.22405017912387848\n",
            "Epoch: 3->Batch: 1560 / 1875. Loss = 0.3254355788230896\n",
            "Epoch: 3->Batch: 1580 / 1875. Loss = 0.565939724445343\n",
            "Epoch: 3->Batch: 1600 / 1875. Loss = 0.34887444972991943\n",
            "Epoch: 3->Batch: 1620 / 1875. Loss = 0.4272887408733368\n",
            "Epoch: 3->Batch: 1640 / 1875. Loss = 0.20309942960739136\n",
            "Epoch: 3->Batch: 1660 / 1875. Loss = 0.42862266302108765\n",
            "Epoch: 3->Batch: 1680 / 1875. Loss = 0.42666149139404297\n",
            "Epoch: 3->Batch: 1700 / 1875. Loss = 0.23200970888137817\n",
            "Epoch: 3->Batch: 1720 / 1875. Loss = 0.31499725580215454\n",
            "Epoch: 3->Batch: 1740 / 1875. Loss = 0.42034968733787537\n",
            "Epoch: 3->Batch: 1760 / 1875. Loss = 0.28387540578842163\n",
            "Epoch: 3->Batch: 1780 / 1875. Loss = 0.18674308061599731\n",
            "Epoch: 3->Batch: 1800 / 1875. Loss = 0.2995128035545349\n",
            "Epoch: 3->Batch: 1820 / 1875. Loss = 0.4241323173046112\n",
            "Epoch: 3->Batch: 1840 / 1875. Loss = 0.39428311586380005\n",
            "Epoch: 3->Batch: 1860 / 1875. Loss = 0.3435784876346588\n",
            "In Testing Function!\n",
            "Accuracy: 0.8662 (8662 / 10000)\n",
            "Epoch: 4 start ------------\n",
            "\n",
            "Epoch: 4->Batch: 0 / 1875. Loss = 0.45343083143234253\n",
            "Epoch: 4->Batch: 20 / 1875. Loss = 0.3188976049423218\n",
            "Epoch: 4->Batch: 40 / 1875. Loss = 0.22179476916790009\n",
            "Epoch: 4->Batch: 60 / 1875. Loss = 0.23922206461429596\n",
            "Epoch: 4->Batch: 80 / 1875. Loss = 0.2076607197523117\n",
            "Epoch: 4->Batch: 100 / 1875. Loss = 0.2864142954349518\n",
            "Epoch: 4->Batch: 120 / 1875. Loss = 0.41661667823791504\n",
            "Epoch: 4->Batch: 140 / 1875. Loss = 0.38872411847114563\n",
            "Epoch: 4->Batch: 160 / 1875. Loss = 0.41987740993499756\n",
            "Epoch: 4->Batch: 180 / 1875. Loss = 0.3012276887893677\n",
            "Epoch: 4->Batch: 200 / 1875. Loss = 0.5068725943565369\n",
            "Epoch: 4->Batch: 220 / 1875. Loss = 0.4374479353427887\n",
            "Epoch: 4->Batch: 240 / 1875. Loss = 0.2393166571855545\n",
            "Epoch: 4->Batch: 260 / 1875. Loss = 0.29365965723991394\n",
            "Epoch: 4->Batch: 280 / 1875. Loss = 0.35795682668685913\n",
            "Epoch: 4->Batch: 300 / 1875. Loss = 0.6009824275970459\n",
            "Epoch: 4->Batch: 320 / 1875. Loss = 0.40666434168815613\n",
            "Epoch: 4->Batch: 340 / 1875. Loss = 0.44993215799331665\n",
            "Epoch: 4->Batch: 360 / 1875. Loss = 0.30743345618247986\n",
            "Epoch: 4->Batch: 380 / 1875. Loss = 0.28226110339164734\n",
            "Epoch: 4->Batch: 400 / 1875. Loss = 0.5589300394058228\n",
            "Epoch: 4->Batch: 420 / 1875. Loss = 0.47445282340049744\n",
            "Epoch: 4->Batch: 440 / 1875. Loss = 0.30922389030456543\n",
            "Epoch: 4->Batch: 460 / 1875. Loss = 0.26739972829818726\n",
            "Epoch: 4->Batch: 480 / 1875. Loss = 0.13811556994915009\n",
            "Epoch: 4->Batch: 500 / 1875. Loss = 0.27623170614242554\n",
            "Epoch: 4->Batch: 520 / 1875. Loss = 0.2660366892814636\n",
            "Epoch: 4->Batch: 540 / 1875. Loss = 0.2015429139137268\n",
            "Epoch: 4->Batch: 560 / 1875. Loss = 0.20480920374393463\n",
            "Epoch: 4->Batch: 580 / 1875. Loss = 0.5375697612762451\n",
            "Epoch: 4->Batch: 600 / 1875. Loss = 0.20739459991455078\n",
            "Epoch: 4->Batch: 620 / 1875. Loss = 0.2905171513557434\n",
            "Epoch: 4->Batch: 640 / 1875. Loss = 0.28384506702423096\n",
            "Epoch: 4->Batch: 660 / 1875. Loss = 0.29810601472854614\n",
            "Epoch: 4->Batch: 680 / 1875. Loss = 0.28774893283843994\n",
            "Epoch: 4->Batch: 700 / 1875. Loss = 0.4944784641265869\n",
            "Epoch: 4->Batch: 720 / 1875. Loss = 0.3486597537994385\n",
            "Epoch: 4->Batch: 740 / 1875. Loss = 0.40389081835746765\n",
            "Epoch: 4->Batch: 760 / 1875. Loss = 0.18742646276950836\n",
            "Epoch: 4->Batch: 780 / 1875. Loss = 0.296144038438797\n",
            "Epoch: 4->Batch: 800 / 1875. Loss = 0.257607638835907\n",
            "Epoch: 4->Batch: 820 / 1875. Loss = 0.23963314294815063\n",
            "Epoch: 4->Batch: 840 / 1875. Loss = 0.51938796043396\n",
            "Epoch: 4->Batch: 860 / 1875. Loss = 0.4606477618217468\n",
            "Epoch: 4->Batch: 880 / 1875. Loss = 0.2124810665845871\n",
            "Epoch: 4->Batch: 900 / 1875. Loss = 0.18135543167591095\n",
            "Epoch: 4->Batch: 920 / 1875. Loss = 0.1776411086320877\n",
            "Epoch: 4->Batch: 940 / 1875. Loss = 0.302422434091568\n",
            "Epoch: 4->Batch: 960 / 1875. Loss = 0.27036550641059875\n",
            "Epoch: 4->Batch: 980 / 1875. Loss = 0.2085290253162384\n",
            "Epoch: 4->Batch: 1000 / 1875. Loss = 0.32119038701057434\n",
            "Epoch: 4->Batch: 1020 / 1875. Loss = 0.0931253433227539\n",
            "Epoch: 4->Batch: 1040 / 1875. Loss = 0.5191316604614258\n",
            "Epoch: 4->Batch: 1060 / 1875. Loss = 0.375016987323761\n",
            "Epoch: 4->Batch: 1080 / 1875. Loss = 0.20059940218925476\n",
            "Epoch: 4->Batch: 1100 / 1875. Loss = 0.2209625542163849\n",
            "Epoch: 4->Batch: 1120 / 1875. Loss = 0.29017335176467896\n",
            "Epoch: 4->Batch: 1140 / 1875. Loss = 0.33494749665260315\n",
            "Epoch: 4->Batch: 1160 / 1875. Loss = 0.2997143864631653\n",
            "Epoch: 4->Batch: 1180 / 1875. Loss = 0.18620730936527252\n",
            "Epoch: 4->Batch: 1200 / 1875. Loss = 0.39772388339042664\n",
            "Epoch: 4->Batch: 1220 / 1875. Loss = 0.3528401255607605\n",
            "Epoch: 4->Batch: 1240 / 1875. Loss = 0.5060158967971802\n",
            "Epoch: 4->Batch: 1260 / 1875. Loss = 0.2754490375518799\n",
            "Epoch: 4->Batch: 1280 / 1875. Loss = 0.4959763288497925\n",
            "Epoch: 4->Batch: 1300 / 1875. Loss = 0.30862629413604736\n",
            "Epoch: 4->Batch: 1320 / 1875. Loss = 0.3719768822193146\n",
            "Epoch: 4->Batch: 1340 / 1875. Loss = 0.19250141084194183\n",
            "Epoch: 4->Batch: 1360 / 1875. Loss = 0.3535340428352356\n",
            "Epoch: 4->Batch: 1380 / 1875. Loss = 0.4481717348098755\n",
            "Epoch: 4->Batch: 1400 / 1875. Loss = 0.28740572929382324\n",
            "Epoch: 4->Batch: 1420 / 1875. Loss = 0.2867642641067505\n",
            "Epoch: 4->Batch: 1440 / 1875. Loss = 0.5092325210571289\n",
            "Epoch: 4->Batch: 1460 / 1875. Loss = 0.345356822013855\n",
            "Epoch: 4->Batch: 1480 / 1875. Loss = 0.3984875977039337\n",
            "Epoch: 4->Batch: 1500 / 1875. Loss = 0.3481324017047882\n",
            "Epoch: 4->Batch: 1520 / 1875. Loss = 0.32751360535621643\n",
            "Epoch: 4->Batch: 1540 / 1875. Loss = 0.21086879074573517\n",
            "Epoch: 4->Batch: 1560 / 1875. Loss = 0.374362587928772\n",
            "Epoch: 4->Batch: 1580 / 1875. Loss = 0.3996616303920746\n",
            "Epoch: 4->Batch: 1600 / 1875. Loss = 0.5537009835243225\n",
            "Epoch: 4->Batch: 1620 / 1875. Loss = 0.5484784841537476\n",
            "Epoch: 4->Batch: 1640 / 1875. Loss = 0.3843595087528229\n",
            "Epoch: 4->Batch: 1660 / 1875. Loss = 0.2580016851425171\n",
            "Epoch: 4->Batch: 1680 / 1875. Loss = 0.47748124599456787\n",
            "Epoch: 4->Batch: 1700 / 1875. Loss = 0.3204566538333893\n",
            "Epoch: 4->Batch: 1720 / 1875. Loss = 0.19261842966079712\n",
            "Epoch: 4->Batch: 1740 / 1875. Loss = 0.2800889313220978\n",
            "Epoch: 4->Batch: 1760 / 1875. Loss = 0.5292140245437622\n",
            "Epoch: 4->Batch: 1780 / 1875. Loss = 0.37161773443222046\n",
            "Epoch: 4->Batch: 1800 / 1875. Loss = 0.324317991733551\n",
            "Epoch: 4->Batch: 1820 / 1875. Loss = 0.39021554589271545\n",
            "Epoch: 4->Batch: 1840 / 1875. Loss = 0.6292359828948975\n",
            "Epoch: 4->Batch: 1860 / 1875. Loss = 0.5005314350128174\n",
            "In Testing Function!\n",
            "Accuracy: 0.8711 (8711 / 10000)\n",
            "Epoch: 5 start ------------\n",
            "\n",
            "Epoch: 5->Batch: 0 / 1875. Loss = 0.6287221312522888\n",
            "Epoch: 5->Batch: 20 / 1875. Loss = 0.2206256091594696\n",
            "Epoch: 5->Batch: 40 / 1875. Loss = 0.2888944447040558\n",
            "Epoch: 5->Batch: 60 / 1875. Loss = 0.5021368265151978\n",
            "Epoch: 5->Batch: 80 / 1875. Loss = 0.3743649125099182\n",
            "Epoch: 5->Batch: 100 / 1875. Loss = 0.21156471967697144\n",
            "Epoch: 5->Batch: 120 / 1875. Loss = 0.24318058788776398\n",
            "Epoch: 5->Batch: 140 / 1875. Loss = 0.3133165240287781\n",
            "Epoch: 5->Batch: 160 / 1875. Loss = 0.3928735554218292\n",
            "Epoch: 5->Batch: 180 / 1875. Loss = 0.5157594680786133\n",
            "Epoch: 5->Batch: 200 / 1875. Loss = 0.36538761854171753\n",
            "Epoch: 5->Batch: 220 / 1875. Loss = 0.7086934447288513\n",
            "Epoch: 5->Batch: 240 / 1875. Loss = 0.2609776556491852\n",
            "Epoch: 5->Batch: 260 / 1875. Loss = 0.2884331941604614\n",
            "Epoch: 5->Batch: 280 / 1875. Loss = 0.31243467330932617\n",
            "Epoch: 5->Batch: 300 / 1875. Loss = 0.24840031564235687\n",
            "Epoch: 5->Batch: 320 / 1875. Loss = 0.5524303913116455\n",
            "Epoch: 5->Batch: 340 / 1875. Loss = 0.45640844106674194\n",
            "Epoch: 5->Batch: 360 / 1875. Loss = 0.1766698658466339\n",
            "Epoch: 5->Batch: 380 / 1875. Loss = 0.29796740412712097\n",
            "Epoch: 5->Batch: 400 / 1875. Loss = 0.7862353324890137\n",
            "Epoch: 5->Batch: 420 / 1875. Loss = 0.14344854652881622\n",
            "Epoch: 5->Batch: 440 / 1875. Loss = 0.15458916127681732\n",
            "Epoch: 5->Batch: 460 / 1875. Loss = 0.25304123759269714\n",
            "Epoch: 5->Batch: 480 / 1875. Loss = 0.31931033730506897\n",
            "Epoch: 5->Batch: 500 / 1875. Loss = 0.271872341632843\n",
            "Epoch: 5->Batch: 520 / 1875. Loss = 0.23368731141090393\n",
            "Epoch: 5->Batch: 540 / 1875. Loss = 0.23927491903305054\n",
            "Epoch: 5->Batch: 560 / 1875. Loss = 0.36224496364593506\n",
            "Epoch: 5->Batch: 580 / 1875. Loss = 0.2666381001472473\n",
            "Epoch: 5->Batch: 600 / 1875. Loss = 0.3513653874397278\n",
            "Epoch: 5->Batch: 620 / 1875. Loss = 0.24808548390865326\n",
            "Epoch: 5->Batch: 640 / 1875. Loss = 0.21782509982585907\n",
            "Epoch: 5->Batch: 660 / 1875. Loss = 0.7128118872642517\n",
            "Epoch: 5->Batch: 680 / 1875. Loss = 0.19535669684410095\n",
            "Epoch: 5->Batch: 700 / 1875. Loss = 0.24041402339935303\n",
            "Epoch: 5->Batch: 720 / 1875. Loss = 0.33304786682128906\n",
            "Epoch: 5->Batch: 740 / 1875. Loss = 0.20051677525043488\n",
            "Epoch: 5->Batch: 760 / 1875. Loss = 0.4964402914047241\n",
            "Epoch: 5->Batch: 780 / 1875. Loss = 0.40733203291893005\n",
            "Epoch: 5->Batch: 800 / 1875. Loss = 0.35102957487106323\n",
            "Epoch: 5->Batch: 820 / 1875. Loss = 0.26496511697769165\n",
            "Epoch: 5->Batch: 840 / 1875. Loss = 0.22939911484718323\n",
            "Epoch: 5->Batch: 860 / 1875. Loss = 0.31728264689445496\n",
            "Epoch: 5->Batch: 880 / 1875. Loss = 0.16772453486919403\n",
            "Epoch: 5->Batch: 900 / 1875. Loss = 0.43374964594841003\n",
            "Epoch: 5->Batch: 920 / 1875. Loss = 0.24451975524425507\n",
            "Epoch: 5->Batch: 940 / 1875. Loss = 0.34122663736343384\n",
            "Epoch: 5->Batch: 960 / 1875. Loss = 0.31606897711753845\n",
            "Epoch: 5->Batch: 980 / 1875. Loss = 0.29190441966056824\n",
            "Epoch: 5->Batch: 1000 / 1875. Loss = 0.14913129806518555\n",
            "Epoch: 5->Batch: 1020 / 1875. Loss = 0.3671553134918213\n",
            "Epoch: 5->Batch: 1040 / 1875. Loss = 0.331623375415802\n",
            "Epoch: 5->Batch: 1060 / 1875. Loss = 0.5105412602424622\n",
            "Epoch: 5->Batch: 1080 / 1875. Loss = 0.5946160554885864\n",
            "Epoch: 5->Batch: 1100 / 1875. Loss = 0.2607506215572357\n",
            "Epoch: 5->Batch: 1120 / 1875. Loss = 0.17594584822654724\n",
            "Epoch: 5->Batch: 1140 / 1875. Loss = 0.3972032368183136\n",
            "Epoch: 5->Batch: 1160 / 1875. Loss = 0.12159641087055206\n",
            "Epoch: 5->Batch: 1180 / 1875. Loss = 0.09417089819908142\n",
            "Epoch: 5->Batch: 1200 / 1875. Loss = 0.21482129395008087\n",
            "Epoch: 5->Batch: 1220 / 1875. Loss = 0.308418333530426\n",
            "Epoch: 5->Batch: 1240 / 1875. Loss = 0.515221357345581\n",
            "Epoch: 5->Batch: 1260 / 1875. Loss = 0.4822719991207123\n",
            "Epoch: 5->Batch: 1280 / 1875. Loss = 0.37704864144325256\n",
            "Epoch: 5->Batch: 1300 / 1875. Loss = 0.24532219767570496\n",
            "Epoch: 5->Batch: 1320 / 1875. Loss = 0.32701146602630615\n",
            "Epoch: 5->Batch: 1340 / 1875. Loss = 0.22060632705688477\n",
            "Epoch: 5->Batch: 1360 / 1875. Loss = 0.3379719853401184\n",
            "Epoch: 5->Batch: 1380 / 1875. Loss = 0.2906726598739624\n",
            "Epoch: 5->Batch: 1400 / 1875. Loss = 0.18948982656002045\n",
            "Epoch: 5->Batch: 1420 / 1875. Loss = 0.14466553926467896\n",
            "Epoch: 5->Batch: 1440 / 1875. Loss = 0.24158485233783722\n",
            "Epoch: 5->Batch: 1460 / 1875. Loss = 0.3726911246776581\n",
            "Epoch: 5->Batch: 1480 / 1875. Loss = 0.5363727807998657\n",
            "Epoch: 5->Batch: 1500 / 1875. Loss = 0.22017118334770203\n",
            "Epoch: 5->Batch: 1520 / 1875. Loss = 0.3683658242225647\n",
            "Epoch: 5->Batch: 1540 / 1875. Loss = 0.4827072024345398\n",
            "Epoch: 5->Batch: 1560 / 1875. Loss = 0.3488151431083679\n",
            "Epoch: 5->Batch: 1580 / 1875. Loss = 0.34175464510917664\n",
            "Epoch: 5->Batch: 1600 / 1875. Loss = 0.569462239742279\n",
            "Epoch: 5->Batch: 1620 / 1875. Loss = 0.19714945554733276\n",
            "Epoch: 5->Batch: 1640 / 1875. Loss = 0.36503028869628906\n",
            "Epoch: 5->Batch: 1660 / 1875. Loss = 0.4366346597671509\n",
            "Epoch: 5->Batch: 1680 / 1875. Loss = 0.31117725372314453\n",
            "Epoch: 5->Batch: 1700 / 1875. Loss = 0.5081173181533813\n",
            "Epoch: 5->Batch: 1720 / 1875. Loss = 0.3460097908973694\n",
            "Epoch: 5->Batch: 1740 / 1875. Loss = 0.24360039830207825\n",
            "Epoch: 5->Batch: 1760 / 1875. Loss = 0.27862316370010376\n",
            "Epoch: 5->Batch: 1780 / 1875. Loss = 0.3047912120819092\n",
            "Epoch: 5->Batch: 1800 / 1875. Loss = 0.5709158182144165\n",
            "Epoch: 5->Batch: 1820 / 1875. Loss = 0.3711976706981659\n",
            "Epoch: 5->Batch: 1840 / 1875. Loss = 0.3058675527572632\n",
            "Epoch: 5->Batch: 1860 / 1875. Loss = 0.17111095786094666\n",
            "In Testing Function!\n",
            "Accuracy: 0.8725 (8725 / 10000)\n",
            "Epoch: 6 start ------------\n",
            "\n",
            "Epoch: 6->Batch: 0 / 1875. Loss = 0.24274350702762604\n",
            "Epoch: 6->Batch: 20 / 1875. Loss = 0.37835001945495605\n",
            "Epoch: 6->Batch: 40 / 1875. Loss = 0.37712332606315613\n",
            "Epoch: 6->Batch: 60 / 1875. Loss = 0.23467440903186798\n",
            "Epoch: 6->Batch: 80 / 1875. Loss = 0.2223518341779709\n",
            "Epoch: 6->Batch: 100 / 1875. Loss = 0.1588354855775833\n",
            "Epoch: 6->Batch: 120 / 1875. Loss = 0.33518534898757935\n",
            "Epoch: 6->Batch: 140 / 1875. Loss = 0.3307044506072998\n",
            "Epoch: 6->Batch: 160 / 1875. Loss = 0.3485835790634155\n",
            "Epoch: 6->Batch: 180 / 1875. Loss = 0.44899964332580566\n",
            "Epoch: 6->Batch: 200 / 1875. Loss = 0.40361887216567993\n",
            "Epoch: 6->Batch: 220 / 1875. Loss = 0.37487977743148804\n",
            "Epoch: 6->Batch: 240 / 1875. Loss = 0.39428335428237915\n",
            "Epoch: 6->Batch: 260 / 1875. Loss = 0.27453166246414185\n",
            "Epoch: 6->Batch: 280 / 1875. Loss = 0.2198004275560379\n",
            "Epoch: 6->Batch: 300 / 1875. Loss = 0.37426868081092834\n",
            "Epoch: 6->Batch: 320 / 1875. Loss = 0.34778568148612976\n",
            "Epoch: 6->Batch: 340 / 1875. Loss = 0.4004594087600708\n",
            "Epoch: 6->Batch: 360 / 1875. Loss = 0.2948225736618042\n",
            "Epoch: 6->Batch: 380 / 1875. Loss = 0.16697640717029572\n",
            "Epoch: 6->Batch: 400 / 1875. Loss = 0.32754430174827576\n",
            "Epoch: 6->Batch: 420 / 1875. Loss = 0.2223820686340332\n",
            "Epoch: 6->Batch: 440 / 1875. Loss = 0.2732698917388916\n",
            "Epoch: 6->Batch: 460 / 1875. Loss = 0.2129603624343872\n",
            "Epoch: 6->Batch: 480 / 1875. Loss = 0.22657126188278198\n",
            "Epoch: 6->Batch: 500 / 1875. Loss = 0.2350326031446457\n",
            "Epoch: 6->Batch: 520 / 1875. Loss = 0.29515695571899414\n",
            "Epoch: 6->Batch: 540 / 1875. Loss = 0.3188142478466034\n",
            "Epoch: 6->Batch: 560 / 1875. Loss = 0.2546118497848511\n",
            "Epoch: 6->Batch: 580 / 1875. Loss = 0.6629456281661987\n",
            "Epoch: 6->Batch: 600 / 1875. Loss = 0.41846686601638794\n",
            "Epoch: 6->Batch: 620 / 1875. Loss = 0.14074373245239258\n",
            "Epoch: 6->Batch: 640 / 1875. Loss = 0.2619847357273102\n",
            "Epoch: 6->Batch: 660 / 1875. Loss = 0.6665959358215332\n",
            "Epoch: 6->Batch: 680 / 1875. Loss = 0.3000296950340271\n",
            "Epoch: 6->Batch: 700 / 1875. Loss = 0.3784778118133545\n",
            "Epoch: 6->Batch: 720 / 1875. Loss = 0.3438337445259094\n",
            "Epoch: 6->Batch: 740 / 1875. Loss = 0.4625321626663208\n",
            "Epoch: 6->Batch: 760 / 1875. Loss = 0.11426663398742676\n",
            "Epoch: 6->Batch: 780 / 1875. Loss = 0.35627779364585876\n",
            "Epoch: 6->Batch: 800 / 1875. Loss = 0.3114588260650635\n",
            "Epoch: 6->Batch: 820 / 1875. Loss = 0.17967216670513153\n",
            "Epoch: 6->Batch: 840 / 1875. Loss = 0.42222338914871216\n",
            "Epoch: 6->Batch: 860 / 1875. Loss = 0.39393770694732666\n",
            "Epoch: 6->Batch: 880 / 1875. Loss = 0.22119338810443878\n",
            "Epoch: 6->Batch: 900 / 1875. Loss = 0.17036089301109314\n",
            "Epoch: 6->Batch: 920 / 1875. Loss = 0.20022857189178467\n",
            "Epoch: 6->Batch: 940 / 1875. Loss = 0.22804635763168335\n",
            "Epoch: 6->Batch: 960 / 1875. Loss = 0.47349029779434204\n",
            "Epoch: 6->Batch: 980 / 1875. Loss = 0.15747874975204468\n",
            "Epoch: 6->Batch: 1000 / 1875. Loss = 0.2481018751859665\n",
            "Epoch: 6->Batch: 1020 / 1875. Loss = 0.5589185357093811\n",
            "Epoch: 6->Batch: 1040 / 1875. Loss = 0.30235588550567627\n",
            "Epoch: 6->Batch: 1060 / 1875. Loss = 0.49729102849960327\n",
            "Epoch: 6->Batch: 1080 / 1875. Loss = 0.46433156728744507\n",
            "Epoch: 6->Batch: 1100 / 1875. Loss = 0.3468323349952698\n",
            "Epoch: 6->Batch: 1120 / 1875. Loss = 0.27969053387641907\n",
            "Epoch: 6->Batch: 1140 / 1875. Loss = 0.13622921705245972\n",
            "Epoch: 6->Batch: 1160 / 1875. Loss = 0.14351505041122437\n",
            "Epoch: 6->Batch: 1180 / 1875. Loss = 0.34759587049484253\n",
            "Epoch: 6->Batch: 1200 / 1875. Loss = 0.2278105914592743\n",
            "Epoch: 6->Batch: 1220 / 1875. Loss = 0.1881682425737381\n",
            "Epoch: 6->Batch: 1240 / 1875. Loss = 0.17929469048976898\n",
            "Epoch: 6->Batch: 1260 / 1875. Loss = 0.3988395035266876\n",
            "Epoch: 6->Batch: 1280 / 1875. Loss = 0.5356124043464661\n",
            "Epoch: 6->Batch: 1300 / 1875. Loss = 0.4949621260166168\n",
            "Epoch: 6->Batch: 1320 / 1875. Loss = 0.1984069049358368\n",
            "Epoch: 6->Batch: 1340 / 1875. Loss = 0.23746632039546967\n",
            "Epoch: 6->Batch: 1360 / 1875. Loss = 0.3054657280445099\n",
            "Epoch: 6->Batch: 1380 / 1875. Loss = 0.417955219745636\n",
            "Epoch: 6->Batch: 1400 / 1875. Loss = 0.37918803095817566\n",
            "Epoch: 6->Batch: 1420 / 1875. Loss = 0.6324390769004822\n",
            "Epoch: 6->Batch: 1440 / 1875. Loss = 0.23288579285144806\n",
            "Epoch: 6->Batch: 1460 / 1875. Loss = 0.38958606123924255\n",
            "Epoch: 6->Batch: 1480 / 1875. Loss = 0.11145827174186707\n",
            "Epoch: 6->Batch: 1500 / 1875. Loss = 0.2381853461265564\n",
            "Epoch: 6->Batch: 1520 / 1875. Loss = 0.2431180626153946\n",
            "Epoch: 6->Batch: 1540 / 1875. Loss = 0.4189711809158325\n",
            "Epoch: 6->Batch: 1560 / 1875. Loss = 0.222703218460083\n",
            "Epoch: 6->Batch: 1580 / 1875. Loss = 0.2834419310092926\n",
            "Epoch: 6->Batch: 1600 / 1875. Loss = 0.33164912462234497\n",
            "Epoch: 6->Batch: 1620 / 1875. Loss = 0.2813413143157959\n",
            "Epoch: 6->Batch: 1640 / 1875. Loss = 0.4358152747154236\n",
            "Epoch: 6->Batch: 1660 / 1875. Loss = 0.4294070601463318\n",
            "Epoch: 6->Batch: 1680 / 1875. Loss = 0.2545778453350067\n",
            "Epoch: 6->Batch: 1700 / 1875. Loss = 0.4458453059196472\n",
            "Epoch: 6->Batch: 1720 / 1875. Loss = 0.30301231145858765\n",
            "Epoch: 6->Batch: 1740 / 1875. Loss = 0.14388558268547058\n",
            "Epoch: 6->Batch: 1760 / 1875. Loss = 0.2414100468158722\n",
            "Epoch: 6->Batch: 1780 / 1875. Loss = 0.34151771664619446\n",
            "Epoch: 6->Batch: 1800 / 1875. Loss = 0.4527718424797058\n",
            "Epoch: 6->Batch: 1820 / 1875. Loss = 0.29752522706985474\n",
            "Epoch: 6->Batch: 1840 / 1875. Loss = 0.27893736958503723\n",
            "Epoch: 6->Batch: 1860 / 1875. Loss = 0.6479688286781311\n",
            "In Testing Function!\n",
            "Accuracy: 0.8792 (8792 / 10000)\n",
            "Epoch: 7 start ------------\n",
            "\n",
            "Epoch: 7->Batch: 0 / 1875. Loss = 0.44915029406547546\n",
            "Epoch: 7->Batch: 20 / 1875. Loss = 0.35565146803855896\n",
            "Epoch: 7->Batch: 40 / 1875. Loss = 0.4991328716278076\n",
            "Epoch: 7->Batch: 60 / 1875. Loss = 0.3571499288082123\n",
            "Epoch: 7->Batch: 80 / 1875. Loss = 0.41809022426605225\n",
            "Epoch: 7->Batch: 100 / 1875. Loss = 0.11151471734046936\n",
            "Epoch: 7->Batch: 120 / 1875. Loss = 0.25017404556274414\n",
            "Epoch: 7->Batch: 140 / 1875. Loss = 0.23330149054527283\n",
            "Epoch: 7->Batch: 160 / 1875. Loss = 0.2905711829662323\n",
            "Epoch: 7->Batch: 180 / 1875. Loss = 0.36111289262771606\n",
            "Epoch: 7->Batch: 200 / 1875. Loss = 0.4710581600666046\n",
            "Epoch: 7->Batch: 220 / 1875. Loss = 0.3158652186393738\n",
            "Epoch: 7->Batch: 240 / 1875. Loss = 0.6690977215766907\n",
            "Epoch: 7->Batch: 260 / 1875. Loss = 0.280281126499176\n",
            "Epoch: 7->Batch: 280 / 1875. Loss = 0.39517855644226074\n",
            "Epoch: 7->Batch: 300 / 1875. Loss = 0.5227051973342896\n",
            "Epoch: 7->Batch: 320 / 1875. Loss = 0.3152279257774353\n",
            "Epoch: 7->Batch: 340 / 1875. Loss = 0.17119812965393066\n",
            "Epoch: 7->Batch: 360 / 1875. Loss = 0.31099116802215576\n",
            "Epoch: 7->Batch: 380 / 1875. Loss = 0.3633798360824585\n",
            "Epoch: 7->Batch: 400 / 1875. Loss = 0.238226056098938\n",
            "Epoch: 7->Batch: 420 / 1875. Loss = 0.2035800665616989\n",
            "Epoch: 7->Batch: 440 / 1875. Loss = 0.38149940967559814\n",
            "Epoch: 7->Batch: 460 / 1875. Loss = 0.19632384181022644\n",
            "Epoch: 7->Batch: 480 / 1875. Loss = 0.2965364456176758\n",
            "Epoch: 7->Batch: 500 / 1875. Loss = 0.18467214703559875\n",
            "Epoch: 7->Batch: 520 / 1875. Loss = 0.2996099293231964\n",
            "Epoch: 7->Batch: 540 / 1875. Loss = 0.4198005497455597\n",
            "Epoch: 7->Batch: 560 / 1875. Loss = 0.24418120086193085\n",
            "Epoch: 7->Batch: 580 / 1875. Loss = 0.47560983896255493\n",
            "Epoch: 7->Batch: 600 / 1875. Loss = 0.370834618806839\n",
            "Epoch: 7->Batch: 620 / 1875. Loss = 0.42110979557037354\n",
            "Epoch: 7->Batch: 640 / 1875. Loss = 0.2482203245162964\n",
            "Epoch: 7->Batch: 660 / 1875. Loss = 0.23143739998340607\n",
            "Epoch: 7->Batch: 680 / 1875. Loss = 0.3047088384628296\n",
            "Epoch: 7->Batch: 700 / 1875. Loss = 0.4087405204772949\n",
            "Epoch: 7->Batch: 720 / 1875. Loss = 0.2794344425201416\n",
            "Epoch: 7->Batch: 740 / 1875. Loss = 0.35267776250839233\n",
            "Epoch: 7->Batch: 760 / 1875. Loss = 0.36808910965919495\n",
            "Epoch: 7->Batch: 780 / 1875. Loss = 0.23777396976947784\n",
            "Epoch: 7->Batch: 800 / 1875. Loss = 0.42373591661453247\n",
            "Epoch: 7->Batch: 820 / 1875. Loss = 0.34623396396636963\n",
            "Epoch: 7->Batch: 840 / 1875. Loss = 0.5035421848297119\n",
            "Epoch: 7->Batch: 860 / 1875. Loss = 0.5586012005805969\n",
            "Epoch: 7->Batch: 880 / 1875. Loss = 0.3140987753868103\n",
            "Epoch: 7->Batch: 900 / 1875. Loss = 0.2821735143661499\n",
            "Epoch: 7->Batch: 920 / 1875. Loss = 0.2731458842754364\n",
            "Epoch: 7->Batch: 940 / 1875. Loss = 0.288984477519989\n",
            "Epoch: 7->Batch: 960 / 1875. Loss = 0.385816365480423\n",
            "Epoch: 7->Batch: 980 / 1875. Loss = 0.5103845000267029\n",
            "Epoch: 7->Batch: 1000 / 1875. Loss = 0.4159681797027588\n",
            "Epoch: 7->Batch: 1020 / 1875. Loss = 0.1924673169851303\n",
            "Epoch: 7->Batch: 1040 / 1875. Loss = 0.335114985704422\n",
            "Epoch: 7->Batch: 1060 / 1875. Loss = 0.30477380752563477\n",
            "Epoch: 7->Batch: 1080 / 1875. Loss = 0.4831496477127075\n",
            "Epoch: 7->Batch: 1100 / 1875. Loss = 0.5641492605209351\n",
            "Epoch: 7->Batch: 1120 / 1875. Loss = 0.22118248045444489\n",
            "Epoch: 7->Batch: 1140 / 1875. Loss = 0.3184942603111267\n",
            "Epoch: 7->Batch: 1160 / 1875. Loss = 0.10236583650112152\n",
            "Epoch: 7->Batch: 1180 / 1875. Loss = 0.3468440771102905\n",
            "Epoch: 7->Batch: 1200 / 1875. Loss = 0.265798419713974\n",
            "Epoch: 7->Batch: 1220 / 1875. Loss = 0.16298235952854156\n",
            "Epoch: 7->Batch: 1240 / 1875. Loss = 0.19425764679908752\n",
            "Epoch: 7->Batch: 1260 / 1875. Loss = 0.4748670756816864\n",
            "Epoch: 7->Batch: 1280 / 1875. Loss = 0.5014065504074097\n",
            "Epoch: 7->Batch: 1300 / 1875. Loss = 0.41700851917266846\n",
            "Epoch: 7->Batch: 1320 / 1875. Loss = 0.3365490734577179\n",
            "Epoch: 7->Batch: 1340 / 1875. Loss = 0.29439038038253784\n",
            "Epoch: 7->Batch: 1360 / 1875. Loss = 0.30826449394226074\n",
            "Epoch: 7->Batch: 1380 / 1875. Loss = 0.23611636459827423\n",
            "Epoch: 7->Batch: 1400 / 1875. Loss = 0.22771313786506653\n",
            "Epoch: 7->Batch: 1420 / 1875. Loss = 0.180308535695076\n",
            "Epoch: 7->Batch: 1440 / 1875. Loss = 0.30990809202194214\n",
            "Epoch: 7->Batch: 1460 / 1875. Loss = 0.171748548746109\n",
            "Epoch: 7->Batch: 1480 / 1875. Loss = 0.1423824429512024\n",
            "Epoch: 7->Batch: 1500 / 1875. Loss = 0.24696020781993866\n",
            "Epoch: 7->Batch: 1520 / 1875. Loss = 0.21110548079013824\n",
            "Epoch: 7->Batch: 1540 / 1875. Loss = 0.2649853825569153\n",
            "Epoch: 7->Batch: 1560 / 1875. Loss = 0.33334338665008545\n",
            "Epoch: 7->Batch: 1580 / 1875. Loss = 0.42761164903640747\n",
            "Epoch: 7->Batch: 1600 / 1875. Loss = 0.25660839676856995\n",
            "Epoch: 7->Batch: 1620 / 1875. Loss = 0.20306555926799774\n",
            "Epoch: 7->Batch: 1640 / 1875. Loss = 0.2721347510814667\n",
            "Epoch: 7->Batch: 1660 / 1875. Loss = 0.2253420352935791\n",
            "Epoch: 7->Batch: 1680 / 1875. Loss = 0.09882362931966782\n",
            "Epoch: 7->Batch: 1700 / 1875. Loss = 0.17323757708072662\n",
            "Epoch: 7->Batch: 1720 / 1875. Loss = 0.23676246404647827\n",
            "Epoch: 7->Batch: 1740 / 1875. Loss = 0.25882962346076965\n",
            "Epoch: 7->Batch: 1760 / 1875. Loss = 0.2418062686920166\n",
            "Epoch: 7->Batch: 1780 / 1875. Loss = 0.19389019906520844\n",
            "Epoch: 7->Batch: 1800 / 1875. Loss = 0.3305291533470154\n",
            "Epoch: 7->Batch: 1820 / 1875. Loss = 0.2195245325565338\n",
            "Epoch: 7->Batch: 1840 / 1875. Loss = 0.4398864507675171\n",
            "Epoch: 7->Batch: 1860 / 1875. Loss = 0.3553548753261566\n",
            "In Testing Function!\n",
            "Accuracy: 0.8804 (8804 / 10000)\n",
            "Epoch: 8 start ------------\n",
            "\n",
            "Epoch: 8->Batch: 0 / 1875. Loss = 0.44295185804367065\n",
            "Epoch: 8->Batch: 20 / 1875. Loss = 0.3622239828109741\n",
            "Epoch: 8->Batch: 40 / 1875. Loss = 0.3943706750869751\n",
            "Epoch: 8->Batch: 60 / 1875. Loss = 0.41530677676200867\n",
            "Epoch: 8->Batch: 80 / 1875. Loss = 0.4157281816005707\n",
            "Epoch: 8->Batch: 100 / 1875. Loss = 0.2293633222579956\n",
            "Epoch: 8->Batch: 120 / 1875. Loss = 0.4784766137599945\n",
            "Epoch: 8->Batch: 140 / 1875. Loss = 0.32400739192962646\n",
            "Epoch: 8->Batch: 160 / 1875. Loss = 0.20226123929023743\n",
            "Epoch: 8->Batch: 180 / 1875. Loss = 0.2044089436531067\n",
            "Epoch: 8->Batch: 200 / 1875. Loss = 0.3221777379512787\n",
            "Epoch: 8->Batch: 220 / 1875. Loss = 0.09929420053958893\n",
            "Epoch: 8->Batch: 240 / 1875. Loss = 0.21365375816822052\n",
            "Epoch: 8->Batch: 260 / 1875. Loss = 0.244444340467453\n",
            "Epoch: 8->Batch: 280 / 1875. Loss = 0.33461934328079224\n",
            "Epoch: 8->Batch: 300 / 1875. Loss = 0.5898573398590088\n",
            "Epoch: 8->Batch: 320 / 1875. Loss = 0.4278736412525177\n",
            "Epoch: 8->Batch: 340 / 1875. Loss = 0.2537257671356201\n",
            "Epoch: 8->Batch: 360 / 1875. Loss = 0.27670419216156006\n",
            "Epoch: 8->Batch: 380 / 1875. Loss = 0.21175119280815125\n",
            "Epoch: 8->Batch: 400 / 1875. Loss = 0.38482293486595154\n",
            "Epoch: 8->Batch: 420 / 1875. Loss = 0.17367206513881683\n",
            "Epoch: 8->Batch: 440 / 1875. Loss = 0.5325859785079956\n",
            "Epoch: 8->Batch: 460 / 1875. Loss = 0.3443572521209717\n",
            "Epoch: 8->Batch: 480 / 1875. Loss = 0.3193211257457733\n",
            "Epoch: 8->Batch: 500 / 1875. Loss = 0.1412743777036667\n",
            "Epoch: 8->Batch: 520 / 1875. Loss = 0.28155258297920227\n",
            "Epoch: 8->Batch: 540 / 1875. Loss = 0.28884902596473694\n",
            "Epoch: 8->Batch: 560 / 1875. Loss = 0.3110053539276123\n",
            "Epoch: 8->Batch: 580 / 1875. Loss = 0.0970255509018898\n",
            "Epoch: 8->Batch: 600 / 1875. Loss = 0.41145479679107666\n",
            "Epoch: 8->Batch: 620 / 1875. Loss = 0.30082017183303833\n",
            "Epoch: 8->Batch: 640 / 1875. Loss = 0.1118403822183609\n",
            "Epoch: 8->Batch: 660 / 1875. Loss = 0.26213791966438293\n",
            "Epoch: 8->Batch: 680 / 1875. Loss = 0.5614128708839417\n",
            "Epoch: 8->Batch: 700 / 1875. Loss = 0.2296285182237625\n",
            "Epoch: 8->Batch: 720 / 1875. Loss = 0.2274855375289917\n",
            "Epoch: 8->Batch: 740 / 1875. Loss = 0.2741610109806061\n",
            "Epoch: 8->Batch: 760 / 1875. Loss = 0.33976417779922485\n",
            "Epoch: 8->Batch: 780 / 1875. Loss = 0.13790321350097656\n",
            "Epoch: 8->Batch: 800 / 1875. Loss = 0.2478390634059906\n",
            "Epoch: 8->Batch: 820 / 1875. Loss = 0.2741800546646118\n",
            "Epoch: 8->Batch: 840 / 1875. Loss = 0.41628503799438477\n",
            "Epoch: 8->Batch: 860 / 1875. Loss = 0.3480752110481262\n",
            "Epoch: 8->Batch: 880 / 1875. Loss = 0.3060448467731476\n",
            "Epoch: 8->Batch: 900 / 1875. Loss = 0.2909071147441864\n",
            "Epoch: 8->Batch: 920 / 1875. Loss = 0.33918750286102295\n",
            "Epoch: 8->Batch: 940 / 1875. Loss = 0.25810885429382324\n",
            "Epoch: 8->Batch: 960 / 1875. Loss = 0.32616037130355835\n",
            "Epoch: 8->Batch: 980 / 1875. Loss = 0.2227078527212143\n",
            "Epoch: 8->Batch: 1000 / 1875. Loss = 0.22757083177566528\n",
            "Epoch: 8->Batch: 1020 / 1875. Loss = 0.14918839931488037\n",
            "Epoch: 8->Batch: 1040 / 1875. Loss = 0.21764278411865234\n",
            "Epoch: 8->Batch: 1060 / 1875. Loss = 0.14915373921394348\n",
            "Epoch: 8->Batch: 1080 / 1875. Loss = 0.2590559720993042\n",
            "Epoch: 8->Batch: 1100 / 1875. Loss = 0.411131888628006\n",
            "Epoch: 8->Batch: 1120 / 1875. Loss = 0.3661841154098511\n",
            "Epoch: 8->Batch: 1140 / 1875. Loss = 0.07396265864372253\n",
            "Epoch: 8->Batch: 1160 / 1875. Loss = 0.32758843898773193\n",
            "Epoch: 8->Batch: 1180 / 1875. Loss = 0.22052910923957825\n",
            "Epoch: 8->Batch: 1200 / 1875. Loss = 0.09979929029941559\n",
            "Epoch: 8->Batch: 1220 / 1875. Loss = 0.2329205870628357\n",
            "Epoch: 8->Batch: 1240 / 1875. Loss = 0.371128112077713\n",
            "Epoch: 8->Batch: 1260 / 1875. Loss = 0.13892440497875214\n",
            "Epoch: 8->Batch: 1280 / 1875. Loss = 0.28938645124435425\n",
            "Epoch: 8->Batch: 1300 / 1875. Loss = 0.18947863578796387\n",
            "Epoch: 8->Batch: 1320 / 1875. Loss = 0.5462862253189087\n",
            "Epoch: 8->Batch: 1340 / 1875. Loss = 0.23923204839229584\n",
            "Epoch: 8->Batch: 1360 / 1875. Loss = 0.2811933159828186\n",
            "Epoch: 8->Batch: 1380 / 1875. Loss = 0.32445457577705383\n",
            "Epoch: 8->Batch: 1400 / 1875. Loss = 0.23543937504291534\n",
            "Epoch: 8->Batch: 1420 / 1875. Loss = 0.2776539921760559\n",
            "Epoch: 8->Batch: 1440 / 1875. Loss = 0.5358082056045532\n",
            "Epoch: 8->Batch: 1460 / 1875. Loss = 0.25140824913978577\n",
            "Epoch: 8->Batch: 1480 / 1875. Loss = 0.17355038225650787\n",
            "Epoch: 8->Batch: 1500 / 1875. Loss = 0.4869903326034546\n",
            "Epoch: 8->Batch: 1520 / 1875. Loss = 0.3294285535812378\n",
            "Epoch: 8->Batch: 1540 / 1875. Loss = 0.6156719923019409\n",
            "Epoch: 8->Batch: 1560 / 1875. Loss = 0.3330007791519165\n",
            "Epoch: 8->Batch: 1580 / 1875. Loss = 0.41112780570983887\n",
            "Epoch: 8->Batch: 1600 / 1875. Loss = 0.13419203460216522\n",
            "Epoch: 8->Batch: 1620 / 1875. Loss = 0.20479460060596466\n",
            "Epoch: 8->Batch: 1640 / 1875. Loss = 0.340684711933136\n",
            "Epoch: 8->Batch: 1660 / 1875. Loss = 0.6428633332252502\n",
            "Epoch: 8->Batch: 1680 / 1875. Loss = 0.2094763070344925\n",
            "Epoch: 8->Batch: 1700 / 1875. Loss = 0.49823179841041565\n",
            "Epoch: 8->Batch: 1720 / 1875. Loss = 0.3847903609275818\n",
            "Epoch: 8->Batch: 1740 / 1875. Loss = 0.3543616831302643\n",
            "Epoch: 8->Batch: 1760 / 1875. Loss = 0.45082512497901917\n",
            "Epoch: 8->Batch: 1780 / 1875. Loss = 0.3446897566318512\n",
            "Epoch: 8->Batch: 1800 / 1875. Loss = 0.1491023451089859\n",
            "Epoch: 8->Batch: 1820 / 1875. Loss = 0.2075817883014679\n",
            "Epoch: 8->Batch: 1840 / 1875. Loss = 0.2990349233150482\n",
            "Epoch: 8->Batch: 1860 / 1875. Loss = 0.1367572396993637\n",
            "In Testing Function!\n",
            "Accuracy: 0.8863 (8863 / 10000)\n",
            "Epoch: 9 start ------------\n",
            "\n",
            "Epoch: 9->Batch: 0 / 1875. Loss = 0.23433063924312592\n",
            "Epoch: 9->Batch: 20 / 1875. Loss = 0.28282850980758667\n",
            "Epoch: 9->Batch: 40 / 1875. Loss = 0.12047310918569565\n",
            "Epoch: 9->Batch: 60 / 1875. Loss = 0.059123650193214417\n",
            "Epoch: 9->Batch: 80 / 1875. Loss = 0.27520066499710083\n",
            "Epoch: 9->Batch: 100 / 1875. Loss = 0.22377662360668182\n",
            "Epoch: 9->Batch: 120 / 1875. Loss = 0.140348881483078\n",
            "Epoch: 9->Batch: 140 / 1875. Loss = 0.22665369510650635\n",
            "Epoch: 9->Batch: 160 / 1875. Loss = 0.48161429166793823\n",
            "Epoch: 9->Batch: 180 / 1875. Loss = 0.47778984904289246\n",
            "Epoch: 9->Batch: 200 / 1875. Loss = 0.3999848961830139\n",
            "Epoch: 9->Batch: 220 / 1875. Loss = 0.20339828729629517\n",
            "Epoch: 9->Batch: 240 / 1875. Loss = 0.1753779649734497\n",
            "Epoch: 9->Batch: 260 / 1875. Loss = 0.21621273458003998\n",
            "Epoch: 9->Batch: 280 / 1875. Loss = 0.3576739430427551\n",
            "Epoch: 9->Batch: 300 / 1875. Loss = 0.2058039903640747\n",
            "Epoch: 9->Batch: 320 / 1875. Loss = 0.11599782854318619\n",
            "Epoch: 9->Batch: 340 / 1875. Loss = 0.2696109414100647\n",
            "Epoch: 9->Batch: 360 / 1875. Loss = 0.19456550478935242\n",
            "Epoch: 9->Batch: 380 / 1875. Loss = 0.16029654443264008\n",
            "Epoch: 9->Batch: 400 / 1875. Loss = 0.4899132549762726\n",
            "Epoch: 9->Batch: 420 / 1875. Loss = 0.22333849966526031\n",
            "Epoch: 9->Batch: 440 / 1875. Loss = 0.2632414698600769\n",
            "Epoch: 9->Batch: 460 / 1875. Loss = 0.2649487853050232\n",
            "Epoch: 9->Batch: 480 / 1875. Loss = 0.40156883001327515\n",
            "Epoch: 9->Batch: 500 / 1875. Loss = 0.2990221381187439\n",
            "Epoch: 9->Batch: 520 / 1875. Loss = 0.1816820502281189\n",
            "Epoch: 9->Batch: 540 / 1875. Loss = 0.17140690982341766\n",
            "Epoch: 9->Batch: 560 / 1875. Loss = 0.26560497283935547\n",
            "Epoch: 9->Batch: 580 / 1875. Loss = 0.4026974141597748\n",
            "Epoch: 9->Batch: 600 / 1875. Loss = 0.0817909687757492\n",
            "Epoch: 9->Batch: 620 / 1875. Loss = 0.3606520891189575\n",
            "Epoch: 9->Batch: 640 / 1875. Loss = 0.25433775782585144\n",
            "Epoch: 9->Batch: 660 / 1875. Loss = 0.22913901507854462\n",
            "Epoch: 9->Batch: 680 / 1875. Loss = 0.29068949818611145\n",
            "Epoch: 9->Batch: 700 / 1875. Loss = 0.27118825912475586\n",
            "Epoch: 9->Batch: 720 / 1875. Loss = 0.31200647354125977\n",
            "Epoch: 9->Batch: 740 / 1875. Loss = 0.33509525656700134\n",
            "Epoch: 9->Batch: 760 / 1875. Loss = 0.30232125520706177\n",
            "Epoch: 9->Batch: 780 / 1875. Loss = 0.2767027020454407\n",
            "Epoch: 9->Batch: 800 / 1875. Loss = 0.4716516137123108\n",
            "Epoch: 9->Batch: 820 / 1875. Loss = 0.15765048563480377\n",
            "Epoch: 9->Batch: 840 / 1875. Loss = 0.22964373230934143\n",
            "Epoch: 9->Batch: 860 / 1875. Loss = 0.26918917894363403\n",
            "Epoch: 9->Batch: 880 / 1875. Loss = 0.24478478729724884\n",
            "Epoch: 9->Batch: 900 / 1875. Loss = 0.37005671858787537\n",
            "Epoch: 9->Batch: 920 / 1875. Loss = 0.20148314535617828\n",
            "Epoch: 9->Batch: 940 / 1875. Loss = 0.29390591382980347\n",
            "Epoch: 9->Batch: 960 / 1875. Loss = 0.3518916666507721\n",
            "Epoch: 9->Batch: 980 / 1875. Loss = 0.17172378301620483\n",
            "Epoch: 9->Batch: 1000 / 1875. Loss = 0.22620493173599243\n",
            "Epoch: 9->Batch: 1020 / 1875. Loss = 0.328999400138855\n",
            "Epoch: 9->Batch: 1040 / 1875. Loss = 0.4025461971759796\n",
            "Epoch: 9->Batch: 1060 / 1875. Loss = 0.36383724212646484\n",
            "Epoch: 9->Batch: 1080 / 1875. Loss = 0.2646608352661133\n",
            "Epoch: 9->Batch: 1100 / 1875. Loss = 0.16920338571071625\n",
            "Epoch: 9->Batch: 1120 / 1875. Loss = 0.43025094270706177\n",
            "Epoch: 9->Batch: 1140 / 1875. Loss = 0.12343423068523407\n",
            "Epoch: 9->Batch: 1160 / 1875. Loss = 0.1861596405506134\n",
            "Epoch: 9->Batch: 1180 / 1875. Loss = 0.2531145215034485\n",
            "Epoch: 9->Batch: 1200 / 1875. Loss = 0.3384488821029663\n",
            "Epoch: 9->Batch: 1220 / 1875. Loss = 0.285603791475296\n",
            "Epoch: 9->Batch: 1240 / 1875. Loss = 0.3694958984851837\n",
            "Epoch: 9->Batch: 1260 / 1875. Loss = 0.26570117473602295\n",
            "Epoch: 9->Batch: 1280 / 1875. Loss = 0.5224471092224121\n",
            "Epoch: 9->Batch: 1300 / 1875. Loss = 0.4719758629798889\n",
            "Epoch: 9->Batch: 1320 / 1875. Loss = 0.2150556743144989\n",
            "Epoch: 9->Batch: 1340 / 1875. Loss = 0.2125573754310608\n",
            "Epoch: 9->Batch: 1360 / 1875. Loss = 0.339934766292572\n",
            "Epoch: 9->Batch: 1380 / 1875. Loss = 0.2482214719057083\n",
            "Epoch: 9->Batch: 1400 / 1875. Loss = 0.3453725576400757\n",
            "Epoch: 9->Batch: 1420 / 1875. Loss = 0.42862415313720703\n",
            "Epoch: 9->Batch: 1440 / 1875. Loss = 0.09156092256307602\n",
            "Epoch: 9->Batch: 1460 / 1875. Loss = 0.16208390891551971\n",
            "Epoch: 9->Batch: 1480 / 1875. Loss = 0.2355019599199295\n",
            "Epoch: 9->Batch: 1500 / 1875. Loss = 0.42460018396377563\n",
            "Epoch: 9->Batch: 1520 / 1875. Loss = 0.26446986198425293\n",
            "Epoch: 9->Batch: 1540 / 1875. Loss = 0.4928024411201477\n",
            "Epoch: 9->Batch: 1560 / 1875. Loss = 0.2261597365140915\n",
            "Epoch: 9->Batch: 1580 / 1875. Loss = 0.3110882043838501\n",
            "Epoch: 9->Batch: 1600 / 1875. Loss = 0.4880583882331848\n",
            "Epoch: 9->Batch: 1620 / 1875. Loss = 0.11716730147600174\n",
            "Epoch: 9->Batch: 1640 / 1875. Loss = 0.3460930585861206\n",
            "Epoch: 9->Batch: 1660 / 1875. Loss = 0.47218164801597595\n",
            "Epoch: 9->Batch: 1680 / 1875. Loss = 0.2966083288192749\n",
            "Epoch: 9->Batch: 1700 / 1875. Loss = 0.20131854712963104\n",
            "Epoch: 9->Batch: 1720 / 1875. Loss = 0.3674526512622833\n",
            "Epoch: 9->Batch: 1740 / 1875. Loss = 0.1436844766139984\n",
            "Epoch: 9->Batch: 1760 / 1875. Loss = 0.2984442710876465\n",
            "Epoch: 9->Batch: 1780 / 1875. Loss = 0.2892692983150482\n",
            "Epoch: 9->Batch: 1800 / 1875. Loss = 0.13315808773040771\n",
            "Epoch: 9->Batch: 1820 / 1875. Loss = 0.22536200284957886\n",
            "Epoch: 9->Batch: 1840 / 1875. Loss = 0.1941465139389038\n",
            "Epoch: 9->Batch: 1860 / 1875. Loss = 0.24437780678272247\n",
            "In Testing Function!\n",
            "Accuracy: 0.8906 (8906 / 10000)\n",
            "Epoch: 10 start ------------\n",
            "\n",
            "Epoch: 10->Batch: 0 / 1875. Loss = 0.11273905634880066\n",
            "Epoch: 10->Batch: 20 / 1875. Loss = 0.15978044271469116\n",
            "Epoch: 10->Batch: 40 / 1875. Loss = 0.16272227466106415\n",
            "Epoch: 10->Batch: 60 / 1875. Loss = 0.26323145627975464\n",
            "Epoch: 10->Batch: 80 / 1875. Loss = 0.16458742320537567\n",
            "Epoch: 10->Batch: 100 / 1875. Loss = 0.30601951479911804\n",
            "Epoch: 10->Batch: 120 / 1875. Loss = 0.40152478218078613\n",
            "Epoch: 10->Batch: 140 / 1875. Loss = 0.29202187061309814\n",
            "Epoch: 10->Batch: 160 / 1875. Loss = 0.08717405796051025\n",
            "Epoch: 10->Batch: 180 / 1875. Loss = 0.29617840051651\n",
            "Epoch: 10->Batch: 200 / 1875. Loss = 0.32259997725486755\n",
            "Epoch: 10->Batch: 220 / 1875. Loss = 0.20780816674232483\n",
            "Epoch: 10->Batch: 240 / 1875. Loss = 0.4601263701915741\n",
            "Epoch: 10->Batch: 260 / 1875. Loss = 0.18445149064064026\n",
            "Epoch: 10->Batch: 280 / 1875. Loss = 0.06287212669849396\n",
            "Epoch: 10->Batch: 300 / 1875. Loss = 0.23889411985874176\n",
            "Epoch: 10->Batch: 320 / 1875. Loss = 0.06936946511268616\n",
            "Epoch: 10->Batch: 340 / 1875. Loss = 0.29979509115219116\n",
            "Epoch: 10->Batch: 360 / 1875. Loss = 0.2611435055732727\n",
            "Epoch: 10->Batch: 380 / 1875. Loss = 0.3074662685394287\n",
            "Epoch: 10->Batch: 400 / 1875. Loss = 0.490488201379776\n",
            "Epoch: 10->Batch: 420 / 1875. Loss = 0.21059498190879822\n",
            "Epoch: 10->Batch: 440 / 1875. Loss = 0.15451471507549286\n",
            "Epoch: 10->Batch: 460 / 1875. Loss = 0.37233766913414\n",
            "Epoch: 10->Batch: 480 / 1875. Loss = 0.16400828957557678\n",
            "Epoch: 10->Batch: 500 / 1875. Loss = 0.39426475763320923\n",
            "Epoch: 10->Batch: 520 / 1875. Loss = 0.3218345046043396\n",
            "Epoch: 10->Batch: 540 / 1875. Loss = 0.24213489890098572\n",
            "Epoch: 10->Batch: 560 / 1875. Loss = 0.12790381908416748\n",
            "Epoch: 10->Batch: 580 / 1875. Loss = 0.27537861466407776\n",
            "Epoch: 10->Batch: 600 / 1875. Loss = 0.35786664485931396\n",
            "Epoch: 10->Batch: 620 / 1875. Loss = 0.08500947803258896\n",
            "Epoch: 10->Batch: 640 / 1875. Loss = 0.08504703640937805\n",
            "Epoch: 10->Batch: 660 / 1875. Loss = 0.036275699734687805\n",
            "Epoch: 10->Batch: 680 / 1875. Loss = 0.24732671678066254\n",
            "Epoch: 10->Batch: 700 / 1875. Loss = 0.21389827132225037\n",
            "Epoch: 10->Batch: 720 / 1875. Loss = 0.09376407414674759\n",
            "Epoch: 10->Batch: 740 / 1875. Loss = 0.285162091255188\n",
            "Epoch: 10->Batch: 760 / 1875. Loss = 0.3155135214328766\n",
            "Epoch: 10->Batch: 780 / 1875. Loss = 0.26175063848495483\n",
            "Epoch: 10->Batch: 800 / 1875. Loss = 0.29945871233940125\n",
            "Epoch: 10->Batch: 820 / 1875. Loss = 0.15424273908138275\n",
            "Epoch: 10->Batch: 840 / 1875. Loss = 0.13918684422969818\n",
            "Epoch: 10->Batch: 860 / 1875. Loss = 0.24461999535560608\n",
            "Epoch: 10->Batch: 880 / 1875. Loss = 0.3393682539463043\n",
            "Epoch: 10->Batch: 900 / 1875. Loss = 0.463293194770813\n",
            "Epoch: 10->Batch: 920 / 1875. Loss = 0.43258172273635864\n",
            "Epoch: 10->Batch: 940 / 1875. Loss = 0.46745049953460693\n",
            "Epoch: 10->Batch: 960 / 1875. Loss = 0.26064878702163696\n",
            "Epoch: 10->Batch: 980 / 1875. Loss = 0.30536383390426636\n",
            "Epoch: 10->Batch: 1000 / 1875. Loss = 0.27121493220329285\n",
            "Epoch: 10->Batch: 1020 / 1875. Loss = 0.40185898542404175\n",
            "Epoch: 10->Batch: 1040 / 1875. Loss = 0.4264352023601532\n",
            "Epoch: 10->Batch: 1060 / 1875. Loss = 0.17623679339885712\n",
            "Epoch: 10->Batch: 1080 / 1875. Loss = 0.17587843537330627\n",
            "Epoch: 10->Batch: 1100 / 1875. Loss = 0.2240258753299713\n",
            "Epoch: 10->Batch: 1120 / 1875. Loss = 0.4867076575756073\n",
            "Epoch: 10->Batch: 1140 / 1875. Loss = 0.3366318345069885\n",
            "Epoch: 10->Batch: 1160 / 1875. Loss = 0.25702106952667236\n",
            "Epoch: 10->Batch: 1180 / 1875. Loss = 0.4117070734500885\n",
            "Epoch: 10->Batch: 1200 / 1875. Loss = 0.3433709144592285\n",
            "Epoch: 10->Batch: 1220 / 1875. Loss = 0.18337193131446838\n",
            "Epoch: 10->Batch: 1240 / 1875. Loss = 0.44695407152175903\n",
            "Epoch: 10->Batch: 1260 / 1875. Loss = 0.1414077877998352\n",
            "Epoch: 10->Batch: 1280 / 1875. Loss = 0.25871357321739197\n",
            "Epoch: 10->Batch: 1300 / 1875. Loss = 0.2696124017238617\n",
            "Epoch: 10->Batch: 1320 / 1875. Loss = 0.3962779939174652\n",
            "Epoch: 10->Batch: 1340 / 1875. Loss = 0.23237407207489014\n",
            "Epoch: 10->Batch: 1360 / 1875. Loss = 0.35967111587524414\n",
            "Epoch: 10->Batch: 1380 / 1875. Loss = 0.2420249581336975\n",
            "Epoch: 10->Batch: 1400 / 1875. Loss = 0.1366942971944809\n",
            "Epoch: 10->Batch: 1420 / 1875. Loss = 0.33689653873443604\n",
            "Epoch: 10->Batch: 1440 / 1875. Loss = 0.21948066353797913\n",
            "Epoch: 10->Batch: 1460 / 1875. Loss = 0.1647568643093109\n",
            "Epoch: 10->Batch: 1480 / 1875. Loss = 0.38672351837158203\n",
            "Epoch: 10->Batch: 1500 / 1875. Loss = 0.12295433133840561\n",
            "Epoch: 10->Batch: 1520 / 1875. Loss = 0.3941398859024048\n",
            "Epoch: 10->Batch: 1540 / 1875. Loss = 0.3241884708404541\n",
            "Epoch: 10->Batch: 1560 / 1875. Loss = 0.18279163539409637\n",
            "Epoch: 10->Batch: 1580 / 1875. Loss = 0.29898786544799805\n",
            "Epoch: 10->Batch: 1600 / 1875. Loss = 0.3400677442550659\n",
            "Epoch: 10->Batch: 1620 / 1875. Loss = 0.09749088436365128\n",
            "Epoch: 10->Batch: 1640 / 1875. Loss = 0.32514625787734985\n",
            "Epoch: 10->Batch: 1660 / 1875. Loss = 0.35703426599502563\n",
            "Epoch: 10->Batch: 1680 / 1875. Loss = 0.3200720548629761\n",
            "Epoch: 10->Batch: 1700 / 1875. Loss = 0.1685374677181244\n",
            "Epoch: 10->Batch: 1720 / 1875. Loss = 0.2662953734397888\n",
            "Epoch: 10->Batch: 1740 / 1875. Loss = 0.1648840755224228\n",
            "Epoch: 10->Batch: 1760 / 1875. Loss = 0.1348685622215271\n",
            "Epoch: 10->Batch: 1780 / 1875. Loss = 0.16413737833499908\n",
            "Epoch: 10->Batch: 1800 / 1875. Loss = 0.2080315351486206\n",
            "Epoch: 10->Batch: 1820 / 1875. Loss = 0.10943730175495148\n",
            "Epoch: 10->Batch: 1840 / 1875. Loss = 0.2561763525009155\n",
            "Epoch: 10->Batch: 1860 / 1875. Loss = 0.11687701940536499\n",
            "In Testing Function!\n",
            "Accuracy: 0.8883 (8883 / 10000)\n",
            "Epoch: 11 start ------------\n",
            "\n",
            "Epoch: 11->Batch: 0 / 1875. Loss = 0.5344850420951843\n",
            "Epoch: 11->Batch: 20 / 1875. Loss = 0.3861074447631836\n",
            "Epoch: 11->Batch: 40 / 1875. Loss = 0.322028249502182\n",
            "Epoch: 11->Batch: 60 / 1875. Loss = 0.29038703441619873\n",
            "Epoch: 11->Batch: 80 / 1875. Loss = 0.1832299679517746\n",
            "Epoch: 11->Batch: 100 / 1875. Loss = 0.2803007960319519\n",
            "Epoch: 11->Batch: 120 / 1875. Loss = 0.23421546816825867\n",
            "Epoch: 11->Batch: 140 / 1875. Loss = 0.16892290115356445\n",
            "Epoch: 11->Batch: 160 / 1875. Loss = 0.22538144886493683\n",
            "Epoch: 11->Batch: 180 / 1875. Loss = 0.39690300822257996\n",
            "Epoch: 11->Batch: 200 / 1875. Loss = 0.21710950136184692\n",
            "Epoch: 11->Batch: 220 / 1875. Loss = 0.23957520723342896\n",
            "Epoch: 11->Batch: 240 / 1875. Loss = 0.25032535195350647\n",
            "Epoch: 11->Batch: 260 / 1875. Loss = 0.6229658126831055\n",
            "Epoch: 11->Batch: 280 / 1875. Loss = 0.230987086892128\n",
            "Epoch: 11->Batch: 300 / 1875. Loss = 0.20507004857063293\n",
            "Epoch: 11->Batch: 320 / 1875. Loss = 0.27326133847236633\n",
            "Epoch: 11->Batch: 340 / 1875. Loss = 0.2877984046936035\n",
            "Epoch: 11->Batch: 360 / 1875. Loss = 0.42409181594848633\n",
            "Epoch: 11->Batch: 380 / 1875. Loss = 0.2474082112312317\n",
            "Epoch: 11->Batch: 400 / 1875. Loss = 0.27926090359687805\n",
            "Epoch: 11->Batch: 420 / 1875. Loss = 0.18472923338413239\n",
            "Epoch: 11->Batch: 440 / 1875. Loss = 0.3459678590297699\n",
            "Epoch: 11->Batch: 460 / 1875. Loss = 0.25647860765457153\n",
            "Epoch: 11->Batch: 480 / 1875. Loss = 0.26623207330703735\n",
            "Epoch: 11->Batch: 500 / 1875. Loss = 0.29860952496528625\n",
            "Epoch: 11->Batch: 520 / 1875. Loss = 0.24160876870155334\n",
            "Epoch: 11->Batch: 540 / 1875. Loss = 0.21655042469501495\n",
            "Epoch: 11->Batch: 560 / 1875. Loss = 0.285366415977478\n",
            "Epoch: 11->Batch: 580 / 1875. Loss = 0.22814959287643433\n",
            "Epoch: 11->Batch: 600 / 1875. Loss = 0.3684971034526825\n",
            "Epoch: 11->Batch: 620 / 1875. Loss = 0.2642859220504761\n",
            "Epoch: 11->Batch: 640 / 1875. Loss = 0.20848095417022705\n",
            "Epoch: 11->Batch: 660 / 1875. Loss = 0.17007774114608765\n",
            "Epoch: 11->Batch: 680 / 1875. Loss = 0.24374066293239594\n",
            "Epoch: 11->Batch: 700 / 1875. Loss = 0.4582318663597107\n",
            "Epoch: 11->Batch: 720 / 1875. Loss = 0.2977715730667114\n",
            "Epoch: 11->Batch: 740 / 1875. Loss = 0.18399107456207275\n",
            "Epoch: 11->Batch: 760 / 1875. Loss = 0.40786826610565186\n",
            "Epoch: 11->Batch: 780 / 1875. Loss = 0.22305499017238617\n",
            "Epoch: 11->Batch: 800 / 1875. Loss = 0.40590736269950867\n",
            "Epoch: 11->Batch: 820 / 1875. Loss = 0.2597508728504181\n",
            "Epoch: 11->Batch: 840 / 1875. Loss = 0.19278739392757416\n",
            "Epoch: 11->Batch: 860 / 1875. Loss = 0.057805612683296204\n",
            "Epoch: 11->Batch: 880 / 1875. Loss = 0.22628724575042725\n",
            "Epoch: 11->Batch: 900 / 1875. Loss = 0.413265585899353\n",
            "Epoch: 11->Batch: 920 / 1875. Loss = 0.3139995336532593\n",
            "Epoch: 11->Batch: 940 / 1875. Loss = 0.2500876188278198\n",
            "Epoch: 11->Batch: 960 / 1875. Loss = 0.17653067409992218\n",
            "Epoch: 11->Batch: 980 / 1875. Loss = 0.3029036819934845\n",
            "Epoch: 11->Batch: 1000 / 1875. Loss = 0.1659124344587326\n",
            "Epoch: 11->Batch: 1020 / 1875. Loss = 0.11556045711040497\n",
            "Epoch: 11->Batch: 1040 / 1875. Loss = 0.27840372920036316\n",
            "Epoch: 11->Batch: 1060 / 1875. Loss = 0.17451021075248718\n",
            "Epoch: 11->Batch: 1080 / 1875. Loss = 0.17486338317394257\n",
            "Epoch: 11->Batch: 1100 / 1875. Loss = 0.433593213558197\n",
            "Epoch: 11->Batch: 1120 / 1875. Loss = 0.18910712003707886\n",
            "Epoch: 11->Batch: 1140 / 1875. Loss = 0.2506006956100464\n",
            "Epoch: 11->Batch: 1160 / 1875. Loss = 0.4651651978492737\n",
            "Epoch: 11->Batch: 1180 / 1875. Loss = 0.10051052272319794\n",
            "Epoch: 11->Batch: 1200 / 1875. Loss = 0.30450594425201416\n",
            "Epoch: 11->Batch: 1220 / 1875. Loss = 0.23662562668323517\n",
            "Epoch: 11->Batch: 1240 / 1875. Loss = 0.25389331579208374\n",
            "Epoch: 11->Batch: 1260 / 1875. Loss = 0.3301696181297302\n",
            "Epoch: 11->Batch: 1280 / 1875. Loss = 0.28432708978652954\n",
            "Epoch: 11->Batch: 1300 / 1875. Loss = 0.19481687247753143\n",
            "Epoch: 11->Batch: 1320 / 1875. Loss = 0.1623445600271225\n",
            "Epoch: 11->Batch: 1340 / 1875. Loss = 0.3168076276779175\n",
            "Epoch: 11->Batch: 1360 / 1875. Loss = 0.3344348669052124\n",
            "Epoch: 11->Batch: 1380 / 1875. Loss = 0.3631522059440613\n",
            "Epoch: 11->Batch: 1400 / 1875. Loss = 0.21844148635864258\n",
            "Epoch: 11->Batch: 1420 / 1875. Loss = 0.3298397660255432\n",
            "Epoch: 11->Batch: 1440 / 1875. Loss = 0.37543901801109314\n",
            "Epoch: 11->Batch: 1460 / 1875. Loss = 0.4156492352485657\n",
            "Epoch: 11->Batch: 1480 / 1875. Loss = 0.2715738117694855\n",
            "Epoch: 11->Batch: 1500 / 1875. Loss = 0.2464275360107422\n",
            "Epoch: 11->Batch: 1520 / 1875. Loss = 0.18502670526504517\n",
            "Epoch: 11->Batch: 1540 / 1875. Loss = 0.10022170841693878\n",
            "Epoch: 11->Batch: 1560 / 1875. Loss = 0.38434529304504395\n",
            "Epoch: 11->Batch: 1580 / 1875. Loss = 0.23926889896392822\n",
            "Epoch: 11->Batch: 1600 / 1875. Loss = 0.23456193506717682\n",
            "Epoch: 11->Batch: 1620 / 1875. Loss = 0.18125632405281067\n",
            "Epoch: 11->Batch: 1640 / 1875. Loss = 0.13725662231445312\n",
            "Epoch: 11->Batch: 1660 / 1875. Loss = 0.16667288541793823\n",
            "Epoch: 11->Batch: 1680 / 1875. Loss = 0.21086280047893524\n",
            "Epoch: 11->Batch: 1700 / 1875. Loss = 0.46851029992103577\n",
            "Epoch: 11->Batch: 1720 / 1875. Loss = 0.2704241871833801\n",
            "Epoch: 11->Batch: 1740 / 1875. Loss = 0.219729483127594\n",
            "Epoch: 11->Batch: 1760 / 1875. Loss = 0.21354109048843384\n",
            "Epoch: 11->Batch: 1780 / 1875. Loss = 0.2927509546279907\n",
            "Epoch: 11->Batch: 1800 / 1875. Loss = 0.40363118052482605\n",
            "Epoch: 11->Batch: 1820 / 1875. Loss = 0.3289240896701813\n",
            "Epoch: 11->Batch: 1840 / 1875. Loss = 0.2186126410961151\n",
            "Epoch: 11->Batch: 1860 / 1875. Loss = 0.6888136863708496\n",
            "In Testing Function!\n",
            "Accuracy: 0.892 (8920 / 10000)\n",
            "Epoch: 12 start ------------\n",
            "\n",
            "Epoch: 12->Batch: 0 / 1875. Loss = 0.38012176752090454\n",
            "Epoch: 12->Batch: 20 / 1875. Loss = 0.29528653621673584\n",
            "Epoch: 12->Batch: 40 / 1875. Loss = 0.48560982942581177\n",
            "Epoch: 12->Batch: 60 / 1875. Loss = 0.10270261764526367\n",
            "Epoch: 12->Batch: 80 / 1875. Loss = 0.3374844491481781\n",
            "Epoch: 12->Batch: 100 / 1875. Loss = 0.2239415943622589\n",
            "Epoch: 12->Batch: 120 / 1875. Loss = 0.06647568941116333\n",
            "Epoch: 12->Batch: 140 / 1875. Loss = 0.35354340076446533\n",
            "Epoch: 12->Batch: 160 / 1875. Loss = 0.22451813519001007\n",
            "Epoch: 12->Batch: 180 / 1875. Loss = 0.2440270334482193\n",
            "Epoch: 12->Batch: 200 / 1875. Loss = 0.32733601331710815\n",
            "Epoch: 12->Batch: 220 / 1875. Loss = 0.18603047728538513\n",
            "Epoch: 12->Batch: 240 / 1875. Loss = 0.23271706700325012\n",
            "Epoch: 12->Batch: 260 / 1875. Loss = 0.35257863998413086\n",
            "Epoch: 12->Batch: 280 / 1875. Loss = 0.3831065595149994\n",
            "Epoch: 12->Batch: 300 / 1875. Loss = 0.23552921414375305\n",
            "Epoch: 12->Batch: 320 / 1875. Loss = 0.3659658133983612\n",
            "Epoch: 12->Batch: 340 / 1875. Loss = 0.19809547066688538\n",
            "Epoch: 12->Batch: 360 / 1875. Loss = 0.24243280291557312\n",
            "Epoch: 12->Batch: 380 / 1875. Loss = 0.271854043006897\n",
            "Epoch: 12->Batch: 400 / 1875. Loss = 0.21397008001804352\n",
            "Epoch: 12->Batch: 420 / 1875. Loss = 0.32277727127075195\n",
            "Epoch: 12->Batch: 440 / 1875. Loss = 0.8747024536132812\n",
            "Epoch: 12->Batch: 460 / 1875. Loss = 0.19325950741767883\n",
            "Epoch: 12->Batch: 480 / 1875. Loss = 0.05301709473133087\n",
            "Epoch: 12->Batch: 500 / 1875. Loss = 0.3238469064235687\n",
            "Epoch: 12->Batch: 520 / 1875. Loss = 0.16383449733257294\n",
            "Epoch: 12->Batch: 540 / 1875. Loss = 0.25979694724082947\n",
            "Epoch: 12->Batch: 560 / 1875. Loss = 0.10458379983901978\n",
            "Epoch: 12->Batch: 580 / 1875. Loss = 0.12939395010471344\n",
            "Epoch: 12->Batch: 600 / 1875. Loss = 0.3798084259033203\n",
            "Epoch: 12->Batch: 620 / 1875. Loss = 0.5770889520645142\n",
            "Epoch: 12->Batch: 640 / 1875. Loss = 0.18395164608955383\n",
            "Epoch: 12->Batch: 660 / 1875. Loss = 0.11670951545238495\n",
            "Epoch: 12->Batch: 680 / 1875. Loss = 0.1890316754579544\n",
            "Epoch: 12->Batch: 700 / 1875. Loss = 0.12801603972911835\n",
            "Epoch: 12->Batch: 720 / 1875. Loss = 0.4415227770805359\n",
            "Epoch: 12->Batch: 740 / 1875. Loss = 0.4801972210407257\n",
            "Epoch: 12->Batch: 760 / 1875. Loss = 0.3728656768798828\n",
            "Epoch: 12->Batch: 780 / 1875. Loss = 0.213520348072052\n",
            "Epoch: 12->Batch: 800 / 1875. Loss = 0.3800423741340637\n",
            "Epoch: 12->Batch: 820 / 1875. Loss = 0.25823846459388733\n",
            "Epoch: 12->Batch: 840 / 1875. Loss = 0.31898677349090576\n",
            "Epoch: 12->Batch: 860 / 1875. Loss = 0.30278536677360535\n",
            "Epoch: 12->Batch: 880 / 1875. Loss = 0.07886697351932526\n",
            "Epoch: 12->Batch: 900 / 1875. Loss = 0.3102857172489166\n",
            "Epoch: 12->Batch: 920 / 1875. Loss = 0.5033432245254517\n",
            "Epoch: 12->Batch: 940 / 1875. Loss = 0.24066290259361267\n",
            "Epoch: 12->Batch: 960 / 1875. Loss = 0.2110297530889511\n",
            "Epoch: 12->Batch: 980 / 1875. Loss = 0.20275822281837463\n",
            "Epoch: 12->Batch: 1000 / 1875. Loss = 0.42166027426719666\n",
            "Epoch: 12->Batch: 1020 / 1875. Loss = 0.17608655989170074\n",
            "Epoch: 12->Batch: 1040 / 1875. Loss = 0.11289165169000626\n",
            "Epoch: 12->Batch: 1060 / 1875. Loss = 0.2193940281867981\n",
            "Epoch: 12->Batch: 1080 / 1875. Loss = 0.13354182243347168\n",
            "Epoch: 12->Batch: 1100 / 1875. Loss = 0.34163448214530945\n",
            "Epoch: 12->Batch: 1120 / 1875. Loss = 0.13990594446659088\n",
            "Epoch: 12->Batch: 1140 / 1875. Loss = 0.23589634895324707\n",
            "Epoch: 12->Batch: 1160 / 1875. Loss = 0.32198604941368103\n",
            "Epoch: 12->Batch: 1180 / 1875. Loss = 0.17966356873512268\n",
            "Epoch: 12->Batch: 1200 / 1875. Loss = 0.12196826934814453\n",
            "Epoch: 12->Batch: 1220 / 1875. Loss = 0.191448375582695\n",
            "Epoch: 12->Batch: 1240 / 1875. Loss = 0.22584332525730133\n",
            "Epoch: 12->Batch: 1260 / 1875. Loss = 0.2248128205537796\n",
            "Epoch: 12->Batch: 1280 / 1875. Loss = 0.35424143075942993\n",
            "Epoch: 12->Batch: 1300 / 1875. Loss = 0.310486763715744\n",
            "Epoch: 12->Batch: 1320 / 1875. Loss = 0.10670305788516998\n",
            "Epoch: 12->Batch: 1340 / 1875. Loss = 0.3373056948184967\n",
            "Epoch: 12->Batch: 1360 / 1875. Loss = 0.40647459030151367\n",
            "Epoch: 12->Batch: 1380 / 1875. Loss = 0.4897560477256775\n",
            "Epoch: 12->Batch: 1400 / 1875. Loss = 0.16473102569580078\n",
            "Epoch: 12->Batch: 1420 / 1875. Loss = 0.32220038771629333\n",
            "Epoch: 12->Batch: 1440 / 1875. Loss = 0.1770733743906021\n",
            "Epoch: 12->Batch: 1460 / 1875. Loss = 0.18042513728141785\n",
            "Epoch: 12->Batch: 1480 / 1875. Loss = 0.16399063169956207\n",
            "Epoch: 12->Batch: 1500 / 1875. Loss = 0.6243420839309692\n",
            "Epoch: 12->Batch: 1520 / 1875. Loss = 0.5644731521606445\n",
            "Epoch: 12->Batch: 1540 / 1875. Loss = 0.2538878917694092\n",
            "Epoch: 12->Batch: 1560 / 1875. Loss = 0.13606882095336914\n",
            "Epoch: 12->Batch: 1580 / 1875. Loss = 0.11861792206764221\n",
            "Epoch: 12->Batch: 1600 / 1875. Loss = 0.22663387656211853\n",
            "Epoch: 12->Batch: 1620 / 1875. Loss = 0.2233085036277771\n",
            "Epoch: 12->Batch: 1640 / 1875. Loss = 0.1300305873155594\n",
            "Epoch: 12->Batch: 1660 / 1875. Loss = 0.325266033411026\n",
            "Epoch: 12->Batch: 1680 / 1875. Loss = 0.19155125319957733\n",
            "Epoch: 12->Batch: 1700 / 1875. Loss = 0.4817723035812378\n",
            "Epoch: 12->Batch: 1720 / 1875. Loss = 0.13387906551361084\n",
            "Epoch: 12->Batch: 1740 / 1875. Loss = 0.13980048894882202\n",
            "Epoch: 12->Batch: 1760 / 1875. Loss = 0.1847536712884903\n",
            "Epoch: 12->Batch: 1780 / 1875. Loss = 0.3635222315788269\n",
            "Epoch: 12->Batch: 1800 / 1875. Loss = 0.41016069054603577\n",
            "Epoch: 12->Batch: 1820 / 1875. Loss = 0.1993873119354248\n",
            "Epoch: 12->Batch: 1840 / 1875. Loss = 0.09140035510063171\n",
            "Epoch: 12->Batch: 1860 / 1875. Loss = 0.12979601323604584\n",
            "In Testing Function!\n",
            "Accuracy: 0.894 (8940 / 10000)\n",
            "Epoch: 13 start ------------\n",
            "\n",
            "Epoch: 13->Batch: 0 / 1875. Loss = 0.3506409823894501\n",
            "Epoch: 13->Batch: 20 / 1875. Loss = 0.11693406105041504\n",
            "Epoch: 13->Batch: 40 / 1875. Loss = 0.3974340558052063\n",
            "Epoch: 13->Batch: 60 / 1875. Loss = 0.5000738501548767\n",
            "Epoch: 13->Batch: 80 / 1875. Loss = 0.2020491063594818\n",
            "Epoch: 13->Batch: 100 / 1875. Loss = 0.15380828082561493\n",
            "Epoch: 13->Batch: 120 / 1875. Loss = 0.13490590453147888\n",
            "Epoch: 13->Batch: 140 / 1875. Loss = 0.1910175085067749\n",
            "Epoch: 13->Batch: 160 / 1875. Loss = 0.17531003057956696\n",
            "Epoch: 13->Batch: 180 / 1875. Loss = 0.46237650513648987\n",
            "Epoch: 13->Batch: 200 / 1875. Loss = 0.2558087110519409\n",
            "Epoch: 13->Batch: 220 / 1875. Loss = 0.2551623582839966\n",
            "Epoch: 13->Batch: 240 / 1875. Loss = 0.21525977551937103\n",
            "Epoch: 13->Batch: 260 / 1875. Loss = 0.29861462116241455\n",
            "Epoch: 13->Batch: 280 / 1875. Loss = 0.15804079174995422\n",
            "Epoch: 13->Batch: 300 / 1875. Loss = 0.3363041579723358\n",
            "Epoch: 13->Batch: 320 / 1875. Loss = 0.22764644026756287\n",
            "Epoch: 13->Batch: 340 / 1875. Loss = 0.019354060292243958\n",
            "Epoch: 13->Batch: 360 / 1875. Loss = 0.4903627634048462\n",
            "Epoch: 13->Batch: 380 / 1875. Loss = 0.22058744728565216\n",
            "Epoch: 13->Batch: 400 / 1875. Loss = 0.4486316740512848\n",
            "Epoch: 13->Batch: 420 / 1875. Loss = 0.18428954482078552\n",
            "Epoch: 13->Batch: 440 / 1875. Loss = 0.08623206615447998\n",
            "Epoch: 13->Batch: 460 / 1875. Loss = 0.5144034028053284\n",
            "Epoch: 13->Batch: 480 / 1875. Loss = 0.3051636219024658\n",
            "Epoch: 13->Batch: 500 / 1875. Loss = 0.33410102128982544\n",
            "Epoch: 13->Batch: 520 / 1875. Loss = 0.2984107434749603\n",
            "Epoch: 13->Batch: 540 / 1875. Loss = 0.6061559915542603\n",
            "Epoch: 13->Batch: 560 / 1875. Loss = 0.18481257557868958\n",
            "Epoch: 13->Batch: 580 / 1875. Loss = 0.1275462955236435\n",
            "Epoch: 13->Batch: 600 / 1875. Loss = 0.26017895340919495\n",
            "Epoch: 13->Batch: 620 / 1875. Loss = 0.4873558282852173\n",
            "Epoch: 13->Batch: 640 / 1875. Loss = 0.3657619059085846\n",
            "Epoch: 13->Batch: 660 / 1875. Loss = 0.15719810128211975\n",
            "Epoch: 13->Batch: 680 / 1875. Loss = 0.32229161262512207\n",
            "Epoch: 13->Batch: 700 / 1875. Loss = 0.24625693261623383\n",
            "Epoch: 13->Batch: 720 / 1875. Loss = 0.14827308058738708\n",
            "Epoch: 13->Batch: 740 / 1875. Loss = 0.1900923103094101\n",
            "Epoch: 13->Batch: 760 / 1875. Loss = 0.15124531090259552\n",
            "Epoch: 13->Batch: 780 / 1875. Loss = 0.13176913559436798\n",
            "Epoch: 13->Batch: 800 / 1875. Loss = 0.46482816338539124\n",
            "Epoch: 13->Batch: 820 / 1875. Loss = 0.12516196072101593\n",
            "Epoch: 13->Batch: 840 / 1875. Loss = 0.297710120677948\n",
            "Epoch: 13->Batch: 860 / 1875. Loss = 0.24051187932491302\n",
            "Epoch: 13->Batch: 880 / 1875. Loss = 0.2467590570449829\n",
            "Epoch: 13->Batch: 900 / 1875. Loss = 0.39023885130882263\n",
            "Epoch: 13->Batch: 920 / 1875. Loss = 0.1423853188753128\n",
            "Epoch: 13->Batch: 940 / 1875. Loss = 0.14975899457931519\n",
            "Epoch: 13->Batch: 960 / 1875. Loss = 0.2545040547847748\n",
            "Epoch: 13->Batch: 980 / 1875. Loss = 0.0723448395729065\n",
            "Epoch: 13->Batch: 1000 / 1875. Loss = 0.30116963386535645\n",
            "Epoch: 13->Batch: 1020 / 1875. Loss = 0.26026588678359985\n",
            "Epoch: 13->Batch: 1040 / 1875. Loss = 0.3011677861213684\n",
            "Epoch: 13->Batch: 1060 / 1875. Loss = 0.3258497714996338\n",
            "Epoch: 13->Batch: 1080 / 1875. Loss = 0.19012005627155304\n",
            "Epoch: 13->Batch: 1100 / 1875. Loss = 0.20095305144786835\n",
            "Epoch: 13->Batch: 1120 / 1875. Loss = 0.5906469225883484\n",
            "Epoch: 13->Batch: 1140 / 1875. Loss = 0.2505940794944763\n",
            "Epoch: 13->Batch: 1160 / 1875. Loss = 0.18420462310314178\n",
            "Epoch: 13->Batch: 1180 / 1875. Loss = 0.2920440137386322\n",
            "Epoch: 13->Batch: 1200 / 1875. Loss = 0.42473551630973816\n",
            "Epoch: 13->Batch: 1220 / 1875. Loss = 0.253903329372406\n",
            "Epoch: 13->Batch: 1240 / 1875. Loss = 0.33070892095565796\n",
            "Epoch: 13->Batch: 1260 / 1875. Loss = 0.4072200059890747\n",
            "Epoch: 13->Batch: 1280 / 1875. Loss = 0.22155538201332092\n",
            "Epoch: 13->Batch: 1300 / 1875. Loss = 0.3012535572052002\n",
            "Epoch: 13->Batch: 1320 / 1875. Loss = 0.0636700838804245\n",
            "Epoch: 13->Batch: 1340 / 1875. Loss = 0.35103917121887207\n",
            "Epoch: 13->Batch: 1360 / 1875. Loss = 0.33467546105384827\n",
            "Epoch: 13->Batch: 1380 / 1875. Loss = 0.1693398654460907\n",
            "Epoch: 13->Batch: 1400 / 1875. Loss = 0.21996106207370758\n",
            "Epoch: 13->Batch: 1420 / 1875. Loss = 0.3242570757865906\n",
            "Epoch: 13->Batch: 1440 / 1875. Loss = 0.12839922308921814\n",
            "Epoch: 13->Batch: 1460 / 1875. Loss = 0.18857361376285553\n",
            "Epoch: 13->Batch: 1480 / 1875. Loss = 0.15450693666934967\n",
            "Epoch: 13->Batch: 1500 / 1875. Loss = 0.6235612630844116\n",
            "Epoch: 13->Batch: 1520 / 1875. Loss = 0.22043287754058838\n",
            "Epoch: 13->Batch: 1540 / 1875. Loss = 0.13673833012580872\n",
            "Epoch: 13->Batch: 1560 / 1875. Loss = 0.29834240674972534\n",
            "Epoch: 13->Batch: 1580 / 1875. Loss = 0.15113113820552826\n",
            "Epoch: 13->Batch: 1600 / 1875. Loss = 0.37084534764289856\n",
            "Epoch: 13->Batch: 1620 / 1875. Loss = 0.11925072968006134\n",
            "Epoch: 13->Batch: 1640 / 1875. Loss = 0.18223215639591217\n",
            "Epoch: 13->Batch: 1660 / 1875. Loss = 0.1817263662815094\n",
            "Epoch: 13->Batch: 1680 / 1875. Loss = 0.12954699993133545\n",
            "Epoch: 13->Batch: 1700 / 1875. Loss = 0.39703643321990967\n",
            "Epoch: 13->Batch: 1720 / 1875. Loss = 0.31111636757850647\n",
            "Epoch: 13->Batch: 1740 / 1875. Loss = 0.10629589855670929\n",
            "Epoch: 13->Batch: 1760 / 1875. Loss = 0.3469262719154358\n",
            "Epoch: 13->Batch: 1780 / 1875. Loss = 0.32299739122390747\n",
            "Epoch: 13->Batch: 1800 / 1875. Loss = 0.20308221876621246\n",
            "Epoch: 13->Batch: 1820 / 1875. Loss = 0.1588158905506134\n",
            "Epoch: 13->Batch: 1840 / 1875. Loss = 0.3065110445022583\n",
            "Epoch: 13->Batch: 1860 / 1875. Loss = 0.3844727575778961\n",
            "In Testing Function!\n",
            "Accuracy: 0.8925 (8925 / 10000)\n",
            "Epoch: 14 start ------------\n",
            "\n",
            "Epoch: 14->Batch: 0 / 1875. Loss = 0.45010268688201904\n",
            "Epoch: 14->Batch: 20 / 1875. Loss = 0.2602883577346802\n",
            "Epoch: 14->Batch: 40 / 1875. Loss = 0.35083240270614624\n",
            "Epoch: 14->Batch: 60 / 1875. Loss = 0.03754784166812897\n",
            "Epoch: 14->Batch: 80 / 1875. Loss = 0.16212108731269836\n",
            "Epoch: 14->Batch: 100 / 1875. Loss = 0.3577883541584015\n",
            "Epoch: 14->Batch: 120 / 1875. Loss = 0.16164074838161469\n",
            "Epoch: 14->Batch: 140 / 1875. Loss = 0.24416862428188324\n",
            "Epoch: 14->Batch: 160 / 1875. Loss = 0.10230895131826401\n",
            "Epoch: 14->Batch: 180 / 1875. Loss = 0.18875433504581451\n",
            "Epoch: 14->Batch: 200 / 1875. Loss = 0.14607545733451843\n",
            "Epoch: 14->Batch: 220 / 1875. Loss = 0.0798293799161911\n",
            "Epoch: 14->Batch: 240 / 1875. Loss = 0.3780220150947571\n",
            "Epoch: 14->Batch: 260 / 1875. Loss = 0.2564219534397125\n",
            "Epoch: 14->Batch: 280 / 1875. Loss = 0.27104100584983826\n",
            "Epoch: 14->Batch: 300 / 1875. Loss = 0.3115820586681366\n",
            "Epoch: 14->Batch: 320 / 1875. Loss = 0.1440838724374771\n",
            "Epoch: 14->Batch: 340 / 1875. Loss = 0.3408397436141968\n",
            "Epoch: 14->Batch: 360 / 1875. Loss = 0.23082798719406128\n",
            "Epoch: 14->Batch: 380 / 1875. Loss = 0.11767403781414032\n",
            "Epoch: 14->Batch: 400 / 1875. Loss = 0.3731173872947693\n",
            "Epoch: 14->Batch: 420 / 1875. Loss = 0.2521117627620697\n",
            "Epoch: 14->Batch: 440 / 1875. Loss = 0.4590904712677002\n",
            "Epoch: 14->Batch: 460 / 1875. Loss = 0.18521879613399506\n",
            "Epoch: 14->Batch: 480 / 1875. Loss = 0.2654861509799957\n",
            "Epoch: 14->Batch: 500 / 1875. Loss = 0.32398325204849243\n",
            "Epoch: 14->Batch: 520 / 1875. Loss = 0.20156548917293549\n",
            "Epoch: 14->Batch: 540 / 1875. Loss = 0.33843761682510376\n",
            "Epoch: 14->Batch: 560 / 1875. Loss = 0.7586418390274048\n",
            "Epoch: 14->Batch: 580 / 1875. Loss = 0.17347271740436554\n",
            "Epoch: 14->Batch: 600 / 1875. Loss = 0.15537910163402557\n",
            "Epoch: 14->Batch: 620 / 1875. Loss = 0.6484417915344238\n",
            "Epoch: 14->Batch: 640 / 1875. Loss = 0.331638365983963\n",
            "Epoch: 14->Batch: 660 / 1875. Loss = 0.3905419111251831\n",
            "Epoch: 14->Batch: 680 / 1875. Loss = 0.16203688085079193\n",
            "Epoch: 14->Batch: 700 / 1875. Loss = 0.2756503224372864\n",
            "Epoch: 14->Batch: 720 / 1875. Loss = 0.13662974536418915\n",
            "Epoch: 14->Batch: 740 / 1875. Loss = 0.3947408199310303\n",
            "Epoch: 14->Batch: 760 / 1875. Loss = 0.2356165647506714\n",
            "Epoch: 14->Batch: 780 / 1875. Loss = 0.209239199757576\n",
            "Epoch: 14->Batch: 800 / 1875. Loss = 0.28284865617752075\n",
            "Epoch: 14->Batch: 820 / 1875. Loss = 0.1917176991701126\n",
            "Epoch: 14->Batch: 840 / 1875. Loss = 0.422755628824234\n",
            "Epoch: 14->Batch: 860 / 1875. Loss = 0.4020524322986603\n",
            "Epoch: 14->Batch: 880 / 1875. Loss = 0.37577685713768005\n",
            "Epoch: 14->Batch: 900 / 1875. Loss = 0.3043973743915558\n",
            "Epoch: 14->Batch: 920 / 1875. Loss = 0.35752394795417786\n",
            "Epoch: 14->Batch: 940 / 1875. Loss = 0.18765977025032043\n",
            "Epoch: 14->Batch: 960 / 1875. Loss = 0.2580772936344147\n",
            "Epoch: 14->Batch: 980 / 1875. Loss = 0.3258262872695923\n",
            "Epoch: 14->Batch: 1000 / 1875. Loss = 0.0522569864988327\n",
            "Epoch: 14->Batch: 1020 / 1875. Loss = 0.4750231206417084\n",
            "Epoch: 14->Batch: 1040 / 1875. Loss = 0.27361953258514404\n",
            "Epoch: 14->Batch: 1060 / 1875. Loss = 0.6915686726570129\n",
            "Epoch: 14->Batch: 1080 / 1875. Loss = 0.2913306653499603\n",
            "Epoch: 14->Batch: 1100 / 1875. Loss = 0.08863618969917297\n",
            "Epoch: 14->Batch: 1120 / 1875. Loss = 0.08398964256048203\n",
            "Epoch: 14->Batch: 1140 / 1875. Loss = 0.1661251187324524\n",
            "Epoch: 14->Batch: 1160 / 1875. Loss = 0.1829719841480255\n",
            "Epoch: 14->Batch: 1180 / 1875. Loss = 0.25719261169433594\n",
            "Epoch: 14->Batch: 1200 / 1875. Loss = 0.27790093421936035\n",
            "Epoch: 14->Batch: 1220 / 1875. Loss = 0.08955566585063934\n",
            "Epoch: 14->Batch: 1240 / 1875. Loss = 0.318276047706604\n",
            "Epoch: 14->Batch: 1260 / 1875. Loss = 0.08385945111513138\n",
            "Epoch: 14->Batch: 1280 / 1875. Loss = 0.2902555465698242\n",
            "Epoch: 14->Batch: 1300 / 1875. Loss = 0.20215177536010742\n",
            "Epoch: 14->Batch: 1320 / 1875. Loss = 0.22709153592586517\n",
            "Epoch: 14->Batch: 1340 / 1875. Loss = 0.2536340057849884\n",
            "Epoch: 14->Batch: 1360 / 1875. Loss = 0.16037914156913757\n",
            "Epoch: 14->Batch: 1380 / 1875. Loss = 0.19839158654212952\n",
            "Epoch: 14->Batch: 1400 / 1875. Loss = 0.2573980689048767\n",
            "Epoch: 14->Batch: 1420 / 1875. Loss = 0.10039026290178299\n",
            "Epoch: 14->Batch: 1440 / 1875. Loss = 0.3849305510520935\n",
            "Epoch: 14->Batch: 1460 / 1875. Loss = 0.22422537207603455\n",
            "Epoch: 14->Batch: 1480 / 1875. Loss = 0.23338289558887482\n",
            "Epoch: 14->Batch: 1500 / 1875. Loss = 0.2595145106315613\n",
            "Epoch: 14->Batch: 1520 / 1875. Loss = 0.29190266132354736\n",
            "Epoch: 14->Batch: 1540 / 1875. Loss = 0.3357137441635132\n",
            "Epoch: 14->Batch: 1560 / 1875. Loss = 0.17138169705867767\n",
            "Epoch: 14->Batch: 1580 / 1875. Loss = 0.2054998278617859\n",
            "Epoch: 14->Batch: 1600 / 1875. Loss = 0.09937632083892822\n",
            "Epoch: 14->Batch: 1620 / 1875. Loss = 0.1547262966632843\n",
            "Epoch: 14->Batch: 1640 / 1875. Loss = 0.31180816888809204\n",
            "Epoch: 14->Batch: 1660 / 1875. Loss = 0.3363875150680542\n",
            "Epoch: 14->Batch: 1680 / 1875. Loss = 0.2970448136329651\n",
            "Epoch: 14->Batch: 1700 / 1875. Loss = 0.11897361278533936\n",
            "Epoch: 14->Batch: 1720 / 1875. Loss = 0.23603449761867523\n",
            "Epoch: 14->Batch: 1740 / 1875. Loss = 0.21366579830646515\n",
            "Epoch: 14->Batch: 1760 / 1875. Loss = 0.32879483699798584\n",
            "Epoch: 14->Batch: 1780 / 1875. Loss = 0.280270516872406\n",
            "Epoch: 14->Batch: 1800 / 1875. Loss = 0.36759892106056213\n",
            "Epoch: 14->Batch: 1820 / 1875. Loss = 0.2387155443429947\n",
            "Epoch: 14->Batch: 1840 / 1875. Loss = 0.1354992687702179\n",
            "Epoch: 14->Batch: 1860 / 1875. Loss = 0.30964744091033936\n",
            "In Testing Function!\n",
            "Accuracy: 0.8947 (8947 / 10000)\n",
            "Epoch: 15 start ------------\n",
            "\n",
            "Epoch: 15->Batch: 0 / 1875. Loss = 0.24265168607234955\n",
            "Epoch: 15->Batch: 20 / 1875. Loss = 0.2597549259662628\n",
            "Epoch: 15->Batch: 40 / 1875. Loss = 0.16362670063972473\n",
            "Epoch: 15->Batch: 60 / 1875. Loss = 0.3902186155319214\n",
            "Epoch: 15->Batch: 80 / 1875. Loss = 0.30822545289993286\n",
            "Epoch: 15->Batch: 100 / 1875. Loss = 0.19068260490894318\n",
            "Epoch: 15->Batch: 120 / 1875. Loss = 0.38579386472702026\n",
            "Epoch: 15->Batch: 140 / 1875. Loss = 0.1425640881061554\n",
            "Epoch: 15->Batch: 160 / 1875. Loss = 0.21715116500854492\n",
            "Epoch: 15->Batch: 180 / 1875. Loss = 0.14091640710830688\n",
            "Epoch: 15->Batch: 200 / 1875. Loss = 0.2567695677280426\n",
            "Epoch: 15->Batch: 220 / 1875. Loss = 0.547819972038269\n",
            "Epoch: 15->Batch: 240 / 1875. Loss = 0.3447940945625305\n",
            "Epoch: 15->Batch: 260 / 1875. Loss = 0.21644742786884308\n",
            "Epoch: 15->Batch: 280 / 1875. Loss = 0.26806506514549255\n",
            "Epoch: 15->Batch: 300 / 1875. Loss = 0.126582071185112\n",
            "Epoch: 15->Batch: 320 / 1875. Loss = 0.10260628908872604\n",
            "Epoch: 15->Batch: 340 / 1875. Loss = 0.12004991620779037\n",
            "Epoch: 15->Batch: 360 / 1875. Loss = 0.3026047348976135\n",
            "Epoch: 15->Batch: 380 / 1875. Loss = 0.14333617687225342\n",
            "Epoch: 15->Batch: 400 / 1875. Loss = 0.17296713590621948\n",
            "Epoch: 15->Batch: 420 / 1875. Loss = 0.21478378772735596\n",
            "Epoch: 15->Batch: 440 / 1875. Loss = 0.3084971606731415\n",
            "Epoch: 15->Batch: 460 / 1875. Loss = 0.45382505655288696\n",
            "Epoch: 15->Batch: 480 / 1875. Loss = 0.20929667353630066\n",
            "Epoch: 15->Batch: 500 / 1875. Loss = 0.43001821637153625\n",
            "Epoch: 15->Batch: 520 / 1875. Loss = 0.25466829538345337\n",
            "Epoch: 15->Batch: 540 / 1875. Loss = 0.20540082454681396\n",
            "Epoch: 15->Batch: 560 / 1875. Loss = 0.05511109530925751\n",
            "Epoch: 15->Batch: 580 / 1875. Loss = 0.3174542188644409\n",
            "Epoch: 15->Batch: 600 / 1875. Loss = 0.14438626170158386\n",
            "Epoch: 15->Batch: 620 / 1875. Loss = 0.3790035843849182\n",
            "Epoch: 15->Batch: 640 / 1875. Loss = 0.2663896679878235\n",
            "Epoch: 15->Batch: 660 / 1875. Loss = 0.3593251705169678\n",
            "Epoch: 15->Batch: 680 / 1875. Loss = 0.16648787260055542\n",
            "Epoch: 15->Batch: 700 / 1875. Loss = 0.2496090978384018\n",
            "Epoch: 15->Batch: 720 / 1875. Loss = 0.21882455050945282\n",
            "Epoch: 15->Batch: 740 / 1875. Loss = 0.20534919202327728\n",
            "Epoch: 15->Batch: 760 / 1875. Loss = 0.24867910146713257\n",
            "Epoch: 15->Batch: 780 / 1875. Loss = 0.26194512844085693\n",
            "Epoch: 15->Batch: 800 / 1875. Loss = 0.3748813271522522\n",
            "Epoch: 15->Batch: 820 / 1875. Loss = 0.5524575710296631\n",
            "Epoch: 15->Batch: 840 / 1875. Loss = 0.11504258960485458\n",
            "Epoch: 15->Batch: 860 / 1875. Loss = 0.33094242215156555\n",
            "Epoch: 15->Batch: 880 / 1875. Loss = 0.1734127253293991\n",
            "Epoch: 15->Batch: 900 / 1875. Loss = 0.36486104130744934\n",
            "Epoch: 15->Batch: 920 / 1875. Loss = 0.11472596228122711\n",
            "Epoch: 15->Batch: 940 / 1875. Loss = 0.25015687942504883\n",
            "Epoch: 15->Batch: 960 / 1875. Loss = 0.27261486649513245\n",
            "Epoch: 15->Batch: 980 / 1875. Loss = 0.29636284708976746\n",
            "Epoch: 15->Batch: 1000 / 1875. Loss = 0.33735835552215576\n",
            "Epoch: 15->Batch: 1020 / 1875. Loss = 0.24550732970237732\n",
            "Epoch: 15->Batch: 1040 / 1875. Loss = 0.1526152789592743\n",
            "Epoch: 15->Batch: 1060 / 1875. Loss = 0.23280228674411774\n",
            "Epoch: 15->Batch: 1080 / 1875. Loss = 0.38086873292922974\n",
            "Epoch: 15->Batch: 1100 / 1875. Loss = 0.15116910636425018\n",
            "Epoch: 15->Batch: 1120 / 1875. Loss = 0.4813988208770752\n",
            "Epoch: 15->Batch: 1140 / 1875. Loss = 0.1608845442533493\n",
            "Epoch: 15->Batch: 1160 / 1875. Loss = 0.15471775829792023\n",
            "Epoch: 15->Batch: 1180 / 1875. Loss = 0.38433679938316345\n",
            "Epoch: 15->Batch: 1200 / 1875. Loss = 0.2515574097633362\n",
            "Epoch: 15->Batch: 1220 / 1875. Loss = 0.4993349015712738\n",
            "Epoch: 15->Batch: 1240 / 1875. Loss = 0.1918359249830246\n",
            "Epoch: 15->Batch: 1260 / 1875. Loss = 0.20195318758487701\n",
            "Epoch: 15->Batch: 1280 / 1875. Loss = 0.18722398579120636\n",
            "Epoch: 15->Batch: 1300 / 1875. Loss = 0.21569038927555084\n",
            "Epoch: 15->Batch: 1320 / 1875. Loss = 0.2433779537677765\n",
            "Epoch: 15->Batch: 1340 / 1875. Loss = 0.32739168405532837\n",
            "Epoch: 15->Batch: 1360 / 1875. Loss = 0.17938359081745148\n",
            "Epoch: 15->Batch: 1380 / 1875. Loss = 0.12781168520450592\n",
            "Epoch: 15->Batch: 1400 / 1875. Loss = 0.20332179963588715\n",
            "Epoch: 15->Batch: 1420 / 1875. Loss = 0.34642285108566284\n",
            "Epoch: 15->Batch: 1440 / 1875. Loss = 0.07253845036029816\n",
            "Epoch: 15->Batch: 1460 / 1875. Loss = 0.2824283540248871\n",
            "Epoch: 15->Batch: 1480 / 1875. Loss = 0.07381080090999603\n",
            "Epoch: 15->Batch: 1500 / 1875. Loss = 0.1272290050983429\n",
            "Epoch: 15->Batch: 1520 / 1875. Loss = 0.21821175515651703\n",
            "Epoch: 15->Batch: 1540 / 1875. Loss = 0.15957438945770264\n",
            "Epoch: 15->Batch: 1560 / 1875. Loss = 0.4063784182071686\n",
            "Epoch: 15->Batch: 1580 / 1875. Loss = 0.15250316262245178\n",
            "Epoch: 15->Batch: 1600 / 1875. Loss = 0.12896405160427094\n",
            "Epoch: 15->Batch: 1620 / 1875. Loss = 0.2586687207221985\n",
            "Epoch: 15->Batch: 1640 / 1875. Loss = 0.42867913842201233\n",
            "Epoch: 15->Batch: 1660 / 1875. Loss = 0.3151707649230957\n",
            "Epoch: 15->Batch: 1680 / 1875. Loss = 0.3928757309913635\n",
            "Epoch: 15->Batch: 1700 / 1875. Loss = 0.24295374751091003\n",
            "Epoch: 15->Batch: 1720 / 1875. Loss = 0.33871233463287354\n",
            "Epoch: 15->Batch: 1740 / 1875. Loss = 0.254537969827652\n",
            "Epoch: 15->Batch: 1760 / 1875. Loss = 0.17065025866031647\n",
            "Epoch: 15->Batch: 1780 / 1875. Loss = 0.1357240378856659\n",
            "Epoch: 15->Batch: 1800 / 1875. Loss = 0.14585930109024048\n",
            "Epoch: 15->Batch: 1820 / 1875. Loss = 0.09165564924478531\n",
            "Epoch: 15->Batch: 1840 / 1875. Loss = 0.4049195647239685\n",
            "Epoch: 15->Batch: 1860 / 1875. Loss = 0.5233688354492188\n",
            "In Testing Function!\n",
            "Accuracy: 0.8954 (8954 / 10000)\n",
            "Epoch: 16 start ------------\n",
            "\n",
            "Epoch: 16->Batch: 0 / 1875. Loss = 0.16762438416481018\n",
            "Epoch: 16->Batch: 20 / 1875. Loss = 0.24986310303211212\n",
            "Epoch: 16->Batch: 40 / 1875. Loss = 0.22500179708003998\n",
            "Epoch: 16->Batch: 60 / 1875. Loss = 0.08202464878559113\n",
            "Epoch: 16->Batch: 80 / 1875. Loss = 0.2223759889602661\n",
            "Epoch: 16->Batch: 100 / 1875. Loss = 0.09188402444124222\n",
            "Epoch: 16->Batch: 120 / 1875. Loss = 0.528438150882721\n",
            "Epoch: 16->Batch: 140 / 1875. Loss = 0.05685795843601227\n",
            "Epoch: 16->Batch: 160 / 1875. Loss = 0.12345938384532928\n",
            "Epoch: 16->Batch: 180 / 1875. Loss = 0.0987357348203659\n",
            "Epoch: 16->Batch: 200 / 1875. Loss = 0.17451238632202148\n",
            "Epoch: 16->Batch: 220 / 1875. Loss = 0.24058042466640472\n",
            "Epoch: 16->Batch: 240 / 1875. Loss = 0.20850320160388947\n",
            "Epoch: 16->Batch: 260 / 1875. Loss = 0.28752484917640686\n",
            "Epoch: 16->Batch: 280 / 1875. Loss = 0.29566484689712524\n",
            "Epoch: 16->Batch: 300 / 1875. Loss = 0.17307089269161224\n",
            "Epoch: 16->Batch: 320 / 1875. Loss = 0.1860497146844864\n",
            "Epoch: 16->Batch: 340 / 1875. Loss = 0.18854692578315735\n",
            "Epoch: 16->Batch: 360 / 1875. Loss = 0.16951337456703186\n",
            "Epoch: 16->Batch: 380 / 1875. Loss = 0.24398137629032135\n",
            "Epoch: 16->Batch: 400 / 1875. Loss = 0.09191940724849701\n",
            "Epoch: 16->Batch: 420 / 1875. Loss = 0.2932868003845215\n",
            "Epoch: 16->Batch: 440 / 1875. Loss = 0.23707140982151031\n",
            "Epoch: 16->Batch: 460 / 1875. Loss = 0.2290673553943634\n",
            "Epoch: 16->Batch: 480 / 1875. Loss = 0.26917269825935364\n",
            "Epoch: 16->Batch: 500 / 1875. Loss = 0.22157001495361328\n",
            "Epoch: 16->Batch: 520 / 1875. Loss = 0.2692299485206604\n",
            "Epoch: 16->Batch: 540 / 1875. Loss = 0.707896888256073\n",
            "Epoch: 16->Batch: 560 / 1875. Loss = 0.3104270100593567\n",
            "Epoch: 16->Batch: 580 / 1875. Loss = 0.14622381329536438\n",
            "Epoch: 16->Batch: 600 / 1875. Loss = 0.25073084235191345\n",
            "Epoch: 16->Batch: 620 / 1875. Loss = 0.11003130674362183\n",
            "Epoch: 16->Batch: 640 / 1875. Loss = 0.14971071481704712\n",
            "Epoch: 16->Batch: 660 / 1875. Loss = 0.17426933348178864\n",
            "Epoch: 16->Batch: 680 / 1875. Loss = 0.37954556941986084\n",
            "Epoch: 16->Batch: 700 / 1875. Loss = 0.16901308298110962\n",
            "Epoch: 16->Batch: 720 / 1875. Loss = 0.16722092032432556\n",
            "Epoch: 16->Batch: 740 / 1875. Loss = 0.39632469415664673\n",
            "Epoch: 16->Batch: 760 / 1875. Loss = 0.17974069714546204\n",
            "Epoch: 16->Batch: 780 / 1875. Loss = 0.5843707323074341\n",
            "Epoch: 16->Batch: 800 / 1875. Loss = 0.15899235010147095\n",
            "Epoch: 16->Batch: 820 / 1875. Loss = 0.261547714471817\n",
            "Epoch: 16->Batch: 840 / 1875. Loss = 0.31656113266944885\n",
            "Epoch: 16->Batch: 860 / 1875. Loss = 0.25219884514808655\n",
            "Epoch: 16->Batch: 880 / 1875. Loss = 0.35008493065834045\n",
            "Epoch: 16->Batch: 900 / 1875. Loss = 0.2665373682975769\n",
            "Epoch: 16->Batch: 920 / 1875. Loss = 0.26511168479919434\n",
            "Epoch: 16->Batch: 940 / 1875. Loss = 0.1941039264202118\n",
            "Epoch: 16->Batch: 960 / 1875. Loss = 0.17852109670639038\n",
            "Epoch: 16->Batch: 980 / 1875. Loss = 0.1995934396982193\n",
            "Epoch: 16->Batch: 1000 / 1875. Loss = 0.08995603024959564\n",
            "Epoch: 16->Batch: 1020 / 1875. Loss = 0.23458847403526306\n",
            "Epoch: 16->Batch: 1040 / 1875. Loss = 0.17339585721492767\n",
            "Epoch: 16->Batch: 1060 / 1875. Loss = 0.18558567762374878\n",
            "Epoch: 16->Batch: 1080 / 1875. Loss = 0.5011799931526184\n",
            "Epoch: 16->Batch: 1100 / 1875. Loss = 0.26497378945350647\n",
            "Epoch: 16->Batch: 1120 / 1875. Loss = 0.1972336322069168\n",
            "Epoch: 16->Batch: 1140 / 1875. Loss = 0.08769705891609192\n",
            "Epoch: 16->Batch: 1160 / 1875. Loss = 0.15800274908542633\n",
            "Epoch: 16->Batch: 1180 / 1875. Loss = 0.2896300256252289\n",
            "Epoch: 16->Batch: 1200 / 1875. Loss = 0.18908455967903137\n",
            "Epoch: 16->Batch: 1220 / 1875. Loss = 0.15637077391147614\n",
            "Epoch: 16->Batch: 1240 / 1875. Loss = 0.2791634202003479\n",
            "Epoch: 16->Batch: 1260 / 1875. Loss = 0.20814035832881927\n",
            "Epoch: 16->Batch: 1280 / 1875. Loss = 0.3295117914676666\n",
            "Epoch: 16->Batch: 1300 / 1875. Loss = 0.3271161913871765\n",
            "Epoch: 16->Batch: 1320 / 1875. Loss = 0.24414721131324768\n",
            "Epoch: 16->Batch: 1340 / 1875. Loss = 0.12254694104194641\n",
            "Epoch: 16->Batch: 1360 / 1875. Loss = 0.07057864964008331\n",
            "Epoch: 16->Batch: 1380 / 1875. Loss = 0.17457254230976105\n",
            "Epoch: 16->Batch: 1400 / 1875. Loss = 0.30782127380371094\n",
            "Epoch: 16->Batch: 1420 / 1875. Loss = 0.13456867635250092\n",
            "Epoch: 16->Batch: 1440 / 1875. Loss = 0.1797865927219391\n",
            "Epoch: 16->Batch: 1460 / 1875. Loss = 0.10987604409456253\n",
            "Epoch: 16->Batch: 1480 / 1875. Loss = 0.4871148467063904\n",
            "Epoch: 16->Batch: 1500 / 1875. Loss = 0.30189624428749084\n",
            "Epoch: 16->Batch: 1520 / 1875. Loss = 0.33559367060661316\n",
            "Epoch: 16->Batch: 1540 / 1875. Loss = 0.2508195638656616\n",
            "Epoch: 16->Batch: 1560 / 1875. Loss = 0.18284142017364502\n",
            "Epoch: 16->Batch: 1580 / 1875. Loss = 0.25579339265823364\n",
            "Epoch: 16->Batch: 1600 / 1875. Loss = 0.21882545948028564\n",
            "Epoch: 16->Batch: 1620 / 1875. Loss = 0.19126483798027039\n",
            "Epoch: 16->Batch: 1640 / 1875. Loss = 0.20733359456062317\n",
            "Epoch: 16->Batch: 1660 / 1875. Loss = 0.4535391926765442\n",
            "Epoch: 16->Batch: 1680 / 1875. Loss = 0.16793015599250793\n",
            "Epoch: 16->Batch: 1700 / 1875. Loss = 0.5018038749694824\n",
            "Epoch: 16->Batch: 1720 / 1875. Loss = 0.4888651669025421\n",
            "Epoch: 16->Batch: 1740 / 1875. Loss = 0.2899681329727173\n",
            "Epoch: 16->Batch: 1760 / 1875. Loss = 0.6177316308021545\n",
            "Epoch: 16->Batch: 1780 / 1875. Loss = 0.030944600701332092\n",
            "Epoch: 16->Batch: 1800 / 1875. Loss = 0.08994938433170319\n",
            "Epoch: 16->Batch: 1820 / 1875. Loss = 0.24741649627685547\n",
            "Epoch: 16->Batch: 1840 / 1875. Loss = 0.2277645319700241\n",
            "Epoch: 16->Batch: 1860 / 1875. Loss = 0.3159205913543701\n",
            "In Testing Function!\n",
            "Accuracy: 0.8971 (8971 / 10000)\n",
            "Epoch: 17 start ------------\n",
            "\n",
            "Epoch: 17->Batch: 0 / 1875. Loss = 0.24929960072040558\n",
            "Epoch: 17->Batch: 20 / 1875. Loss = 0.2254415899515152\n",
            "Epoch: 17->Batch: 40 / 1875. Loss = 0.11954672634601593\n",
            "Epoch: 17->Batch: 60 / 1875. Loss = 0.17969681322574615\n",
            "Epoch: 17->Batch: 80 / 1875. Loss = 0.35307085514068604\n",
            "Epoch: 17->Batch: 100 / 1875. Loss = 0.23160645365715027\n",
            "Epoch: 17->Batch: 120 / 1875. Loss = 0.26410233974456787\n",
            "Epoch: 17->Batch: 140 / 1875. Loss = 0.22314605116844177\n",
            "Epoch: 17->Batch: 160 / 1875. Loss = 0.16930828988552094\n",
            "Epoch: 17->Batch: 180 / 1875. Loss = 0.2979324758052826\n",
            "Epoch: 17->Batch: 200 / 1875. Loss = 0.36669760942459106\n",
            "Epoch: 17->Batch: 220 / 1875. Loss = 0.22964827716350555\n",
            "Epoch: 17->Batch: 240 / 1875. Loss = 0.1799759715795517\n",
            "Epoch: 17->Batch: 260 / 1875. Loss = 0.26677799224853516\n",
            "Epoch: 17->Batch: 280 / 1875. Loss = 0.16373856365680695\n",
            "Epoch: 17->Batch: 300 / 1875. Loss = 0.35808220505714417\n",
            "Epoch: 17->Batch: 320 / 1875. Loss = 0.06801056861877441\n",
            "Epoch: 17->Batch: 340 / 1875. Loss = 0.11205051839351654\n",
            "Epoch: 17->Batch: 360 / 1875. Loss = 0.10181660950183868\n",
            "Epoch: 17->Batch: 380 / 1875. Loss = 0.2731184661388397\n",
            "Epoch: 17->Batch: 400 / 1875. Loss = 0.13283532857894897\n",
            "Epoch: 17->Batch: 420 / 1875. Loss = 0.2506362497806549\n",
            "Epoch: 17->Batch: 440 / 1875. Loss = 0.08187165856361389\n",
            "Epoch: 17->Batch: 460 / 1875. Loss = 0.3575878441333771\n",
            "Epoch: 17->Batch: 480 / 1875. Loss = 0.14652778208255768\n",
            "Epoch: 17->Batch: 500 / 1875. Loss = 0.2615535855293274\n",
            "Epoch: 17->Batch: 520 / 1875. Loss = 0.16212743520736694\n",
            "Epoch: 17->Batch: 540 / 1875. Loss = 0.40169093012809753\n",
            "Epoch: 17->Batch: 560 / 1875. Loss = 0.2518373727798462\n",
            "Epoch: 17->Batch: 580 / 1875. Loss = 0.2010735422372818\n",
            "Epoch: 17->Batch: 600 / 1875. Loss = 0.24617458879947662\n",
            "Epoch: 17->Batch: 620 / 1875. Loss = 0.27964186668395996\n",
            "Epoch: 17->Batch: 640 / 1875. Loss = 0.05705833435058594\n",
            "Epoch: 17->Batch: 660 / 1875. Loss = 0.15009146928787231\n",
            "Epoch: 17->Batch: 680 / 1875. Loss = 0.4209834635257721\n",
            "Epoch: 17->Batch: 700 / 1875. Loss = 0.16070382297039032\n",
            "Epoch: 17->Batch: 720 / 1875. Loss = 0.1488625556230545\n",
            "Epoch: 17->Batch: 740 / 1875. Loss = 0.40759071707725525\n",
            "Epoch: 17->Batch: 760 / 1875. Loss = 0.3444588780403137\n",
            "Epoch: 17->Batch: 780 / 1875. Loss = 0.1473558098077774\n",
            "Epoch: 17->Batch: 800 / 1875. Loss = 0.15464654564857483\n",
            "Epoch: 17->Batch: 820 / 1875. Loss = 0.12307459115982056\n",
            "Epoch: 17->Batch: 840 / 1875. Loss = 0.1734321266412735\n",
            "Epoch: 17->Batch: 860 / 1875. Loss = 0.22382979094982147\n",
            "Epoch: 17->Batch: 880 / 1875. Loss = 0.24209873378276825\n",
            "Epoch: 17->Batch: 900 / 1875. Loss = 0.2000706046819687\n",
            "Epoch: 17->Batch: 920 / 1875. Loss = 0.17343269288539886\n",
            "Epoch: 17->Batch: 940 / 1875. Loss = 0.14615942537784576\n",
            "Epoch: 17->Batch: 960 / 1875. Loss = 0.22853679955005646\n",
            "Epoch: 17->Batch: 980 / 1875. Loss = 0.16544859111309052\n",
            "Epoch: 17->Batch: 1000 / 1875. Loss = 0.1976952999830246\n",
            "Epoch: 17->Batch: 1020 / 1875. Loss = 0.10716234892606735\n",
            "Epoch: 17->Batch: 1040 / 1875. Loss = 0.3476267457008362\n",
            "Epoch: 17->Batch: 1060 / 1875. Loss = 0.33729326725006104\n",
            "Epoch: 17->Batch: 1080 / 1875. Loss = 0.12704403698444366\n",
            "Epoch: 17->Batch: 1100 / 1875. Loss = 0.3551412522792816\n",
            "Epoch: 17->Batch: 1120 / 1875. Loss = 0.23185312747955322\n",
            "Epoch: 17->Batch: 1140 / 1875. Loss = 0.24867428839206696\n",
            "Epoch: 17->Batch: 1160 / 1875. Loss = 0.2876477539539337\n",
            "Epoch: 17->Batch: 1180 / 1875. Loss = 0.34395867586135864\n",
            "Epoch: 17->Batch: 1200 / 1875. Loss = 0.14652661979198456\n",
            "Epoch: 17->Batch: 1220 / 1875. Loss = 0.0968644767999649\n",
            "Epoch: 17->Batch: 1240 / 1875. Loss = 0.2035767138004303\n",
            "Epoch: 17->Batch: 1260 / 1875. Loss = 0.22785937786102295\n",
            "Epoch: 17->Batch: 1280 / 1875. Loss = 0.1081189513206482\n",
            "Epoch: 17->Batch: 1300 / 1875. Loss = 0.25963959097862244\n",
            "Epoch: 17->Batch: 1320 / 1875. Loss = 0.24332600831985474\n",
            "Epoch: 17->Batch: 1340 / 1875. Loss = 0.13823923468589783\n",
            "Epoch: 17->Batch: 1360 / 1875. Loss = 0.18821188807487488\n",
            "Epoch: 17->Batch: 1380 / 1875. Loss = 0.15750500559806824\n",
            "Epoch: 17->Batch: 1400 / 1875. Loss = 0.3706085979938507\n",
            "Epoch: 17->Batch: 1420 / 1875. Loss = 0.17259980738162994\n",
            "Epoch: 17->Batch: 1440 / 1875. Loss = 0.21316057443618774\n",
            "Epoch: 17->Batch: 1460 / 1875. Loss = 0.2177346795797348\n",
            "Epoch: 17->Batch: 1480 / 1875. Loss = 0.3080824613571167\n",
            "Epoch: 17->Batch: 1500 / 1875. Loss = 0.0898265391588211\n",
            "Epoch: 17->Batch: 1520 / 1875. Loss = 0.21206602454185486\n",
            "Epoch: 17->Batch: 1540 / 1875. Loss = 0.21873705089092255\n",
            "Epoch: 17->Batch: 1560 / 1875. Loss = 0.18712972104549408\n",
            "Epoch: 17->Batch: 1580 / 1875. Loss = 0.28325238823890686\n",
            "Epoch: 17->Batch: 1600 / 1875. Loss = 0.21565628051757812\n",
            "Epoch: 17->Batch: 1620 / 1875. Loss = 0.07604562491178513\n",
            "Epoch: 17->Batch: 1640 / 1875. Loss = 0.1779496967792511\n",
            "Epoch: 17->Batch: 1660 / 1875. Loss = 0.2014298290014267\n",
            "Epoch: 17->Batch: 1680 / 1875. Loss = 0.21769782900810242\n",
            "Epoch: 17->Batch: 1700 / 1875. Loss = 0.1139380931854248\n",
            "Epoch: 17->Batch: 1720 / 1875. Loss = 0.2321920394897461\n",
            "Epoch: 17->Batch: 1740 / 1875. Loss = 0.19162920117378235\n",
            "Epoch: 17->Batch: 1760 / 1875. Loss = 0.29337334632873535\n",
            "Epoch: 17->Batch: 1780 / 1875. Loss = 0.16388030350208282\n",
            "Epoch: 17->Batch: 1800 / 1875. Loss = 0.21676324307918549\n",
            "Epoch: 17->Batch: 1820 / 1875. Loss = 0.20112961530685425\n",
            "Epoch: 17->Batch: 1840 / 1875. Loss = 0.26132071018218994\n",
            "Epoch: 17->Batch: 1860 / 1875. Loss = 0.15954139828681946\n",
            "In Testing Function!\n",
            "Accuracy: 0.8993 (8993 / 10000)\n",
            "Epoch: 18 start ------------\n",
            "\n",
            "Epoch: 18->Batch: 0 / 1875. Loss = 0.19425715506076813\n",
            "Epoch: 18->Batch: 20 / 1875. Loss = 0.20428813993930817\n",
            "Epoch: 18->Batch: 40 / 1875. Loss = 0.23507803678512573\n",
            "Epoch: 18->Batch: 60 / 1875. Loss = 0.20281092822551727\n",
            "Epoch: 18->Batch: 80 / 1875. Loss = 0.18108677864074707\n",
            "Epoch: 18->Batch: 100 / 1875. Loss = 0.439340740442276\n",
            "Epoch: 18->Batch: 120 / 1875. Loss = 0.19344490766525269\n",
            "Epoch: 18->Batch: 140 / 1875. Loss = 0.26998263597488403\n",
            "Epoch: 18->Batch: 160 / 1875. Loss = 0.16214947402477264\n",
            "Epoch: 18->Batch: 180 / 1875. Loss = 0.09827925264835358\n",
            "Epoch: 18->Batch: 200 / 1875. Loss = 0.19003286957740784\n",
            "Epoch: 18->Batch: 220 / 1875. Loss = 0.12985612452030182\n",
            "Epoch: 18->Batch: 240 / 1875. Loss = 0.27948522567749023\n",
            "Epoch: 18->Batch: 260 / 1875. Loss = 0.20211006700992584\n",
            "Epoch: 18->Batch: 280 / 1875. Loss = 0.2789178192615509\n",
            "Epoch: 18->Batch: 300 / 1875. Loss = 0.14743365347385406\n",
            "Epoch: 18->Batch: 320 / 1875. Loss = 0.16558107733726501\n",
            "Epoch: 18->Batch: 340 / 1875. Loss = 0.13413366675376892\n",
            "Epoch: 18->Batch: 360 / 1875. Loss = 0.10518980026245117\n",
            "Epoch: 18->Batch: 380 / 1875. Loss = 0.2636517882347107\n",
            "Epoch: 18->Batch: 400 / 1875. Loss = 0.17800156772136688\n",
            "Epoch: 18->Batch: 420 / 1875. Loss = 0.18245947360992432\n",
            "Epoch: 18->Batch: 440 / 1875. Loss = 0.2304733395576477\n",
            "Epoch: 18->Batch: 460 / 1875. Loss = 0.17225483059883118\n",
            "Epoch: 18->Batch: 480 / 1875. Loss = 0.08871915936470032\n",
            "Epoch: 18->Batch: 500 / 1875. Loss = 0.2218770533800125\n",
            "Epoch: 18->Batch: 520 / 1875. Loss = 0.12665879726409912\n",
            "Epoch: 18->Batch: 540 / 1875. Loss = 0.403726726770401\n",
            "Epoch: 18->Batch: 560 / 1875. Loss = 0.11871400475502014\n",
            "Epoch: 18->Batch: 580 / 1875. Loss = 0.29474014043807983\n",
            "Epoch: 18->Batch: 600 / 1875. Loss = 0.16360318660736084\n",
            "Epoch: 18->Batch: 620 / 1875. Loss = 0.32962357997894287\n",
            "Epoch: 18->Batch: 640 / 1875. Loss = 0.10273368656635284\n",
            "Epoch: 18->Batch: 660 / 1875. Loss = 0.17430570721626282\n",
            "Epoch: 18->Batch: 680 / 1875. Loss = 0.16216424107551575\n",
            "Epoch: 18->Batch: 700 / 1875. Loss = 0.32961079478263855\n",
            "Epoch: 18->Batch: 720 / 1875. Loss = 0.2704903781414032\n",
            "Epoch: 18->Batch: 740 / 1875. Loss = 0.2682931423187256\n",
            "Epoch: 18->Batch: 760 / 1875. Loss = 0.1643000841140747\n",
            "Epoch: 18->Batch: 780 / 1875. Loss = 0.14335918426513672\n",
            "Epoch: 18->Batch: 800 / 1875. Loss = 0.17353369295597076\n",
            "Epoch: 18->Batch: 820 / 1875. Loss = 0.2818376123905182\n",
            "Epoch: 18->Batch: 840 / 1875. Loss = 0.17167341709136963\n",
            "Epoch: 18->Batch: 860 / 1875. Loss = 0.24669724702835083\n",
            "Epoch: 18->Batch: 880 / 1875. Loss = 0.13410209119319916\n",
            "Epoch: 18->Batch: 900 / 1875. Loss = 0.1416378617286682\n",
            "Epoch: 18->Batch: 920 / 1875. Loss = 0.37557512521743774\n",
            "Epoch: 18->Batch: 940 / 1875. Loss = 0.2598750591278076\n",
            "Epoch: 18->Batch: 960 / 1875. Loss = 0.2519373595714569\n",
            "Epoch: 18->Batch: 980 / 1875. Loss = 0.18716846406459808\n",
            "Epoch: 18->Batch: 1000 / 1875. Loss = 0.11072869598865509\n",
            "Epoch: 18->Batch: 1020 / 1875. Loss = 0.3082091808319092\n",
            "Epoch: 18->Batch: 1040 / 1875. Loss = 0.2670956552028656\n",
            "Epoch: 18->Batch: 1060 / 1875. Loss = 0.19989405572414398\n",
            "Epoch: 18->Batch: 1080 / 1875. Loss = 0.3032931387424469\n",
            "Epoch: 18->Batch: 1100 / 1875. Loss = 0.21249175071716309\n",
            "Epoch: 18->Batch: 1120 / 1875. Loss = 0.11654067784547806\n",
            "Epoch: 18->Batch: 1140 / 1875. Loss = 0.21147316694259644\n",
            "Epoch: 18->Batch: 1160 / 1875. Loss = 0.4114101231098175\n",
            "Epoch: 18->Batch: 1180 / 1875. Loss = 0.2352466583251953\n",
            "Epoch: 18->Batch: 1200 / 1875. Loss = 0.28730982542037964\n",
            "Epoch: 18->Batch: 1220 / 1875. Loss = 0.1677827388048172\n",
            "Epoch: 18->Batch: 1240 / 1875. Loss = 0.2621113657951355\n",
            "Epoch: 18->Batch: 1260 / 1875. Loss = 0.36026060581207275\n",
            "Epoch: 18->Batch: 1280 / 1875. Loss = 0.35964107513427734\n",
            "Epoch: 18->Batch: 1300 / 1875. Loss = 0.3161235451698303\n",
            "Epoch: 18->Batch: 1320 / 1875. Loss = 0.117396280169487\n",
            "Epoch: 18->Batch: 1340 / 1875. Loss = 0.23516221344470978\n",
            "Epoch: 18->Batch: 1360 / 1875. Loss = 0.1650885045528412\n",
            "Epoch: 18->Batch: 1380 / 1875. Loss = 0.1785164475440979\n",
            "Epoch: 18->Batch: 1400 / 1875. Loss = 0.22351761162281036\n",
            "Epoch: 18->Batch: 1420 / 1875. Loss = 0.10700166970491409\n",
            "Epoch: 18->Batch: 1440 / 1875. Loss = 0.13007089495658875\n",
            "Epoch: 18->Batch: 1460 / 1875. Loss = 0.17185045778751373\n",
            "Epoch: 18->Batch: 1480 / 1875. Loss = 0.298056423664093\n",
            "Epoch: 18->Batch: 1500 / 1875. Loss = 0.21588465571403503\n",
            "Epoch: 18->Batch: 1520 / 1875. Loss = 0.13962125778198242\n",
            "Epoch: 18->Batch: 1540 / 1875. Loss = 0.204452246427536\n",
            "Epoch: 18->Batch: 1560 / 1875. Loss = 0.19455838203430176\n",
            "Epoch: 18->Batch: 1580 / 1875. Loss = 0.05215418338775635\n",
            "Epoch: 18->Batch: 1600 / 1875. Loss = 0.16665810346603394\n",
            "Epoch: 18->Batch: 1620 / 1875. Loss = 0.19921649992465973\n",
            "Epoch: 18->Batch: 1640 / 1875. Loss = 0.1972772181034088\n",
            "Epoch: 18->Batch: 1660 / 1875. Loss = 0.1530134081840515\n",
            "Epoch: 18->Batch: 1680 / 1875. Loss = 0.22559064626693726\n",
            "Epoch: 18->Batch: 1700 / 1875. Loss = 0.17896415293216705\n",
            "Epoch: 18->Batch: 1720 / 1875. Loss = 0.25129660964012146\n",
            "Epoch: 18->Batch: 1740 / 1875. Loss = 0.2928175628185272\n",
            "Epoch: 18->Batch: 1760 / 1875. Loss = 0.1971784085035324\n",
            "Epoch: 18->Batch: 1780 / 1875. Loss = 0.2735080420970917\n",
            "Epoch: 18->Batch: 1800 / 1875. Loss = 0.2729518413543701\n",
            "Epoch: 18->Batch: 1820 / 1875. Loss = 0.47809064388275146\n",
            "Epoch: 18->Batch: 1840 / 1875. Loss = 0.2894396185874939\n",
            "Epoch: 18->Batch: 1860 / 1875. Loss = 0.43423593044281006\n",
            "In Testing Function!\n",
            "Accuracy: 0.898 (8980 / 10000)\n",
            "Epoch: 19 start ------------\n",
            "\n",
            "Epoch: 19->Batch: 0 / 1875. Loss = 0.20394907891750336\n",
            "Epoch: 19->Batch: 20 / 1875. Loss = 0.2238842397928238\n",
            "Epoch: 19->Batch: 40 / 1875. Loss = 0.29132047295570374\n",
            "Epoch: 19->Batch: 60 / 1875. Loss = 0.2429812252521515\n",
            "Epoch: 19->Batch: 80 / 1875. Loss = 0.3111492097377777\n",
            "Epoch: 19->Batch: 100 / 1875. Loss = 0.3786298930644989\n",
            "Epoch: 19->Batch: 120 / 1875. Loss = 0.37633904814720154\n",
            "Epoch: 19->Batch: 140 / 1875. Loss = 0.11655308306217194\n",
            "Epoch: 19->Batch: 160 / 1875. Loss = 0.29730868339538574\n",
            "Epoch: 19->Batch: 180 / 1875. Loss = 0.30885645747184753\n",
            "Epoch: 19->Batch: 200 / 1875. Loss = 0.2633070945739746\n",
            "Epoch: 19->Batch: 220 / 1875. Loss = 0.3348205089569092\n",
            "Epoch: 19->Batch: 240 / 1875. Loss = 0.2581797242164612\n",
            "Epoch: 19->Batch: 260 / 1875. Loss = 0.23159544169902802\n",
            "Epoch: 19->Batch: 280 / 1875. Loss = 0.2171715497970581\n",
            "Epoch: 19->Batch: 300 / 1875. Loss = 0.18031787872314453\n",
            "Epoch: 19->Batch: 320 / 1875. Loss = 0.0911920815706253\n",
            "Epoch: 19->Batch: 340 / 1875. Loss = 0.2278963178396225\n",
            "Epoch: 19->Batch: 360 / 1875. Loss = 0.09776614606380463\n",
            "Epoch: 19->Batch: 380 / 1875. Loss = 0.16934575140476227\n",
            "Epoch: 19->Batch: 400 / 1875. Loss = 0.11430328339338303\n",
            "Epoch: 19->Batch: 420 / 1875. Loss = 0.06547039747238159\n",
            "Epoch: 19->Batch: 440 / 1875. Loss = 0.1874052882194519\n",
            "Epoch: 19->Batch: 460 / 1875. Loss = 0.2398255169391632\n",
            "Epoch: 19->Batch: 480 / 1875. Loss = 0.35547688603401184\n",
            "Epoch: 19->Batch: 500 / 1875. Loss = 0.19583283364772797\n",
            "Epoch: 19->Batch: 520 / 1875. Loss = 0.2502252459526062\n",
            "Epoch: 19->Batch: 540 / 1875. Loss = 0.20976410806179047\n",
            "Epoch: 19->Batch: 560 / 1875. Loss = 0.2915711998939514\n",
            "Epoch: 19->Batch: 580 / 1875. Loss = 0.2844545245170593\n",
            "Epoch: 19->Batch: 600 / 1875. Loss = 0.4032314419746399\n",
            "Epoch: 19->Batch: 620 / 1875. Loss = 0.3518967628479004\n",
            "Epoch: 19->Batch: 640 / 1875. Loss = 0.2483660876750946\n",
            "Epoch: 19->Batch: 660 / 1875. Loss = 0.04971891641616821\n",
            "Epoch: 19->Batch: 680 / 1875. Loss = 0.21010667085647583\n",
            "Epoch: 19->Batch: 700 / 1875. Loss = 0.2721109390258789\n",
            "Epoch: 19->Batch: 720 / 1875. Loss = 0.1814679354429245\n",
            "Epoch: 19->Batch: 740 / 1875. Loss = 0.18510203063488007\n",
            "Epoch: 19->Batch: 760 / 1875. Loss = 0.1960616558790207\n",
            "Epoch: 19->Batch: 780 / 1875. Loss = 0.3404294550418854\n",
            "Epoch: 19->Batch: 800 / 1875. Loss = 0.2134813666343689\n",
            "Epoch: 19->Batch: 820 / 1875. Loss = 0.1754942685365677\n",
            "Epoch: 19->Batch: 840 / 1875. Loss = 0.1853760927915573\n",
            "Epoch: 19->Batch: 860 / 1875. Loss = 0.23183773458003998\n",
            "Epoch: 19->Batch: 880 / 1875. Loss = 0.1320037543773651\n",
            "Epoch: 19->Batch: 900 / 1875. Loss = 0.08581119775772095\n",
            "Epoch: 19->Batch: 920 / 1875. Loss = 0.21951891481876373\n",
            "Epoch: 19->Batch: 940 / 1875. Loss = 0.33607017993927\n",
            "Epoch: 19->Batch: 960 / 1875. Loss = 0.20111024379730225\n",
            "Epoch: 19->Batch: 980 / 1875. Loss = 0.08853576332330704\n",
            "Epoch: 19->Batch: 1000 / 1875. Loss = 0.22648005187511444\n",
            "Epoch: 19->Batch: 1020 / 1875. Loss = 0.3258070647716522\n",
            "Epoch: 19->Batch: 1040 / 1875. Loss = 0.10057225823402405\n",
            "Epoch: 19->Batch: 1060 / 1875. Loss = 0.08643096685409546\n",
            "Epoch: 19->Batch: 1080 / 1875. Loss = 0.19275254011154175\n",
            "Epoch: 19->Batch: 1100 / 1875. Loss = 0.25261932611465454\n",
            "Epoch: 19->Batch: 1120 / 1875. Loss = 0.11357979476451874\n",
            "Epoch: 19->Batch: 1140 / 1875. Loss = 0.08413490653038025\n",
            "Epoch: 19->Batch: 1160 / 1875. Loss = 0.22578683495521545\n",
            "Epoch: 19->Batch: 1180 / 1875. Loss = 0.2924514412879944\n",
            "Epoch: 19->Batch: 1200 / 1875. Loss = 0.23852349817752838\n",
            "Epoch: 19->Batch: 1220 / 1875. Loss = 0.18641126155853271\n",
            "Epoch: 19->Batch: 1240 / 1875. Loss = 0.1763189136981964\n",
            "Epoch: 19->Batch: 1260 / 1875. Loss = 0.16930356621742249\n",
            "Epoch: 19->Batch: 1280 / 1875. Loss = 0.2880796790122986\n",
            "Epoch: 19->Batch: 1300 / 1875. Loss = 0.28417491912841797\n",
            "Epoch: 19->Batch: 1320 / 1875. Loss = 0.23471441864967346\n",
            "Epoch: 19->Batch: 1340 / 1875. Loss = 0.15288442373275757\n",
            "Epoch: 19->Batch: 1360 / 1875. Loss = 0.26127690076828003\n",
            "Epoch: 19->Batch: 1380 / 1875. Loss = 0.263496994972229\n",
            "Epoch: 19->Batch: 1400 / 1875. Loss = 0.0801815539598465\n",
            "Epoch: 19->Batch: 1420 / 1875. Loss = 0.1337953358888626\n",
            "Epoch: 19->Batch: 1440 / 1875. Loss = 0.2134997397661209\n",
            "Epoch: 19->Batch: 1460 / 1875. Loss = 0.3170680105686188\n",
            "Epoch: 19->Batch: 1480 / 1875. Loss = 0.243294820189476\n",
            "Epoch: 19->Batch: 1500 / 1875. Loss = 0.5650420188903809\n",
            "Epoch: 19->Batch: 1520 / 1875. Loss = 0.2382357269525528\n",
            "Epoch: 19->Batch: 1540 / 1875. Loss = 0.21088247001171112\n",
            "Epoch: 19->Batch: 1560 / 1875. Loss = 0.24748574197292328\n",
            "Epoch: 19->Batch: 1580 / 1875. Loss = 0.3093053698539734\n",
            "Epoch: 19->Batch: 1600 / 1875. Loss = 0.24975630640983582\n",
            "Epoch: 19->Batch: 1620 / 1875. Loss = 0.13387076556682587\n",
            "Epoch: 19->Batch: 1640 / 1875. Loss = 0.26162558794021606\n",
            "Epoch: 19->Batch: 1660 / 1875. Loss = 0.430733323097229\n",
            "Epoch: 19->Batch: 1680 / 1875. Loss = 0.2152445763349533\n",
            "Epoch: 19->Batch: 1700 / 1875. Loss = 0.1746239960193634\n",
            "Epoch: 19->Batch: 1720 / 1875. Loss = 0.0927167758345604\n",
            "Epoch: 19->Batch: 1740 / 1875. Loss = 0.3338179290294647\n",
            "Epoch: 19->Batch: 1760 / 1875. Loss = 0.24257756769657135\n",
            "Epoch: 19->Batch: 1780 / 1875. Loss = 0.604184091091156\n",
            "Epoch: 19->Batch: 1800 / 1875. Loss = 0.1621343344449997\n",
            "Epoch: 19->Batch: 1820 / 1875. Loss = 0.40385472774505615\n",
            "Epoch: 19->Batch: 1840 / 1875. Loss = 0.16113197803497314\n",
            "Epoch: 19->Batch: 1860 / 1875. Loss = 0.37813320755958557\n",
            "In Testing Function!\n",
            "Accuracy: 0.8969 (8969 / 10000)\n",
            "Epoch: 20 start ------------\n",
            "\n",
            "Epoch: 20->Batch: 0 / 1875. Loss = 0.2592907249927521\n",
            "Epoch: 20->Batch: 20 / 1875. Loss = 0.39395850896835327\n",
            "Epoch: 20->Batch: 40 / 1875. Loss = 0.22774842381477356\n",
            "Epoch: 20->Batch: 60 / 1875. Loss = 0.15235108137130737\n",
            "Epoch: 20->Batch: 80 / 1875. Loss = 0.13507920503616333\n",
            "Epoch: 20->Batch: 100 / 1875. Loss = 0.16566620767116547\n",
            "Epoch: 20->Batch: 120 / 1875. Loss = 0.19633226096630096\n",
            "Epoch: 20->Batch: 140 / 1875. Loss = 0.05006708204746246\n",
            "Epoch: 20->Batch: 160 / 1875. Loss = 0.3138180375099182\n",
            "Epoch: 20->Batch: 180 / 1875. Loss = 0.26564252376556396\n",
            "Epoch: 20->Batch: 200 / 1875. Loss = 0.11710406839847565\n",
            "Epoch: 20->Batch: 220 / 1875. Loss = 0.07992839813232422\n",
            "Epoch: 20->Batch: 240 / 1875. Loss = 0.31416773796081543\n",
            "Epoch: 20->Batch: 260 / 1875. Loss = 0.14347131550312042\n",
            "Epoch: 20->Batch: 280 / 1875. Loss = 0.27544379234313965\n",
            "Epoch: 20->Batch: 300 / 1875. Loss = 0.2868417501449585\n",
            "Epoch: 20->Batch: 320 / 1875. Loss = 0.29934462904930115\n",
            "Epoch: 20->Batch: 340 / 1875. Loss = 0.22839686274528503\n",
            "Epoch: 20->Batch: 360 / 1875. Loss = 0.1024317741394043\n",
            "Epoch: 20->Batch: 380 / 1875. Loss = 0.3113144636154175\n",
            "Epoch: 20->Batch: 400 / 1875. Loss = 0.19721855223178864\n",
            "Epoch: 20->Batch: 420 / 1875. Loss = 0.17900393903255463\n",
            "Epoch: 20->Batch: 440 / 1875. Loss = 0.07166309654712677\n",
            "Epoch: 20->Batch: 460 / 1875. Loss = 0.30414122343063354\n",
            "Epoch: 20->Batch: 480 / 1875. Loss = 0.2533063292503357\n",
            "Epoch: 20->Batch: 500 / 1875. Loss = 0.3451557755470276\n",
            "Epoch: 20->Batch: 520 / 1875. Loss = 0.14171721041202545\n",
            "Epoch: 20->Batch: 540 / 1875. Loss = 0.32706889510154724\n",
            "Epoch: 20->Batch: 560 / 1875. Loss = 0.2457495778799057\n",
            "Epoch: 20->Batch: 580 / 1875. Loss = 0.18856172263622284\n",
            "Epoch: 20->Batch: 600 / 1875. Loss = 0.1876051276922226\n",
            "Epoch: 20->Batch: 620 / 1875. Loss = 0.0885828286409378\n",
            "Epoch: 20->Batch: 640 / 1875. Loss = 0.21637724339962006\n",
            "Epoch: 20->Batch: 660 / 1875. Loss = 0.10825122892856598\n",
            "Epoch: 20->Batch: 680 / 1875. Loss = 0.2705712914466858\n",
            "Epoch: 20->Batch: 700 / 1875. Loss = 0.18494606018066406\n",
            "Epoch: 20->Batch: 720 / 1875. Loss = 0.1441982388496399\n",
            "Epoch: 20->Batch: 740 / 1875. Loss = 0.2497521936893463\n",
            "Epoch: 20->Batch: 760 / 1875. Loss = 0.17676205933094025\n",
            "Epoch: 20->Batch: 780 / 1875. Loss = 0.13492938876152039\n",
            "Epoch: 20->Batch: 800 / 1875. Loss = 0.07126636803150177\n",
            "Epoch: 20->Batch: 820 / 1875. Loss = 0.3656432032585144\n",
            "Epoch: 20->Batch: 840 / 1875. Loss = 0.11221610009670258\n",
            "Epoch: 20->Batch: 860 / 1875. Loss = 0.15844202041625977\n",
            "Epoch: 20->Batch: 880 / 1875. Loss = 0.15787817537784576\n",
            "Epoch: 20->Batch: 900 / 1875. Loss = 0.14956879615783691\n",
            "Epoch: 20->Batch: 920 / 1875. Loss = 0.12946464121341705\n",
            "Epoch: 20->Batch: 940 / 1875. Loss = 0.15207748115062714\n",
            "Epoch: 20->Batch: 960 / 1875. Loss = 0.17837634682655334\n",
            "Epoch: 20->Batch: 980 / 1875. Loss = 0.22535324096679688\n",
            "Epoch: 20->Batch: 1000 / 1875. Loss = 0.11114056408405304\n",
            "Epoch: 20->Batch: 1020 / 1875. Loss = 0.20894776284694672\n",
            "Epoch: 20->Batch: 1040 / 1875. Loss = 0.2322540432214737\n",
            "Epoch: 20->Batch: 1060 / 1875. Loss = 0.08144735544919968\n",
            "Epoch: 20->Batch: 1080 / 1875. Loss = 0.29372337460517883\n",
            "Epoch: 20->Batch: 1100 / 1875. Loss = 0.1247163712978363\n",
            "Epoch: 20->Batch: 1120 / 1875. Loss = 0.07887235283851624\n",
            "Epoch: 20->Batch: 1140 / 1875. Loss = 0.3053131699562073\n",
            "Epoch: 20->Batch: 1160 / 1875. Loss = 0.3156583309173584\n",
            "Epoch: 20->Batch: 1180 / 1875. Loss = 0.16541483998298645\n",
            "Epoch: 20->Batch: 1200 / 1875. Loss = 0.2536981403827667\n",
            "Epoch: 20->Batch: 1220 / 1875. Loss = 0.14090536534786224\n",
            "Epoch: 20->Batch: 1240 / 1875. Loss = 0.26941317319869995\n",
            "Epoch: 20->Batch: 1260 / 1875. Loss = 0.10867875814437866\n",
            "Epoch: 20->Batch: 1280 / 1875. Loss = 0.2830953896045685\n",
            "Epoch: 20->Batch: 1300 / 1875. Loss = 0.3342173099517822\n",
            "Epoch: 20->Batch: 1320 / 1875. Loss = 0.24128910899162292\n",
            "Epoch: 20->Batch: 1340 / 1875. Loss = 0.11520364880561829\n",
            "Epoch: 20->Batch: 1360 / 1875. Loss = 0.15796718001365662\n",
            "Epoch: 20->Batch: 1380 / 1875. Loss = 0.12925703823566437\n",
            "Epoch: 20->Batch: 1400 / 1875. Loss = 0.2595006227493286\n",
            "Epoch: 20->Batch: 1420 / 1875. Loss = 0.07674774527549744\n",
            "Epoch: 20->Batch: 1440 / 1875. Loss = 0.1824238896369934\n",
            "Epoch: 20->Batch: 1460 / 1875. Loss = 0.3330097198486328\n",
            "Epoch: 20->Batch: 1480 / 1875. Loss = 0.13932469487190247\n",
            "Epoch: 20->Batch: 1500 / 1875. Loss = 0.3403207063674927\n",
            "Epoch: 20->Batch: 1520 / 1875. Loss = 0.09965907037258148\n",
            "Epoch: 20->Batch: 1540 / 1875. Loss = 0.24346934258937836\n",
            "Epoch: 20->Batch: 1560 / 1875. Loss = 0.25747108459472656\n",
            "Epoch: 20->Batch: 1580 / 1875. Loss = 0.24769055843353271\n",
            "Epoch: 20->Batch: 1600 / 1875. Loss = 0.22732393443584442\n",
            "Epoch: 20->Batch: 1620 / 1875. Loss = 0.3524301052093506\n",
            "Epoch: 20->Batch: 1640 / 1875. Loss = 0.13030320405960083\n",
            "Epoch: 20->Batch: 1660 / 1875. Loss = 0.18701839447021484\n",
            "Epoch: 20->Batch: 1680 / 1875. Loss = 0.20901472866535187\n",
            "Epoch: 20->Batch: 1700 / 1875. Loss = 0.11343739926815033\n",
            "Epoch: 20->Batch: 1720 / 1875. Loss = 0.2768808901309967\n",
            "Epoch: 20->Batch: 1740 / 1875. Loss = 0.19644160568714142\n",
            "Epoch: 20->Batch: 1760 / 1875. Loss = 0.33925706148147583\n",
            "Epoch: 20->Batch: 1780 / 1875. Loss = 0.15279346704483032\n",
            "Epoch: 20->Batch: 1800 / 1875. Loss = 0.2563678026199341\n",
            "Epoch: 20->Batch: 1820 / 1875. Loss = 0.16620655357837677\n",
            "Epoch: 20->Batch: 1840 / 1875. Loss = 0.27508214116096497\n",
            "Epoch: 20->Batch: 1860 / 1875. Loss = 0.1962621957063675\n",
            "In Testing Function!\n",
            "Accuracy: 0.9014 (9014 / 10000)\n",
            "Epoch: 21 start ------------\n",
            "\n",
            "Epoch: 21->Batch: 0 / 1875. Loss = 0.13518306612968445\n",
            "Epoch: 21->Batch: 20 / 1875. Loss = 0.2184692919254303\n",
            "Epoch: 21->Batch: 40 / 1875. Loss = 0.11278292536735535\n",
            "Epoch: 21->Batch: 60 / 1875. Loss = 0.12731458246707916\n",
            "Epoch: 21->Batch: 80 / 1875. Loss = 0.29568177461624146\n",
            "Epoch: 21->Batch: 100 / 1875. Loss = 0.27038222551345825\n",
            "Epoch: 21->Batch: 120 / 1875. Loss = 0.20835065841674805\n",
            "Epoch: 21->Batch: 140 / 1875. Loss = 0.18326230347156525\n",
            "Epoch: 21->Batch: 160 / 1875. Loss = 0.16598935425281525\n",
            "Epoch: 21->Batch: 180 / 1875. Loss = 0.2900380492210388\n",
            "Epoch: 21->Batch: 200 / 1875. Loss = 0.18550898134708405\n",
            "Epoch: 21->Batch: 220 / 1875. Loss = 0.15108023583889008\n",
            "Epoch: 21->Batch: 240 / 1875. Loss = 0.21007883548736572\n",
            "Epoch: 21->Batch: 260 / 1875. Loss = 0.19804561138153076\n",
            "Epoch: 21->Batch: 280 / 1875. Loss = 0.10212633013725281\n",
            "Epoch: 21->Batch: 300 / 1875. Loss = 0.1525014489889145\n",
            "Epoch: 21->Batch: 320 / 1875. Loss = 0.1334945261478424\n",
            "Epoch: 21->Batch: 340 / 1875. Loss = 0.14571250975131989\n",
            "Epoch: 21->Batch: 360 / 1875. Loss = 0.5416737794876099\n",
            "Epoch: 21->Batch: 380 / 1875. Loss = 0.09183584153652191\n",
            "Epoch: 21->Batch: 400 / 1875. Loss = 0.2220401018857956\n",
            "Epoch: 21->Batch: 420 / 1875. Loss = 0.29603898525238037\n",
            "Epoch: 21->Batch: 440 / 1875. Loss = 0.2643410563468933\n",
            "Epoch: 21->Batch: 460 / 1875. Loss = 0.15528032183647156\n",
            "Epoch: 21->Batch: 480 / 1875. Loss = 0.16815407574176788\n",
            "Epoch: 21->Batch: 500 / 1875. Loss = 0.2739229202270508\n",
            "Epoch: 21->Batch: 520 / 1875. Loss = 0.16808730363845825\n",
            "Epoch: 21->Batch: 540 / 1875. Loss = 0.2528226673603058\n",
            "Epoch: 21->Batch: 560 / 1875. Loss = 0.1661061942577362\n",
            "Epoch: 21->Batch: 580 / 1875. Loss = 0.100834921002388\n",
            "Epoch: 21->Batch: 600 / 1875. Loss = 0.1104244664311409\n",
            "Epoch: 21->Batch: 620 / 1875. Loss = 0.3891422748565674\n",
            "Epoch: 21->Batch: 640 / 1875. Loss = 0.3212895691394806\n",
            "Epoch: 21->Batch: 660 / 1875. Loss = 0.06282801926136017\n",
            "Epoch: 21->Batch: 680 / 1875. Loss = 0.1575089991092682\n",
            "Epoch: 21->Batch: 700 / 1875. Loss = 0.06475678086280823\n",
            "Epoch: 21->Batch: 720 / 1875. Loss = 0.0940326452255249\n",
            "Epoch: 21->Batch: 740 / 1875. Loss = 0.16731026768684387\n",
            "Epoch: 21->Batch: 760 / 1875. Loss = 0.16674168407917023\n",
            "Epoch: 21->Batch: 780 / 1875. Loss = 0.15160395205020905\n",
            "Epoch: 21->Batch: 800 / 1875. Loss = 0.3786987066268921\n",
            "Epoch: 21->Batch: 820 / 1875. Loss = 0.23331545293331146\n",
            "Epoch: 21->Batch: 840 / 1875. Loss = 0.1501419097185135\n",
            "Epoch: 21->Batch: 860 / 1875. Loss = 0.15728460252285004\n",
            "Epoch: 21->Batch: 880 / 1875. Loss = 0.1717936247587204\n",
            "Epoch: 21->Batch: 900 / 1875. Loss = 0.11963877081871033\n",
            "Epoch: 21->Batch: 920 / 1875. Loss = 0.21294601261615753\n",
            "Epoch: 21->Batch: 940 / 1875. Loss = 0.3496582806110382\n",
            "Epoch: 21->Batch: 960 / 1875. Loss = 0.2197071611881256\n",
            "Epoch: 21->Batch: 980 / 1875. Loss = 0.12755358219146729\n",
            "Epoch: 21->Batch: 1000 / 1875. Loss = 0.3410058617591858\n",
            "Epoch: 21->Batch: 1020 / 1875. Loss = 0.0897381454706192\n",
            "Epoch: 21->Batch: 1040 / 1875. Loss = 0.3613314628601074\n",
            "Epoch: 21->Batch: 1060 / 1875. Loss = 0.2300383448600769\n",
            "Epoch: 21->Batch: 1080 / 1875. Loss = 0.11675386130809784\n",
            "Epoch: 21->Batch: 1100 / 1875. Loss = 0.09127102792263031\n",
            "Epoch: 21->Batch: 1120 / 1875. Loss = 0.22532102465629578\n",
            "Epoch: 21->Batch: 1140 / 1875. Loss = 0.13990454375743866\n",
            "Epoch: 21->Batch: 1160 / 1875. Loss = 0.09179788827896118\n",
            "Epoch: 21->Batch: 1180 / 1875. Loss = 0.3303671181201935\n",
            "Epoch: 21->Batch: 1200 / 1875. Loss = 0.23910799622535706\n",
            "Epoch: 21->Batch: 1220 / 1875. Loss = 0.12305739521980286\n",
            "Epoch: 21->Batch: 1240 / 1875. Loss = 0.4214710295200348\n",
            "Epoch: 21->Batch: 1260 / 1875. Loss = 0.09296154975891113\n",
            "Epoch: 21->Batch: 1280 / 1875. Loss = 0.20683088898658752\n",
            "Epoch: 21->Batch: 1300 / 1875. Loss = 0.5684783458709717\n",
            "Epoch: 21->Batch: 1320 / 1875. Loss = 0.3421509861946106\n",
            "Epoch: 21->Batch: 1340 / 1875. Loss = 0.24466128647327423\n",
            "Epoch: 21->Batch: 1360 / 1875. Loss = 0.33823761343955994\n",
            "Epoch: 21->Batch: 1380 / 1875. Loss = 0.17236053943634033\n",
            "Epoch: 21->Batch: 1400 / 1875. Loss = 0.23472389578819275\n",
            "Epoch: 21->Batch: 1420 / 1875. Loss = 0.2707770764827728\n",
            "Epoch: 21->Batch: 1440 / 1875. Loss = 0.1048126369714737\n",
            "Epoch: 21->Batch: 1460 / 1875. Loss = 0.1176784560084343\n",
            "Epoch: 21->Batch: 1480 / 1875. Loss = 0.237624853849411\n",
            "Epoch: 21->Batch: 1500 / 1875. Loss = 0.48041269183158875\n",
            "Epoch: 21->Batch: 1520 / 1875. Loss = 0.10969021916389465\n",
            "Epoch: 21->Batch: 1540 / 1875. Loss = 0.17727628350257874\n",
            "Epoch: 21->Batch: 1560 / 1875. Loss = 0.0983462780714035\n",
            "Epoch: 21->Batch: 1580 / 1875. Loss = 0.24198059737682343\n",
            "Epoch: 21->Batch: 1600 / 1875. Loss = 0.10633614659309387\n",
            "Epoch: 21->Batch: 1620 / 1875. Loss = 0.1795910894870758\n",
            "Epoch: 21->Batch: 1640 / 1875. Loss = 0.12537284195423126\n",
            "Epoch: 21->Batch: 1660 / 1875. Loss = 0.3285899758338928\n",
            "Epoch: 21->Batch: 1680 / 1875. Loss = 0.26701709628105164\n",
            "Epoch: 21->Batch: 1700 / 1875. Loss = 0.1091100350022316\n",
            "Epoch: 21->Batch: 1720 / 1875. Loss = 0.2644052803516388\n",
            "Epoch: 21->Batch: 1740 / 1875. Loss = 0.22348248958587646\n",
            "Epoch: 21->Batch: 1760 / 1875. Loss = 0.24446187913417816\n",
            "Epoch: 21->Batch: 1780 / 1875. Loss = 0.1723157912492752\n",
            "Epoch: 21->Batch: 1800 / 1875. Loss = 0.17873914539813995\n",
            "Epoch: 21->Batch: 1820 / 1875. Loss = 0.2495749294757843\n",
            "Epoch: 21->Batch: 1840 / 1875. Loss = 0.07558122277259827\n",
            "Epoch: 21->Batch: 1860 / 1875. Loss = 0.09250358492136002\n",
            "In Testing Function!\n",
            "Accuracy: 0.9021 (9021 / 10000)\n",
            "Epoch: 22 start ------------\n",
            "\n",
            "Epoch: 22->Batch: 0 / 1875. Loss = 0.16008360683918\n",
            "Epoch: 22->Batch: 20 / 1875. Loss = 0.11556561291217804\n",
            "Epoch: 22->Batch: 40 / 1875. Loss = 0.48960617184638977\n",
            "Epoch: 22->Batch: 60 / 1875. Loss = 0.20026637613773346\n",
            "Epoch: 22->Batch: 80 / 1875. Loss = 0.2542591392993927\n",
            "Epoch: 22->Batch: 100 / 1875. Loss = 0.12594172358512878\n",
            "Epoch: 22->Batch: 120 / 1875. Loss = 0.18201078474521637\n",
            "Epoch: 22->Batch: 140 / 1875. Loss = 0.12104714661836624\n",
            "Epoch: 22->Batch: 160 / 1875. Loss = 0.3368973731994629\n",
            "Epoch: 22->Batch: 180 / 1875. Loss = 0.312854528427124\n",
            "Epoch: 22->Batch: 200 / 1875. Loss = 0.34353911876678467\n",
            "Epoch: 22->Batch: 220 / 1875. Loss = 0.14836786687374115\n",
            "Epoch: 22->Batch: 240 / 1875. Loss = 0.30467334389686584\n",
            "Epoch: 22->Batch: 260 / 1875. Loss = 0.09225167334079742\n",
            "Epoch: 22->Batch: 280 / 1875. Loss = 0.2473655343055725\n",
            "Epoch: 22->Batch: 300 / 1875. Loss = 0.1363789290189743\n",
            "Epoch: 22->Batch: 320 / 1875. Loss = 0.33058908581733704\n",
            "Epoch: 22->Batch: 340 / 1875. Loss = 0.31997406482696533\n",
            "Epoch: 22->Batch: 360 / 1875. Loss = 0.08287027478218079\n",
            "Epoch: 22->Batch: 380 / 1875. Loss = 0.2037040889263153\n",
            "Epoch: 22->Batch: 400 / 1875. Loss = 0.16079829633235931\n",
            "Epoch: 22->Batch: 420 / 1875. Loss = 0.07742127776145935\n",
            "Epoch: 22->Batch: 440 / 1875. Loss = 0.12132110446691513\n",
            "Epoch: 22->Batch: 460 / 1875. Loss = 0.3341636061668396\n",
            "Epoch: 22->Batch: 480 / 1875. Loss = 0.4036758244037628\n",
            "Epoch: 22->Batch: 500 / 1875. Loss = 0.22955472767353058\n",
            "Epoch: 22->Batch: 520 / 1875. Loss = 0.28413674235343933\n",
            "Epoch: 22->Batch: 540 / 1875. Loss = 0.16703689098358154\n",
            "Epoch: 22->Batch: 560 / 1875. Loss = 0.23065638542175293\n",
            "Epoch: 22->Batch: 580 / 1875. Loss = 0.16175049543380737\n",
            "Epoch: 22->Batch: 600 / 1875. Loss = 0.14053913950920105\n",
            "Epoch: 22->Batch: 620 / 1875. Loss = 0.14749622344970703\n",
            "Epoch: 22->Batch: 640 / 1875. Loss = 0.17061085999011993\n",
            "Epoch: 22->Batch: 660 / 1875. Loss = 0.1699356883764267\n",
            "Epoch: 22->Batch: 680 / 1875. Loss = 0.16702929139137268\n",
            "Epoch: 22->Batch: 700 / 1875. Loss = 0.34527984261512756\n",
            "Epoch: 22->Batch: 720 / 1875. Loss = 0.4071905016899109\n",
            "Epoch: 22->Batch: 740 / 1875. Loss = 0.296522319316864\n",
            "Epoch: 22->Batch: 760 / 1875. Loss = 0.10402722656726837\n",
            "Epoch: 22->Batch: 780 / 1875. Loss = 0.29260361194610596\n",
            "Epoch: 22->Batch: 800 / 1875. Loss = 0.1655673384666443\n",
            "Epoch: 22->Batch: 820 / 1875. Loss = 0.16145586967468262\n",
            "Epoch: 22->Batch: 840 / 1875. Loss = 0.17486172914505005\n",
            "Epoch: 22->Batch: 860 / 1875. Loss = 0.1530301719903946\n",
            "Epoch: 22->Batch: 880 / 1875. Loss = 0.13589604198932648\n",
            "Epoch: 22->Batch: 900 / 1875. Loss = 0.20194359123706818\n",
            "Epoch: 22->Batch: 920 / 1875. Loss = 0.12454575300216675\n",
            "Epoch: 22->Batch: 940 / 1875. Loss = 0.22850829362869263\n",
            "Epoch: 22->Batch: 960 / 1875. Loss = 0.40262705087661743\n",
            "Epoch: 22->Batch: 980 / 1875. Loss = 0.27233022451400757\n",
            "Epoch: 22->Batch: 1000 / 1875. Loss = 0.31308668851852417\n",
            "Epoch: 22->Batch: 1020 / 1875. Loss = 0.19234688580036163\n",
            "Epoch: 22->Batch: 1040 / 1875. Loss = 0.4281138777732849\n",
            "Epoch: 22->Batch: 1060 / 1875. Loss = 0.24109649658203125\n",
            "Epoch: 22->Batch: 1080 / 1875. Loss = 0.43100178241729736\n",
            "Epoch: 22->Batch: 1100 / 1875. Loss = 0.21712061762809753\n",
            "Epoch: 22->Batch: 1120 / 1875. Loss = 0.19172631204128265\n",
            "Epoch: 22->Batch: 1140 / 1875. Loss = 0.4178794026374817\n",
            "Epoch: 22->Batch: 1160 / 1875. Loss = 0.4367188513278961\n",
            "Epoch: 22->Batch: 1180 / 1875. Loss = 0.3041146397590637\n",
            "Epoch: 22->Batch: 1200 / 1875. Loss = 0.1533534973859787\n",
            "Epoch: 22->Batch: 1220 / 1875. Loss = 0.23574583232402802\n",
            "Epoch: 22->Batch: 1240 / 1875. Loss = 0.39610737562179565\n",
            "Epoch: 22->Batch: 1260 / 1875. Loss = 0.32853737473487854\n",
            "Epoch: 22->Batch: 1280 / 1875. Loss = 0.18888108432292938\n",
            "Epoch: 22->Batch: 1300 / 1875. Loss = 0.053648918867111206\n",
            "Epoch: 22->Batch: 1320 / 1875. Loss = 0.1471865177154541\n",
            "Epoch: 22->Batch: 1340 / 1875. Loss = 0.15766635537147522\n",
            "Epoch: 22->Batch: 1360 / 1875. Loss = 0.10334517061710358\n",
            "Epoch: 22->Batch: 1380 / 1875. Loss = 0.36613553762435913\n",
            "Epoch: 22->Batch: 1400 / 1875. Loss = 0.5612089037895203\n",
            "Epoch: 22->Batch: 1420 / 1875. Loss = 0.09798271954059601\n",
            "Epoch: 22->Batch: 1440 / 1875. Loss = 0.1880965381860733\n",
            "Epoch: 22->Batch: 1460 / 1875. Loss = 0.07183746993541718\n",
            "Epoch: 22->Batch: 1480 / 1875. Loss = 0.18787851929664612\n",
            "Epoch: 22->Batch: 1500 / 1875. Loss = 0.13455335795879364\n",
            "Epoch: 22->Batch: 1520 / 1875. Loss = 0.22631224989891052\n",
            "Epoch: 22->Batch: 1540 / 1875. Loss = 0.1300690621137619\n",
            "Epoch: 22->Batch: 1560 / 1875. Loss = 0.3236944079399109\n",
            "Epoch: 22->Batch: 1580 / 1875. Loss = 0.1686684787273407\n",
            "Epoch: 22->Batch: 1600 / 1875. Loss = 0.01497696340084076\n",
            "Epoch: 22->Batch: 1620 / 1875. Loss = 0.3877292275428772\n",
            "Epoch: 22->Batch: 1640 / 1875. Loss = 0.17769742012023926\n",
            "Epoch: 22->Batch: 1660 / 1875. Loss = 0.11846716701984406\n",
            "Epoch: 22->Batch: 1680 / 1875. Loss = 0.20807194709777832\n",
            "Epoch: 22->Batch: 1700 / 1875. Loss = 0.12010926753282547\n",
            "Epoch: 22->Batch: 1720 / 1875. Loss = 0.34305325150489807\n",
            "Epoch: 22->Batch: 1740 / 1875. Loss = 0.25047916173934937\n",
            "Epoch: 22->Batch: 1760 / 1875. Loss = 0.27297598123550415\n",
            "Epoch: 22->Batch: 1780 / 1875. Loss = 0.190366730093956\n",
            "Epoch: 22->Batch: 1800 / 1875. Loss = 0.20372800529003143\n",
            "Epoch: 22->Batch: 1820 / 1875. Loss = 0.10329210013151169\n",
            "Epoch: 22->Batch: 1840 / 1875. Loss = 0.23398062586784363\n",
            "Epoch: 22->Batch: 1860 / 1875. Loss = 0.28641384840011597\n",
            "In Testing Function!\n",
            "Accuracy: 0.9056 (9056 / 10000)\n",
            "Epoch: 23 start ------------\n",
            "\n",
            "Epoch: 23->Batch: 0 / 1875. Loss = 0.18165208399295807\n",
            "Epoch: 23->Batch: 20 / 1875. Loss = 0.20847734808921814\n",
            "Epoch: 23->Batch: 40 / 1875. Loss = 0.15656331181526184\n",
            "Epoch: 23->Batch: 60 / 1875. Loss = 0.23998567461967468\n",
            "Epoch: 23->Batch: 80 / 1875. Loss = 0.28240251541137695\n",
            "Epoch: 23->Batch: 100 / 1875. Loss = 0.12153425812721252\n",
            "Epoch: 23->Batch: 120 / 1875. Loss = 0.30822986364364624\n",
            "Epoch: 23->Batch: 140 / 1875. Loss = 0.25784924626350403\n",
            "Epoch: 23->Batch: 160 / 1875. Loss = 0.14219392836093903\n",
            "Epoch: 23->Batch: 180 / 1875. Loss = 0.33072352409362793\n",
            "Epoch: 23->Batch: 200 / 1875. Loss = 0.2001369297504425\n",
            "Epoch: 23->Batch: 220 / 1875. Loss = 0.19033046066761017\n",
            "Epoch: 23->Batch: 240 / 1875. Loss = 0.23701249063014984\n",
            "Epoch: 23->Batch: 260 / 1875. Loss = 0.16557654738426208\n",
            "Epoch: 23->Batch: 280 / 1875. Loss = 0.2052793949842453\n",
            "Epoch: 23->Batch: 300 / 1875. Loss = 0.4578641951084137\n",
            "Epoch: 23->Batch: 320 / 1875. Loss = 0.11387339234352112\n",
            "Epoch: 23->Batch: 340 / 1875. Loss = 0.20174600183963776\n",
            "Epoch: 23->Batch: 360 / 1875. Loss = 0.24089868366718292\n",
            "Epoch: 23->Batch: 380 / 1875. Loss = 0.13114263117313385\n",
            "Epoch: 23->Batch: 400 / 1875. Loss = 0.5897241830825806\n",
            "Epoch: 23->Batch: 420 / 1875. Loss = 0.2269938886165619\n",
            "Epoch: 23->Batch: 440 / 1875. Loss = 0.3577048182487488\n",
            "Epoch: 23->Batch: 460 / 1875. Loss = 0.42556458711624146\n",
            "Epoch: 23->Batch: 480 / 1875. Loss = 0.28139856457710266\n",
            "Epoch: 23->Batch: 500 / 1875. Loss = 0.2224518209695816\n",
            "Epoch: 23->Batch: 520 / 1875. Loss = 0.34343212842941284\n",
            "Epoch: 23->Batch: 540 / 1875. Loss = 0.1381191611289978\n",
            "Epoch: 23->Batch: 560 / 1875. Loss = 0.1884460747241974\n",
            "Epoch: 23->Batch: 580 / 1875. Loss = 0.12769870460033417\n",
            "Epoch: 23->Batch: 600 / 1875. Loss = 0.05529865622520447\n",
            "Epoch: 23->Batch: 620 / 1875. Loss = 0.20609252154827118\n",
            "Epoch: 23->Batch: 640 / 1875. Loss = 0.3080407679080963\n",
            "Epoch: 23->Batch: 660 / 1875. Loss = 0.035589009523391724\n",
            "Epoch: 23->Batch: 680 / 1875. Loss = 0.26384490728378296\n",
            "Epoch: 23->Batch: 700 / 1875. Loss = 0.12828616797924042\n",
            "Epoch: 23->Batch: 720 / 1875. Loss = 0.3302471339702606\n",
            "Epoch: 23->Batch: 740 / 1875. Loss = 0.18832308053970337\n",
            "Epoch: 23->Batch: 760 / 1875. Loss = 0.15052135288715363\n",
            "Epoch: 23->Batch: 780 / 1875. Loss = 0.0554625540971756\n",
            "Epoch: 23->Batch: 800 / 1875. Loss = 0.3064628541469574\n",
            "Epoch: 23->Batch: 820 / 1875. Loss = 0.2496844232082367\n",
            "Epoch: 23->Batch: 840 / 1875. Loss = 0.1171698123216629\n",
            "Epoch: 23->Batch: 860 / 1875. Loss = 0.36559367179870605\n",
            "Epoch: 23->Batch: 880 / 1875. Loss = 0.11047577112913132\n",
            "Epoch: 23->Batch: 900 / 1875. Loss = 0.1814468502998352\n",
            "Epoch: 23->Batch: 920 / 1875. Loss = 0.28484538197517395\n",
            "Epoch: 23->Batch: 940 / 1875. Loss = 0.29821619391441345\n",
            "Epoch: 23->Batch: 960 / 1875. Loss = 0.2788351774215698\n",
            "Epoch: 23->Batch: 980 / 1875. Loss = 0.19773274660110474\n",
            "Epoch: 23->Batch: 1000 / 1875. Loss = 0.16918180882930756\n",
            "Epoch: 23->Batch: 1020 / 1875. Loss = 0.21620677411556244\n",
            "Epoch: 23->Batch: 1040 / 1875. Loss = 0.3132692873477936\n",
            "Epoch: 23->Batch: 1060 / 1875. Loss = 0.24330441653728485\n",
            "Epoch: 23->Batch: 1080 / 1875. Loss = 0.4239615201950073\n",
            "Epoch: 23->Batch: 1100 / 1875. Loss = 0.21529005467891693\n",
            "Epoch: 23->Batch: 1120 / 1875. Loss = 0.09889659285545349\n",
            "Epoch: 23->Batch: 1140 / 1875. Loss = 0.1897220015525818\n",
            "Epoch: 23->Batch: 1160 / 1875. Loss = 0.08243795484304428\n",
            "Epoch: 23->Batch: 1180 / 1875. Loss = 0.12680935859680176\n",
            "Epoch: 23->Batch: 1200 / 1875. Loss = 0.2997884452342987\n",
            "Epoch: 23->Batch: 1220 / 1875. Loss = 0.10026918351650238\n",
            "Epoch: 23->Batch: 1240 / 1875. Loss = 0.29595935344696045\n",
            "Epoch: 23->Batch: 1260 / 1875. Loss = 0.1818404495716095\n",
            "Epoch: 23->Batch: 1280 / 1875. Loss = 0.0634949654340744\n",
            "Epoch: 23->Batch: 1300 / 1875. Loss = 0.21462717652320862\n",
            "Epoch: 23->Batch: 1320 / 1875. Loss = 0.16151848435401917\n",
            "Epoch: 23->Batch: 1340 / 1875. Loss = 0.24090391397476196\n",
            "Epoch: 23->Batch: 1360 / 1875. Loss = 0.23593106865882874\n",
            "Epoch: 23->Batch: 1380 / 1875. Loss = 0.24608173966407776\n",
            "Epoch: 23->Batch: 1400 / 1875. Loss = 0.21610720455646515\n",
            "Epoch: 23->Batch: 1420 / 1875. Loss = 0.13737821578979492\n",
            "Epoch: 23->Batch: 1440 / 1875. Loss = 0.19230030477046967\n",
            "Epoch: 23->Batch: 1460 / 1875. Loss = 0.10776902735233307\n",
            "Epoch: 23->Batch: 1480 / 1875. Loss = 0.28619301319122314\n",
            "Epoch: 23->Batch: 1500 / 1875. Loss = 0.1542322188615799\n",
            "Epoch: 23->Batch: 1520 / 1875. Loss = 0.35693418979644775\n",
            "Epoch: 23->Batch: 1540 / 1875. Loss = 0.30328169465065\n",
            "Epoch: 23->Batch: 1560 / 1875. Loss = 0.12635746598243713\n",
            "Epoch: 23->Batch: 1580 / 1875. Loss = 0.14154081046581268\n",
            "Epoch: 23->Batch: 1600 / 1875. Loss = 0.10077746212482452\n",
            "Epoch: 23->Batch: 1620 / 1875. Loss = 0.2141762673854828\n",
            "Epoch: 23->Batch: 1640 / 1875. Loss = 0.2112412452697754\n",
            "Epoch: 23->Batch: 1660 / 1875. Loss = 0.22910763323307037\n",
            "Epoch: 23->Batch: 1680 / 1875. Loss = 0.5330933332443237\n",
            "Epoch: 23->Batch: 1700 / 1875. Loss = 0.11179374158382416\n",
            "Epoch: 23->Batch: 1720 / 1875. Loss = 0.1488843858242035\n",
            "Epoch: 23->Batch: 1740 / 1875. Loss = 0.11469577252864838\n",
            "Epoch: 23->Batch: 1760 / 1875. Loss = 0.17609985172748566\n",
            "Epoch: 23->Batch: 1780 / 1875. Loss = 0.1966785341501236\n",
            "Epoch: 23->Batch: 1800 / 1875. Loss = 0.22727741301059723\n",
            "Epoch: 23->Batch: 1820 / 1875. Loss = 0.2520614564418793\n",
            "Epoch: 23->Batch: 1840 / 1875. Loss = 0.1830483078956604\n",
            "Epoch: 23->Batch: 1860 / 1875. Loss = 0.2164938598871231\n",
            "In Testing Function!\n",
            "Accuracy: 0.9035 (9035 / 10000)\n",
            "Current stats of MNIST_NET:\n",
            "Accuracy:      0.9035\n",
            "Training Loss: 0.14731457829475403\n",
            "Test Loss:     2720.6381375789642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u4eq4TEYsBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du7C4tUx61za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}